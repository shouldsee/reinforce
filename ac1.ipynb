{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[2017-06-12 17:21:27,247] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 80, 80, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 25)        925       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 5)         4505      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3645)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                72920     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 78,896\n",
      "Trainable params: 78,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(25, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(3, 3))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(1, 1))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:61: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, Flatten\n",
    "from keras.layers import Input, Dense, convolutional,core,Flatten,Reshape,Concatenate\n",
    "from keras.models import Model,load_model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "\n",
    "standardise = lambda rewards: (rewards - np.mean(rewards,keepdims =1)) / np.std(rewards ) ;\n",
    "\n",
    "\n",
    "class temp:\n",
    "    def __init__(self):\n",
    "        self.clean();\n",
    "    def clean(self):\n",
    "        self.states = []\n",
    "        self.gradients = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "        self.tpreds = [];\n",
    "        self.aprobs = [];\n",
    "        self.actions = [];\n",
    "    def remember(self,state,action,prob,reward,tpred):\n",
    "        self.states.append(state);\n",
    "        self.probs.append(prob)\n",
    "        self.rewards.append(reward);\n",
    "        self.tpreds.append(tpred);\n",
    "        y = np.zeros([action_size])\n",
    "        y[action] = 1\n",
    "        self.actions.append(y);\n",
    "        self.gradients.append(np.array(y).astype('float32') - prob)\n",
    "        \n",
    "class PGAgent:\n",
    "    def __init__(self, state_size, action_size, tmp):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "#         self.states = []\n",
    "#         self.gradients = []\n",
    "#         self.rewards = []\n",
    "#         self.probs = []\n",
    "        self.tmp = tmp;\n",
    "        self.model = self._build_model()\n",
    "        self.summary = self.model.summary;\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((80, 80, 1), input_shape=(self.state_size,)))\n",
    "        model.add(Convolution2D(25, (6, 6), subsample=(3, 3), border_mode='same',\n",
    "                                activation='relu', init='he_uniform'))\n",
    "        model.add(Convolution2D(5, (6, 6), subsample=(1, 1), border_mode='same',\n",
    "                                activation='relu', init='he_uniform'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(20, activation='relu', init='he_uniform'))\n",
    "        model.add(Dense(20, activation='relu', init='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        opt = Adam(lr=self.learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, prob, reward):\n",
    "        y = np.zeros([self.action_size])\n",
    "        y[action] = 1\n",
    "        self.gradients.append(np.array(y).astype('float32') - prob)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def act(self, state):\n",
    "        # state = state.reshape([1, state.shape[0]])\n",
    "        aprob = self.model.predict(state, batch_size=1).flatten()\n",
    "        self.tmp.aprobs.append(aprob)\n",
    "        prob = aprob / np.sum(aprob)\n",
    "        action = np.random.choice(self.action_size, 1, p=prob)[0]\n",
    "        return action, prob\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, rewards.size)):\n",
    "            if rewards[t] != 0:\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    def train(self,rewards):\n",
    "        gradients = np.vstack(self.tmp.gradients)\n",
    "        # rewards = np.vstack(self.rewards)\n",
    "        # rewards = self.discount_rewards(rewards)\n",
    "#         rewards = (rewards - np.mean(rewards,keepdims=1)) / np.std(rewards)\n",
    "        gradients *= rewards\n",
    "        X = np.squeeze(np.vstack([self.tmp.states]))\n",
    "        Y = self.tmp.aprobs + self.learning_rate * np.squeeze(np.vstack([gradients]))\n",
    "        self.model.train_on_batch(X, Y)\n",
    "\n",
    "    def clean(self): \n",
    "        self.tmp.clean();\n",
    "#         self.states, self.probs, self.gradients, self.rewards = [], [], [], []\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "    def readlog(self, LogName):\n",
    "#         LogName = self.LogName;\n",
    "        with open(LogName,'rb') as f:\n",
    "                first = f.readline()      # Read the first line.\n",
    "                f.seek(-2, 2)             # Jump to the second last byte.\n",
    "                while f.read(1) != b\"\\n\": # Until EOL is found...\n",
    "                    f.seek(-2, 1)         # ...jump back the read byte plus one more.\n",
    "                last = f.readline() \n",
    "                lst = last.split('\\t');\n",
    "                eind = lst.index('Episode')+1;\n",
    "                self.episode = int(lst[eind]);\n",
    "        \n",
    "def preprocess(I):\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(np.float).ravel()\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "\n",
    "    state_size = 80 * 80\n",
    "    action_size = env.action_space.n\n",
    "    agent = PGAgent(state_size, action_size,temp())\n",
    "    \n",
    "agent.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 6400)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)              (None, 80, 80, 1)     0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 27, 27, 5)     185         reshape_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 27, 27, 25)    150         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 18225)         0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 18231)         0           flatten_2[0][0]                  \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 2)             36464       concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 36,799\n",
      "Trainable params: 36,799\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class critic:\n",
    "    def __init__(self,state_size,action_size,tmp):\n",
    "        self.D = state_size;\n",
    "        self.action_size = action_size;\n",
    "        self.H = 5;\n",
    "        self.tmp = tmp;\n",
    "        self.model = self.model_init();\n",
    "        self.predict = self.model.predict;\n",
    "        self.summary = self.model.summary;\n",
    "        self.load = self.model.load_weights;\n",
    "        self.save = self.model.save_weights;\n",
    "        \n",
    "    def model_init(self):\n",
    "        def lossfunc(y_true,y_pred):\n",
    "            return K.mean(K.mean( K.square( y_pred / (K.abs(y_true)+1) - 1)  )); \n",
    "\n",
    "        D = self.D;\n",
    "        H = self.H;\n",
    "        x_input = Input(shape=(D,));\n",
    "        po_input = Input(shape=(self.action_size,));\n",
    "        conv1 = convolutional.Conv2D(filters=H,\n",
    "                             kernel_size=(6,6),\n",
    "                            strides=(3,3),\n",
    "                            padding='same',\n",
    "                            activation='relu')(Reshape((80, 80,1))(x_input))\n",
    "        den1 = Flatten()(\n",
    "            Dense(units=5*H,\n",
    "             activation='relu')(conv1)\n",
    "            )\n",
    "        den1c = Concatenate()([den1,po_input]);\n",
    "        \n",
    "        score = Dense(units=2,\n",
    "              activation = 'relu')(den1c)\n",
    "    \n",
    "        model = Model(inputs=[x_input,po_input], outputs=[score])\n",
    "#         optimiser = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "        model.compile(optimizer='rmsprop',\n",
    "              loss=lossfunc)\n",
    "        return model\n",
    "    \n",
    "    def time_rewards(self,r):\n",
    "        \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        grad = 0;\n",
    "        for t in reversed(xrange(0, r.size)):\n",
    "    #         grad = grad * gamma + r[t] ;\n",
    "            if r[t] != 0: \n",
    "                running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "                grad = 2 * (r[t] > 0) - 1; \n",
    "            running_add = running_add + grad;\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r\n",
    "    \n",
    "    def decouple(self,tpr):\n",
    "        tpr1=np.maximum(tpr,0);\n",
    "        tpr2=np.minimum(tpr,0);\n",
    "        tpr1[tpr1==0]=np.maximum(np.max(tpr1),20);\n",
    "        tpr2[tpr2==0]=np.minimum(np.min(tpr2),-20);\n",
    "        tpr2=-tpr2;\n",
    "        time_epr = np.hstack([tpr1,tpr2])\n",
    "        return(time_epr)\n",
    "    \n",
    "    def get_rsignal(self):\n",
    "        x = np.vstack(self.tmp.tpreds);\n",
    "        x_diff = convolve2d(x,[[0.],[-1.],[1.]],mode='same');\n",
    "        x_mean = convolve2d(x,[[0.],[.5],[.5]],mode='same');\n",
    "        x_vari = abs(x_diff)/x_mean;\n",
    "        x_vari[np.isnan(x_vari)]=0;\n",
    "        x_vari = np.mean(x_vari,axis = 1,keepdims = 1);\n",
    "        return(x_vari);\n",
    "    #     sx_avg = x_avg * rsign;\n",
    "    def train(self):\n",
    "        epx = np.vstack(self.tmp.states);\n",
    "        epy = np.vstack(self.tmp.actions);\n",
    "        epr = np.vstack(self.tmp.rewards);\n",
    "        tpr=self.time_rewards(epr);\n",
    "        time_epr = self.decouple(tpr);\n",
    "        curr_loss = self.model.train_on_batch([epx,epy], time_epr);\n",
    "        return curr_loss\n",
    "        \n",
    "#     def save(self,name):\n",
    "#         print('no method to save');\n",
    "#     def load(self,name):\n",
    "#         print('no method to load')\n",
    "cagent = critic(state_size,action_size,temp());\n",
    "cagent.model_init();\n",
    "cagent.summary();\n",
    "pbatch = lambda x: np.reshape(x,(1,-1))\n",
    "# tpred = cagent.predict([pbatch(x),pbatch(prob)])\n",
    "# print(tpred)\n",
    "# tpred = cagent.predict([x,prob]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-12 17:21:28,211] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_4 (Reshape)          (None, 80, 80, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 27, 27, 25)        925       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 27, 27, 5)         4505      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3645)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                72920     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 78,896\n",
      "Trainable params: 78,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(25, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(3, 3))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(1, 1))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:61: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6400, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "\n",
    "    state_size = 80 * 80\n",
    "    action_size = env.action_space.n\n",
    "    agent = PGAgent(state_size, action_size,temp())\n",
    "    \n",
    "agent.summary();\n",
    "state_size,action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-12 17:21:28,841] Making new env: Pong-v0\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(25, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(3, 3))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(1, 1))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:61: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode\t1\tScore\t-20.000000\tLoss\t0.999573\n",
      "Episode\t2\tScore\t-20.000000\tLoss\t0.991180\n",
      "Episode\t3\tScore\t-20.000000\tLoss\t0.978813\n",
      "Episode\t4\tScore\t-21.000000\tLoss\t0.907223\n",
      "Episode\t5\tScore\t-21.000000\tLoss\t0.840776\n",
      "Episode\t6\tScore\t-20.000000\tLoss\t0.846171\n",
      "Episode\t7\tScore\t-21.000000\tLoss\t0.688707\n",
      "Episode\t8\tScore\t-20.000000\tLoss\t0.748515\n",
      "Episode\t9\tScore\t-19.000000\tLoss\t0.770432\n",
      "Episode\t10\tScore\t-21.000000\tLoss\t0.586274\n",
      "Episode\t11\tScore\t-21.000000\tLoss\t0.515917\n",
      "Episode\t12\tScore\t-20.000000\tLoss\t0.778500\n",
      "Episode\t13\tScore\t-21.000000\tLoss\t0.500952\n",
      "Episode\t14\tScore\t-19.000000\tLoss\t0.687984\n",
      "Episode\t15\tScore\t-20.000000\tLoss\t0.780151\n",
      "Episode\t16\tScore\t-21.000000\tLoss\t0.443985\n",
      "Episode\t17\tScore\t-21.000000\tLoss\t0.392968\n",
      "Episode\t18\tScore\t-20.000000\tLoss\t0.664151\n",
      "Episode\t19\tScore\t-20.000000\tLoss\t0.644038\n",
      "Episode\t20\tScore\t-20.000000\tLoss\t0.610484\n",
      "Episode\t21\tScore\t-21.000000\tLoss\t0.351758\n",
      "Episode\t22\tScore\t-20.000000\tLoss\t0.686612\n",
      "Episode\t23\tScore\t-20.000000\tLoss\t0.607621\n",
      "Episode\t24\tScore\t-21.000000\tLoss\t0.330386\n",
      "Episode\t25\tScore\t-21.000000\tLoss\t0.305392\n",
      "Episode\t26\tScore\t-20.000000\tLoss\t0.678757\n",
      "Episode\t27\tScore\t-19.000000\tLoss\t0.669372\n",
      "Episode\t28\tScore\t-21.000000\tLoss\t0.308998\n",
      "Episode\t29\tScore\t-19.000000\tLoss\t0.676971\n",
      "Episode\t30\tScore\t-21.000000\tLoss\t0.333345\n",
      "Episode\t31\tScore\t-19.000000\tLoss\t0.716079\n",
      "Episode\t32\tScore\t-20.000000\tLoss\t0.612167\n",
      "Episode\t33\tScore\t-20.000000\tLoss\t0.596968\n",
      "Episode\t34\tScore\t-19.000000\tLoss\t0.752445\n",
      "Episode\t35\tScore\t-21.000000\tLoss\t0.309934\n",
      "Episode\t36\tScore\t-20.000000\tLoss\t0.669602\n",
      "Episode\t37\tScore\t-20.000000\tLoss\t0.607665\n",
      "Episode\t38\tScore\t-19.000000\tLoss\t0.665112\n",
      "Episode\t39\tScore\t-19.000000\tLoss\t0.765353\n",
      "Episode\t40\tScore\t-21.000000\tLoss\t0.313200\n",
      "Episode\t41\tScore\t-21.000000\tLoss\t0.314356\n",
      "Episode\t42\tScore\t-21.000000\tLoss\t0.299812\n",
      "Episode\t43\tScore\t-20.000000\tLoss\t0.577561\n",
      "Episode\t44\tScore\t-20.000000\tLoss\t0.769318\n",
      "Episode\t45\tScore\t-20.000000\tLoss\t0.598476\n",
      "Episode\t46\tScore\t-18.000000\tLoss\t0.777109\n",
      "Episode\t47\tScore\t-21.000000\tLoss\t0.302791\n",
      "Episode\t48\tScore\t-20.000000\tLoss\t0.669662\n",
      "Episode\t49\tScore\t-20.000000\tLoss\t0.589344\n",
      "Episode\t50\tScore\t-21.000000\tLoss\t0.303751\n",
      "Episode\t51\tScore\t-20.000000\tLoss\t0.656544\n",
      "Episode\t52\tScore\t-21.000000\tLoss\t0.311167\n",
      "Episode\t53\tScore\t-20.000000\tLoss\t0.665005\n",
      "Episode\t54\tScore\t-20.000000\tLoss\t0.601796\n",
      "Episode\t55\tScore\t-21.000000\tLoss\t0.293950\n",
      "Episode\t56\tScore\t-20.000000\tLoss\t0.600740\n",
      "Episode\t57\tScore\t-21.000000\tLoss\t0.301246\n",
      "Episode\t58\tScore\t-21.000000\tLoss\t0.278594\n",
      "Episode\t59\tScore\t-21.000000\tLoss\t0.281443\n",
      "Episode\t60\tScore\t-20.000000\tLoss\t0.660321\n",
      "Episode\t61\tScore\t-19.000000\tLoss\t0.713517\n",
      "Episode\t62\tScore\t-20.000000\tLoss\t0.609909\n",
      "Episode\t63\tScore\t-21.000000\tLoss\t0.293149\n",
      "Episode\t64\tScore\t-21.000000\tLoss\t0.299591\n",
      "Episode\t65\tScore\t-21.000000\tLoss\t0.285256\n",
      "Episode\t66\tScore\t-20.000000\tLoss\t0.663234\n",
      "Episode\t67\tScore\t-21.000000\tLoss\t0.296671\n",
      "Episode\t68\tScore\t-20.000000\tLoss\t0.663772\n",
      "Episode\t69\tScore\t-20.000000\tLoss\t0.604981\n",
      "Episode\t70\tScore\t-20.000000\tLoss\t0.616093\n",
      "Episode\t71\tScore\t-21.000000\tLoss\t0.316034\n",
      "Episode\t72\tScore\t-19.000000\tLoss\t0.728679\n",
      "Episode\t73\tScore\t-21.000000\tLoss\t0.307447\n",
      "Episode\t74\tScore\t-21.000000\tLoss\t0.270062\n",
      "Episode\t75\tScore\t-18.000000\tLoss\t0.713607\n",
      "Episode\t76\tScore\t-21.000000\tLoss\t0.309183\n",
      "Episode\t77\tScore\t-20.000000\tLoss\t0.587215\n",
      "Episode\t78\tScore\t-21.000000\tLoss\t0.294924\n",
      "Episode\t79\tScore\t-19.000000\tLoss\t0.652574\n",
      "Episode\t80\tScore\t-19.000000\tLoss\t0.792971\n",
      "Episode\t81\tScore\t-21.000000\tLoss\t0.294663\n",
      "Episode\t82\tScore\t-21.000000\tLoss\t0.301313\n",
      "Episode\t83\tScore\t-21.000000\tLoss\t0.300046\n",
      "Episode\t84\tScore\t-21.000000\tLoss\t0.269455\n",
      "Episode\t85\tScore\t-21.000000\tLoss\t0.284757\n",
      "Episode\t86\tScore\t-21.000000\tLoss\t0.285560\n",
      "Episode\t87\tScore\t-20.000000\tLoss\t0.581417\n",
      "Episode\t88\tScore\t-20.000000\tLoss\t0.594106\n",
      "Episode\t89\tScore\t-20.000000\tLoss\t0.550964\n",
      "Episode\t90\tScore\t-20.000000\tLoss\t0.648870\n",
      "Episode\t91\tScore\t-20.000000\tLoss\t0.649837\n",
      "Episode\t92\tScore\t-21.000000\tLoss\t0.276122\n",
      "Episode\t93\tScore\t-19.000000\tLoss\t0.664463\n",
      "Episode\t94\tScore\t-20.000000\tLoss\t0.633488\n",
      "Episode\t95\tScore\t-19.000000\tLoss\t0.645081\n",
      "Episode\t96\tScore\t-20.000000\tLoss\t0.639964\n",
      "Episode\t97\tScore\t-21.000000\tLoss\t0.259880\n",
      "Episode\t98\tScore\t-21.000000\tLoss\t0.299095\n",
      "Episode\t99\tScore\t-20.000000\tLoss\t0.591095\n",
      "Episode\t100\tScore\t-21.000000\tLoss\t0.282161\n",
      "Episode\t101\tScore\t-20.000000\tLoss\t0.625621\n",
      "Episode\t102\tScore\t-20.000000\tLoss\t0.574306\n",
      "Episode\t103\tScore\t-21.000000\tLoss\t0.252605\n",
      "Episode\t104\tScore\t-19.000000\tLoss\t0.709911\n",
      "Episode\t105\tScore\t-20.000000\tLoss\t0.582661\n",
      "Episode\t106\tScore\t-18.000000\tLoss\t0.791260\n",
      "Episode\t107\tScore\t-21.000000\tLoss\t0.264122\n",
      "Episode\t108\tScore\t-21.000000\tLoss\t0.252632\n",
      "Episode\t109\tScore\t-20.000000\tLoss\t0.576869\n",
      "Episode\t110\tScore\t-20.000000\tLoss\t0.548004\n",
      "Episode\t111\tScore\t-21.000000\tLoss\t0.262047\n",
      "Episode\t112\tScore\t-21.000000\tLoss\t0.244481\n",
      "Episode\t113\tScore\t-21.000000\tLoss\t0.265687\n",
      "Episode\t114\tScore\t-20.000000\tLoss\t0.578232\n",
      "Episode\t115\tScore\t-20.000000\tLoss\t0.541850\n",
      "Episode\t116\tScore\t-21.000000\tLoss\t0.267247\n",
      "Episode\t117\tScore\t-20.000000\tLoss\t0.580563\n",
      "Episode\t118\tScore\t-19.000000\tLoss\t0.702301\n",
      "Episode\t119\tScore\t-20.000000\tLoss\t0.574737\n",
      "Episode\t120\tScore\t-19.000000\tLoss\t0.682225\n",
      "Episode\t121\tScore\t-20.000000\tLoss\t0.560653\n",
      "Episode\t122\tScore\t-21.000000\tLoss\t0.236141\n",
      "Episode\t123\tScore\t-20.000000\tLoss\t0.580662\n",
      "Episode\t124\tScore\t-21.000000\tLoss\t0.254958\n",
      "Episode\t125\tScore\t-21.000000\tLoss\t0.274279\n"
     ]
    }
   ],
   "source": [
    "AgentName = 'ac1'\n",
    "AgentFile = 'Models/%s.h5'%AgentName;\n",
    "cAgentFile = 'Models/%s-critic.h5'%AgentName;\n",
    "\n",
    "LogName = 'Models/%s.log'%AgentName;\n",
    "# LogFile = open(LogName,'a');\n",
    "resume = 0;\n",
    "render = 0;\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "\n",
    "    state_size = 80 * 80\n",
    "    action_size = env.action_space.n\n",
    "    tmp = temp();\n",
    "    agent = PGAgent(state_size, action_size,tmp)\n",
    "    cagent = critic(state_size,action_size,tmp);\n",
    "#     agent.summary();\n",
    "#     cagent.summary();\n",
    "    \n",
    "\n",
    "    if resume:\n",
    "        agent.load(AgentFile)\n",
    "        agent.readlog(LogName);\n",
    "        episode = agent.episode;\n",
    "        cagent.load(cAgentFile)\n",
    "        \n",
    "    while episode > -1:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        cur_x = preprocess(state)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(state_size)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        action, prob = agent.act(np.expand_dims(x,1).T)\n",
    "        avct = np.zeros([action_size])\n",
    "        avct[action] = 1\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        ### manage the list \n",
    "#         agent.remember(x, action, prob, reward)\n",
    "        tpred = cagent.predict([pbatch(x),pbatch(avct)]);\n",
    "        tmp.remember(x,action,prob,reward,tpred);\n",
    "#         cagent.remember(x, action, prob, reward, tpred);\n",
    "        \n",
    "        if done:\n",
    "            episode += 1\n",
    "            rewards = np.vstack(tmp.rewards)\n",
    "            \n",
    "            \n",
    "            ## process reward on a per event basis\n",
    "            curr_loss = cagent.train();\n",
    "#             losshist.add(episode_number,curr_loss);                        \n",
    "#             rsignal = merge_signal(epr,eptpred,.5);\n",
    "            rewards = standardise(agent.discount_rewards(rewards) * cagent.get_rsignal())\n",
    "            \n",
    "            agent.train(rewards)\n",
    "            tmp.clean();\n",
    "#             agent.record()\n",
    "            msg = '%s\\t%d\\t%s\\t%f\\t%s\\t%f' % ('Episode',episode,'Score', score,'Loss',curr_loss);\n",
    "            print(msg);\n",
    "            if episode == 1  and resume == 0:\n",
    "                open(LogName,'w').close();\n",
    "            with open(LogName,'a+') as LogFile:\n",
    "                LogFile.write(msg+'\\n');\n",
    "            \n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            prev_x = None\n",
    "            if episode > 1 and episode % 10 == 0:\n",
    "                agent.save(AgentFile)\n",
    "                cagent.save(cAgentFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print >> LogFile, 'Hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogFile = open('Models/%s.log'%AgentName,'a')\n",
    "LogFile.write('Hi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
