{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "# gamma = 1-0.\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "# resume = False;\n",
    "render = False\n",
    "# render = True\n",
    "backlen=20;\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "def lookback(lst):\n",
    "    lst = lst[-backlen:];\n",
    "#     np.pad(lst,(20-lst.size,), 'constant', constant_values=0);\n",
    "    if len(lst) != backlen:\n",
    "        lst = [None]*(backlen-len(lst)) + lst;\n",
    "    return(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8a28f7fbfa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# discount_rewards(epdlogp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# discount_rewards(epdlogp)\n",
    "x.size\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig=plt.figure(figsize=[10,10])\n",
    "ax1=plt.subplot()\n",
    "# ax1.plot(time_epr)\n",
    "# ax1.plot(discounted_epr)\n",
    "ax1.scatter(abs(time_epr),eptpred)\n",
    "# ax1.set_xlim([0, 200])\n",
    "# ax1.set_xlim([800, 1000])\n",
    "# ax1.set_ylim([-10, 000])\n",
    "\n",
    "\n",
    "# ax1.imshow(eph[:500,:500].T)\n",
    "time_epr.size\n",
    "# tpreds.size\n",
    "D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141592653589793"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "resume = 1;\n",
    "# resume = True;\n",
    "render = False;\n",
    "render = True;\n",
    "H=5;\n",
    "gamma = 0.99;\n",
    "D1=80;D2=80;\n",
    "\n",
    "def time_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    grad = 0;\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "#         grad = grad * gamma + r[t] ;\n",
    "        if r[t] != 0: \n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "            grad = 2 * (r[t] > 0) - 1; \n",
    "        running_add = running_add + grad;\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "tf.reset_default_graph()\n",
    "# if resume:\n",
    "#     pass\n",
    "\n",
    "# else:\n",
    "#     observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "#     W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "#                initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b1 = tf.constant(0.1, shape=[1,H]);\n",
    "#     W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "#                initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b2 = tf.constant(0.1, shape=[1,1]);\n",
    "xinput = tf.placeholder(tf.float32, name=\"input_x\")\n",
    "observations = tf.reshape(xinput,[-1,D1,D2,1])\n",
    "conv1 = tf.layers.conv2d(\n",
    "  inputs=observations,\n",
    "  filters=H,\n",
    "  kernel_size=[5, 5],\n",
    "  padding=\"same\",\n",
    "  activation=tf.nn.relu)\n",
    "dense = tf.layers.dense(inputs=tf.reshape(conv1,[-1, D*H]), units=H, activation=tf.nn.relu)\n",
    "dropout = dense;\n",
    "#     logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "score = tf.layers.dense(inputs=dropout, units=1,activation = tf.nn.relu)\n",
    "tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "#     advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "rtime = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "#     loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "# loss = tf.reduce_mean( tf.square(tf.abs(score/rtime) - 1) - tf.sign(score/rtime) ); \n",
    "loss = tf.reduce_mean( tf.square(tf.abs(score/rtime) - 1)  ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 17:46:18,190] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "batchGrad = [];\n",
    "for i,var in enumerate(tvars):\n",
    "    exec('Grad%d'%i + ' = tf.placeholder(tf.float32)');\n",
    "    exec('batchGrad += [Grad%d]'%i)\n",
    "\n",
    "\n",
    "optimiser = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "#     batchGrad = optimiser.compute_gradients(loss,tvars);\n",
    "batchGrad = newGrads;\n",
    "updateGrads = optimiser.apply_gradients(zip(batchGrad,tvars))\n",
    "init = tf.global_variables_initializer();\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "ys=[];byss=[];rss=[];tpreds=[];\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape(None)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xinput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.variable_scope.get_variable>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_vars\n",
    "# tf.get_tensor_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Models/expolike_RL_pong_RMSprop.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 17:46:21,353] Restoring parameters from Models/expolike_RL_pong_RMSprop.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 1 reward total was -21.000000. loss_func: 0.375355\n",
      "resetting env. episode 2 reward total was -20.000000. loss_func: 0.366969\n",
      "resetting env. episode 3 reward total was -21.000000. loss_func: 0.322063\n",
      "resetting env. episode 4 reward total was -21.000000. loss_func: 0.439940\n",
      "resetting env. episode 5 reward total was -21.000000. loss_func: 0.288423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-96ae70f66a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtime_epr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#            curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdateGrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxinput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtime\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime_epr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mcurr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m#             curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ModelName = 'expolike_RL_pong_RMSprop'\n",
    "ModelFile = 'Models/'+ModelName+'.ckpt';\n",
    "render = False;\n",
    "resume = 1;\n",
    "batch_size=1;\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# ModelName='save.ckpt'\n",
    "# fname = \n",
    "# with tf.Session() as sess:\n",
    "# gdcmd='grad_dict = {'+','.join(['Grad%d: gradBuffer[%d]'%(i,i) for i,k in enumerate(tvars)])+'}';\n",
    "while True:\n",
    "    sess = tf.Session();\n",
    "#     gradBuffer = [np.zeros_like(v) for v in sess.run(tvars)]\n",
    "#     grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "    if resume: \n",
    "#         tf.train.import_meta_graph(ModelFile+'.meta')    \n",
    "        oSaver = tf.train.Saver()\n",
    "#         oSaver.restore(sess,ModelFile)\n",
    "#         oSaver = tf.train.import_meta_graph(ModelFile+'.meta')    \n",
    "#         oSaver.restore(sess,tf.train.latest_checkpoint('./Models/'))\n",
    "        oSaver.restore(sess,ModelFile)\n",
    "    \n",
    "    else:\n",
    "        sess.run(init)\n",
    "    while True:\n",
    "        if render: env.render()\n",
    "\n",
    "        # preprocess the observation, set input to network to be difference image\n",
    "        cur_x = prepro(observation)\n",
    "        diff_x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "        x = np.reshape(diff_x,[1,D1,D2,1]);\n",
    "#         aprob = sess.run(probability,feed_dict={observations: x})\n",
    "        aprob = 0.5;\n",
    "        tpred = sess.run(score,feed_dict={observations: x})    \n",
    "        action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "        tpreds.append(tpred);\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys);\n",
    "            epr = np.vstack(drs)\n",
    "            eptpred=np.vstack(tpreds);\n",
    "            xs,hs,dlogps,drs,ys,tpreds = [],[],[],[],[],[] # reset array memory\n",
    "            \n",
    "#             # compute the discounted reward backwards through time\n",
    "#             discounted_epr = discount_rewards(epr)\n",
    "#             # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "#             discounted_epr -= np.mean(discounted_epr)\n",
    "#             discounted_epr /= np.std(discounted_epr)\n",
    "#             discounted_epr = discount_rewards(epr);\n",
    "            time_epr=time_rewards(epr);\n",
    "#            curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "            lst =  sess.run([loss,updateGrads],feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "            curr_loss = lst[0];\n",
    "#             curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "            \n",
    "#             tGrad = sess.run([loss,newGrads,updateGrads],feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "#             for ix,grad in enumerate(tGrad):\n",
    "#                 gradBuffer[ix] += grad\n",
    "            \n",
    "            # perform rmsprop parameter update every batch_size episodes\n",
    "            if episode_number % batch_size == 0:\n",
    "                pass\n",
    "#                 grad_dict={varname:}\n",
    "#                 exec(gdcmd);\n",
    "#                 sess.run(updateGrads,feed_dict={Grad0: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "#                 sess.run(updateGrads)\n",
    "#                 sess.run(updateGrads,feed_dict={newGrads:tGrad})\n",
    "#                 sess.run(updateGrads,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "             \n",
    "                \n",
    "#                 gradBuffer = [np.zeros_like(v) for v in sess.run(tvars)]\n",
    "    \n",
    "            # boring book-keeping\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print 'resetting env. episode %d reward total was %f. loss_func: %f' % (episode_number, reward_sum, curr_loss)\n",
    "            if episode_number % 10  == 9: \n",
    "                oSaver = tf.train.Saver()\n",
    "                oSess = sess\n",
    "                oSaver.save(oSess, ModelFile) \n",
    "#                 pickle.dump(tf, open('save.p', 'wb'))\n",
    "            reward_sum = 0\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "\n",
    "        if reward != 0: # Pong has either +1 or -1 reward exactly when game ends. \n",
    "            bys=lookback(ys);\n",
    "            byss.append(bys);\n",
    "            rss.append(reward);\n",
    "            if len(byss)-1 == 100:\n",
    "                    byss.pop(0);\n",
    "                    rss.pop(0);\n",
    "\n",
    "            pass;\n",
    "    #     print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " ..., \n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n"
     ]
    }
   ],
   "source": [
    "# print(time_epr.ravel())\n",
    "# time_epr\n",
    "# tpreds\n",
    "# H\n",
    "# np.expand_dims(epx,1).shape\n",
    "# curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "# epx.shape\n",
    "# tf.reshape(epy);\n",
    "# D*H\n",
    "# oSaver = tf.train.Saver()\n",
    "print(sess.run(score,feed_dict={xinput: epx, input_y: epy, rtime: time_epr}))\n",
    "# oSess = sess\n",
    "# oSaver.save(oSess, ModelFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
