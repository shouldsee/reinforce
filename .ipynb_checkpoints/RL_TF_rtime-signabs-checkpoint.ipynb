{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "# gamma = 1-0.\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "# resume = False;\n",
    "render = False\n",
    "# render = True\n",
    "backlen=20;\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "def lookback(lst):\n",
    "    lst = lst[-backlen:];\n",
    "#     np.pad(lst,(20-lst.size,), 'constant', constant_values=0);\n",
    "    if len(lst) != backlen:\n",
    "        lst = [None]*(backlen-len(lst)) + lst;\n",
    "    return(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "resume = False;\n",
    "# resume = True;\n",
    "render = False;\n",
    "render = True;\n",
    "H=5;\n",
    "gamma = 0.99;\n",
    "D1=80;D2=80;\n",
    "# signof = lambda x : x\n",
    "loglik = lambda prob,lab:tf.log(lab*(lab - prob) + (1 - lab)*(lab + prob));\n",
    "\n",
    "\n",
    "def time_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    grad = 0;\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "#         grad = grad * gamma + r[t] ;\n",
    "        if r[t] != 0: \n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "            grad = 2 * (r[t] > 0) - 1; \n",
    "        running_add = running_add + grad;\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "tf.reset_default_graph()\n",
    "# if resume:\n",
    "#     pass\n",
    "\n",
    "# else:\n",
    "#     observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "#     W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "#                initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b1 = tf.constant(0.1, shape=[1,H]);\n",
    "#     W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "#                initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     b2 = tf.constant(0.1, shape=[1,1]);\n",
    "xinput = tf.placeholder(tf.float32, name=\"input_x\")\n",
    "observations = tf.reshape(xinput,[-1,D1,D2,1])\n",
    "conv1 = tf.layers.conv2d(\n",
    "  inputs=observations,\n",
    "  filters=H,\n",
    "  kernel_size=[5, 5],\n",
    "  padding=\"same\",\n",
    "  activation=tf.nn.relu)\n",
    "dense = tf.layers.dense(inputs=tf.reshape(conv1,[-1, D*H]), units=H, activation=tf.nn.relu)\n",
    "dropout = dense;\n",
    "#     logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "# score = tf.layers.dense(inputs=dropout, units=1,activation = tf.nn.relu)\n",
    "\n",
    "input_y = tf.reshape( tf.placeholder(tf.float32, name=\"input_y\"), [-1,1]);\n",
    "score = tf.layers.dense(inputs=tf.concat([dense,input_y],1), units=1,activation = tf.nn.relu)\n",
    "sign =  tf.layers.dense(inputs=tf.concat([dense,input_y],1), units=1,activation = tf.nn.sigmoid)\n",
    "\n",
    "# tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "# input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "#     advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "rtime = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "#     loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "# loss = tf.reduce_mean( tf.square(score/(tf.abs(rtime)+1) - 1) - tf.sign(sign - tf.sign(rtime)) ); \n",
    "loss = tf.reduce_mean( tf.square(score/(tf.abs(rtime)+1) - 1) - loglik(sign,tf.sign(rtime)/2+0.5) ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 16:47:27,010] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "batchGrad = [];\n",
    "for i,var in enumerate(tvars):\n",
    "    exec('Grad%d'%i + ' = tf.placeholder(tf.float32)');\n",
    "    exec('batchGrad += [Grad%d]'%i)\n",
    "\n",
    "\n",
    "optimiser = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "#     batchGrad = optimiser.compute_gradients(loss,tvars);\n",
    "batchGrad = newGrads;\n",
    "updateGrads = optimiser.apply_gradients(zip(batchGrad,tvars))\n",
    "init = tf.global_variables_initializer();\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "ys=[];byss=[];rss=[];tpreds=[];\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape(None)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xinput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_vars\n",
    "# tf.get_tensor_by\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "INFO:tensorflow:Restoring parameters from Models/signabs_RL_pong_RMSprop.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-03 17:11:16,229] Restoring parameters from Models/signabs_RL_pong_RMSprop.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode 133 reward total was -20.000000. loss_func: 1.201498\n",
      "resetting env. episode 134 reward total was -21.000000. loss_func: 1.088402\n",
      "resetting env. episode 135 reward total was -20.000000. loss_func: 1.284551\n",
      "resetting env. episode 136 reward total was -21.000000. loss_func: 1.099302\n",
      "resetting env. episode 137 reward total was -21.000000. loss_func: 1.089321\n",
      "resetting env. episode 138 reward total was -21.000000. loss_func: 1.072787\n",
      "resetting env. episode 139 reward total was -21.000000. loss_func: 1.060967\n",
      "resetting env. episode 140 reward total was -20.000000. loss_func: 1.180430\n",
      "resetting env. episode 141 reward total was -21.000000. loss_func: 1.062440\n",
      "resetting env. episode 142 reward total was -21.000000. loss_func: 1.057021\n",
      "resetting env. episode 143 reward total was -21.000000. loss_func: 1.048258\n",
      "resetting env. episode 144 reward total was -21.000000. loss_func: 1.067431\n",
      "resetting env. episode 145 reward total was -20.000000. loss_func: 1.159281\n",
      "resetting env. episode 146 reward total was -20.000000. loss_func: 1.160462\n",
      "resetting env. episode 147 reward total was -21.000000. loss_func: 1.039470\n",
      "resetting env. episode 148 reward total was -21.000000. loss_func: 1.050948\n",
      "resetting env. episode 149 reward total was -21.000000. loss_func: 1.054693\n",
      "resetting env. episode 150 reward total was -21.000000. loss_func: 1.021120\n",
      "resetting env. episode 151 reward total was -21.000000. loss_func: 1.033754\n",
      "resetting env. episode 152 reward total was -21.000000. loss_func: 1.043337\n",
      "resetting env. episode 153 reward total was -20.000000. loss_func: 1.147324\n",
      "resetting env. episode 154 reward total was -20.000000. loss_func: 1.151771\n",
      "resetting env. episode 155 reward total was -18.000000. loss_func: 1.413108\n",
      "resetting env. episode 156 reward total was -21.000000. loss_func: 1.013468\n",
      "resetting env. episode 157 reward total was -21.000000. loss_func: 1.010062\n",
      "resetting env. episode 158 reward total was -21.000000. loss_func: 1.024726\n",
      "resetting env. episode 159 reward total was -20.000000. loss_func: 1.174503\n",
      "resetting env. episode 160 reward total was -20.000000. loss_func: 1.124938\n",
      "resetting env. episode 161 reward total was -19.000000. loss_func: 1.243630\n",
      "resetting env. episode 162 reward total was -21.000000. loss_func: 0.997289\n",
      "resetting env. episode 163 reward total was -19.000000. loss_func: 1.235150\n",
      "resetting env. episode 164 reward total was -21.000000. loss_func: 0.994017\n",
      "resetting env. episode 165 reward total was -21.000000. loss_func: 1.000132\n",
      "resetting env. episode 166 reward total was -21.000000. loss_func: 0.977484\n",
      "resetting env. episode 167 reward total was -21.000000. loss_func: 0.973429\n",
      "resetting env. episode 168 reward total was -21.000000. loss_func: 0.980256\n",
      "resetting env. episode 169 reward total was -20.000000. loss_func: 1.115414\n",
      "resetting env. episode 170 reward total was -20.000000. loss_func: 1.216465\n",
      "resetting env. episode 171 reward total was -21.000000. loss_func: 0.996604\n",
      "resetting env. episode 172 reward total was -20.000000. loss_func: 1.110885\n",
      "resetting env. episode 173 reward total was -18.000000. loss_func: 1.451932\n",
      "resetting env. episode 174 reward total was -21.000000. loss_func: 0.990187\n",
      "resetting env. episode 175 reward total was -20.000000. loss_func: 1.099022\n",
      "resetting env. episode 176 reward total was -20.000000. loss_func: 1.103323\n",
      "resetting env. episode 177 reward total was -21.000000. loss_func: 0.937613\n",
      "resetting env. episode 178 reward total was -19.000000. loss_func: 1.201850\n",
      "resetting env. episode 179 reward total was -20.000000. loss_func: 1.215768\n",
      "resetting env. episode 180 reward total was -19.000000. loss_func: 1.184410\n",
      "resetting env. episode 181 reward total was -21.000000. loss_func: 0.985154\n",
      "resetting env. episode 182 reward total was -19.000000. loss_func: 1.220298\n",
      "resetting env. episode 183 reward total was -18.000000. loss_func: 1.327800\n",
      "resetting env. episode 184 reward total was -20.000000. loss_func: 1.243028\n",
      "resetting env. episode 185 reward total was -21.000000. loss_func: 0.974704\n",
      "resetting env. episode 186 reward total was -20.000000. loss_func: 1.089695\n",
      "resetting env. episode 187 reward total was -20.000000. loss_func: 1.084547\n",
      "resetting env. episode 188 reward total was -20.000000. loss_func: 1.089400\n",
      "resetting env. episode 189 reward total was -20.000000. loss_func: 1.136395\n",
      "resetting env. episode 190 reward total was -21.000000. loss_func: 0.924679\n",
      "resetting env. episode 191 reward total was -21.000000. loss_func: 0.953568\n",
      "resetting env. episode 192 reward total was -21.000000. loss_func: 0.941889\n",
      "resetting env. episode 193 reward total was -21.000000. loss_func: 0.939424\n",
      "resetting env. episode 194 reward total was -19.000000. loss_func: 1.193856\n",
      "resetting env. episode 195 reward total was -21.000000. loss_func: 0.930487\n",
      "resetting env. episode 196 reward total was -21.000000. loss_func: 0.940507\n",
      "resetting env. episode 197 reward total was -21.000000. loss_func: 0.904143\n",
      "resetting env. episode 198 reward total was -21.000000. loss_func: 0.930547\n",
      "resetting env. episode 199 reward total was -21.000000. loss_func: 0.910426\n",
      "resetting env. episode 200 reward total was -21.000000. loss_func: 0.931857\n",
      "resetting env. episode 201 reward total was -21.000000. loss_func: 0.897747\n",
      "resetting env. episode 202 reward total was -21.000000. loss_func: 0.894921\n",
      "resetting env. episode 203 reward total was -21.000000. loss_func: 0.890996\n",
      "resetting env. episode 204 reward total was -21.000000. loss_func: 0.933595\n",
      "resetting env. episode 205 reward total was -21.000000. loss_func: 0.912912\n",
      "resetting env. episode 206 reward total was -21.000000. loss_func: 0.905617\n",
      "resetting env. episode 207 reward total was -21.000000. loss_func: 0.893642\n",
      "resetting env. episode 208 reward total was -21.000000. loss_func: 0.909821\n",
      "resetting env. episode 209 reward total was -19.000000. loss_func: 1.170621\n",
      "resetting env. episode 210 reward total was -20.000000. loss_func: 1.060976\n",
      "resetting env. episode 211 reward total was -19.000000. loss_func: 1.194684\n",
      "resetting env. episode 212 reward total was -20.000000. loss_func: 1.055725\n",
      "resetting env. episode 213 reward total was -20.000000. loss_func: 1.047407\n",
      "resetting env. episode 214 reward total was -20.000000. loss_func: 1.108544\n",
      "resetting env. episode 215 reward total was -20.000000. loss_func: 1.042671\n",
      "resetting env. episode 216 reward total was -21.000000. loss_func: 0.902783\n",
      "resetting env. episode 217 reward total was -21.000000. loss_func: 0.907016\n",
      "resetting env. episode 218 reward total was -21.000000. loss_func: 0.876421\n",
      "resetting env. episode 219 reward total was -21.000000. loss_func: 0.838595\n",
      "resetting env. episode 220 reward total was -21.000000. loss_func: 0.888791\n",
      "resetting env. episode 221 reward total was -21.000000. loss_func: 0.877332\n",
      "resetting env. episode 222 reward total was -20.000000. loss_func: 1.265382\n",
      "resetting env. episode 223 reward total was -21.000000. loss_func: 0.863603\n",
      "resetting env. episode 224 reward total was -21.000000. loss_func: 0.871281\n",
      "resetting env. episode 225 reward total was -21.000000. loss_func: 0.871313\n",
      "resetting env. episode 226 reward total was -21.000000. loss_func: 0.895419\n",
      "resetting env. episode 227 reward total was -20.000000. loss_func: 1.039991\n",
      "resetting env. episode 228 reward total was -20.000000. loss_func: 1.028507\n",
      "resetting env. episode 229 reward total was -20.000000. loss_func: 1.030828\n",
      "resetting env. episode 230 reward total was -21.000000. loss_func: 0.817862\n",
      "resetting env. episode 231 reward total was -21.000000. loss_func: 0.814817\n",
      "resetting env. episode 232 reward total was -20.000000. loss_func: 1.023970\n",
      "resetting env. episode 233 reward total was -21.000000. loss_func: 0.856028\n",
      "resetting env. episode 234 reward total was -21.000000. loss_func: 0.868794\n",
      "resetting env. episode 235 reward total was -21.000000. loss_func: 0.862971\n",
      "resetting env. episode 236 reward total was -19.000000. loss_func: 1.192586\n",
      "resetting env. episode 237 reward total was -21.000000. loss_func: 0.836052\n",
      "resetting env. episode 238 reward total was -19.000000. loss_func: 1.144103\n",
      "resetting env. episode 239 reward total was -21.000000. loss_func: 0.824940\n",
      "resetting env. episode 240 reward total was -21.000000. loss_func: 0.818172\n",
      "resetting env. episode 241 reward total was -21.000000. loss_func: 0.845800\n",
      "resetting env. episode 242 reward total was -20.000000. loss_func: 1.016678\n",
      "resetting env. episode 243 reward total was -21.000000. loss_func: 0.814980\n",
      "resetting env. episode 244 reward total was -21.000000. loss_func: 0.878892\n",
      "resetting env. episode 245 reward total was -20.000000. loss_func: 1.009881\n",
      "resetting env. episode 246 reward total was -18.000000. loss_func: 1.268670\n",
      "resetting env. episode 247 reward total was -21.000000. loss_func: 0.857161\n",
      "resetting env. episode 248 reward total was -21.000000. loss_func: 0.807281\n",
      "resetting env. episode 249 reward total was -21.000000. loss_func: 0.834502\n",
      "resetting env. episode 250 reward total was -20.000000. loss_func: 1.014601\n",
      "resetting env. episode 251 reward total was -21.000000. loss_func: 0.830121\n",
      "resetting env. episode 252 reward total was -21.000000. loss_func: 0.825863\n",
      "resetting env. episode 253 reward total was -20.000000. loss_func: 1.014730\n",
      "resetting env. episode 254 reward total was -21.000000. loss_func: 0.846936\n",
      "resetting env. episode 255 reward total was -21.000000. loss_func: 0.833949\n",
      "resetting env. episode 256 reward total was -19.000000. loss_func: 1.139193\n",
      "resetting env. episode 257 reward total was -21.000000. loss_func: 0.863671\n",
      "resetting env. episode 258 reward total was -21.000000. loss_func: 0.836686\n",
      "resetting env. episode 259 reward total was -20.000000. loss_func: 1.146839\n",
      "resetting env. episode 260 reward total was -20.000000. loss_func: 1.005885\n",
      "resetting env. episode 261 reward total was -21.000000. loss_func: 0.816458\n",
      "resetting env. episode 262 reward total was -21.000000. loss_func: 0.833131\n",
      "resetting env. episode 263 reward total was -20.000000. loss_func: 1.358901\n",
      "resetting env. episode 264 reward total was -21.000000. loss_func: 0.873555\n",
      "resetting env. episode 265 reward total was -21.000000. loss_func: 0.814556\n",
      "resetting env. episode 266 reward total was -21.000000. loss_func: 0.781342\n",
      "resetting env. episode 267 reward total was -21.000000. loss_func: 0.812910\n",
      "resetting env. episode 268 reward total was -20.000000. loss_func: 1.152543\n",
      "resetting env. episode 269 reward total was -21.000000. loss_func: 0.785772\n",
      "resetting env. episode 270 reward total was -21.000000. loss_func: 0.777341\n",
      "resetting env. episode 271 reward total was -20.000000. loss_func: 1.065516\n",
      "resetting env. episode 272 reward total was -21.000000. loss_func: 0.806625\n",
      "resetting env. episode 273 reward total was -21.000000. loss_func: 0.815569\n",
      "resetting env. episode 274 reward total was -21.000000. loss_func: 0.785712\n",
      "resetting env. episode 275 reward total was -19.000000. loss_func: 1.161660\n",
      "resetting env. episode 276 reward total was -20.000000. loss_func: 0.997509\n",
      "resetting env. episode 277 reward total was -21.000000. loss_func: 0.823732\n",
      "resetting env. episode 278 reward total was -19.000000. loss_func: 1.143368\n",
      "resetting env. episode 279 reward total was -20.000000. loss_func: 1.152208\n",
      "resetting env. episode 280 reward total was -17.000000. loss_func: 1.375989\n",
      "resetting env. episode 281 reward total was -21.000000. loss_func: 0.837241\n",
      "resetting env. episode 282 reward total was -21.000000. loss_func: 0.824406\n",
      "resetting env. episode 283 reward total was -21.000000. loss_func: 0.798347\n",
      "resetting env. episode 284 reward total was -20.000000. loss_func: 1.142145\n",
      "resetting env. episode 285 reward total was -18.000000. loss_func: 1.246892\n",
      "resetting env. episode 286 reward total was -21.000000. loss_func: 0.800694\n",
      "resetting env. episode 287 reward total was -21.000000. loss_func: 0.820448\n",
      "resetting env. episode 288 reward total was -21.000000. loss_func: 0.798403\n",
      "resetting env. episode 289 reward total was -20.000000. loss_func: 0.990561\n",
      "resetting env. episode 290 reward total was -21.000000. loss_func: 0.796251\n",
      "resetting env. episode 291 reward total was -20.000000. loss_func: 0.976256\n",
      "resetting env. episode 292 reward total was -20.000000. loss_func: 1.131594\n",
      "resetting env. episode 293 reward total was -21.000000. loss_func: 0.803470\n",
      "resetting env. episode 294 reward total was -20.000000. loss_func: 1.038235\n",
      "resetting env. episode 295 reward total was -20.000000. loss_func: 0.982131\n",
      "resetting env. episode 296 reward total was -20.000000. loss_func: 0.969027\n",
      "resetting env. episode 297 reward total was -21.000000. loss_func: 0.736953\n",
      "resetting env. episode 298 reward total was -21.000000. loss_func: 0.770277\n",
      "resetting env. episode 299 reward total was -21.000000. loss_func: 0.733700\n",
      "resetting env. episode 300 reward total was -20.000000. loss_func: 1.107353\n",
      "resetting env. episode 301 reward total was -21.000000. loss_func: 0.838639\n",
      "resetting env. episode 302 reward total was -21.000000. loss_func: 0.794765\n",
      "resetting env. episode 303 reward total was -20.000000. loss_func: 1.144968\n",
      "resetting env. episode 304 reward total was -21.000000. loss_func: 0.822700\n",
      "resetting env. episode 305 reward total was -21.000000. loss_func: 0.803731\n",
      "resetting env. episode 306 reward total was -20.000000. loss_func: 0.971412\n",
      "resetting env. episode 307 reward total was -18.000000. loss_func: 1.240073\n",
      "resetting env. episode 308 reward total was -20.000000. loss_func: 0.964827\n",
      "resetting env. episode 309 reward total was -20.000000. loss_func: 1.051044\n",
      "resetting env. episode 310 reward total was -19.000000. loss_func: 1.127067\n",
      "resetting env. episode 311 reward total was -20.000000. loss_func: 0.960101\n",
      "resetting env. episode 312 reward total was -20.000000. loss_func: 0.961664\n",
      "resetting env. episode 313 reward total was -19.000000. loss_func: 1.230763\n",
      "resetting env. episode 314 reward total was -21.000000. loss_func: 0.791927\n",
      "resetting env. episode 315 reward total was -20.000000. loss_func: 0.961524\n",
      "resetting env. episode 316 reward total was -21.000000. loss_func: 0.785854\n",
      "resetting env. episode 317 reward total was -21.000000. loss_func: 0.778577\n",
      "resetting env. episode 318 reward total was -20.000000. loss_func: 1.120464\n",
      "resetting env. episode 319 reward total was -21.000000. loss_func: 0.755473\n",
      "resetting env. episode 320 reward total was -21.000000. loss_func: 0.782018\n",
      "resetting env. episode 321 reward total was -21.000000. loss_func: 0.820226\n",
      "resetting env. episode 322 reward total was -20.000000. loss_func: 1.098908\n",
      "resetting env. episode 323 reward total was -21.000000. loss_func: 0.840469\n",
      "resetting env. episode 324 reward total was -21.000000. loss_func: 0.756314\n",
      "resetting env. episode 325 reward total was -21.000000. loss_func: 0.815348\n",
      "resetting env. episode 326 reward total was -21.000000. loss_func: 0.811877\n",
      "resetting env. episode 327 reward total was -21.000000. loss_func: 0.751491\n",
      "resetting env. episode 328 reward total was -19.000000. loss_func: 1.592440\n",
      "resetting env. episode 329 reward total was -20.000000. loss_func: 0.968319\n",
      "resetting env. episode 330 reward total was -21.000000. loss_func: 0.727843\n",
      "resetting env. episode 331 reward total was -20.000000. loss_func: 0.962300\n",
      "resetting env. episode 332 reward total was -21.000000. loss_func: 0.761971\n",
      "resetting env. episode 333 reward total was -21.000000. loss_func: 0.726882\n",
      "resetting env. episode 334 reward total was -21.000000. loss_func: 0.743528\n",
      "resetting env. episode 335 reward total was -21.000000. loss_func: 0.729010\n",
      "resetting env. episode 336 reward total was -21.000000. loss_func: 0.829132\n",
      "resetting env. episode 337 reward total was -20.000000. loss_func: 0.957488\n",
      "resetting env. episode 338 reward total was -21.000000. loss_func: 0.720862\n",
      "resetting env. episode 339 reward total was -21.000000. loss_func: 0.771511\n",
      "resetting env. episode 340 reward total was -21.000000. loss_func: 0.772574\n",
      "resetting env. episode 341 reward total was -20.000000. loss_func: 0.952143\n",
      "resetting env. episode 342 reward total was -21.000000. loss_func: 0.792848\n",
      "resetting env. episode 343 reward total was -20.000000. loss_func: 0.959577\n",
      "resetting env. episode 344 reward total was -21.000000. loss_func: 0.736684\n",
      "resetting env. episode 345 reward total was -21.000000. loss_func: 0.766880\n",
      "resetting env. episode 346 reward total was -21.000000. loss_func: 0.731931\n",
      "resetting env. episode 347 reward total was -21.000000. loss_func: 0.766026\n",
      "resetting env. episode 348 reward total was -21.000000. loss_func: 0.763931\n",
      "resetting env. episode 349 reward total was -21.000000. loss_func: 0.773399\n",
      "resetting env. episode 350 reward total was -21.000000. loss_func: 0.805563\n",
      "resetting env. episode 351 reward total was -20.000000. loss_func: 0.936255\n",
      "resetting env. episode 352 reward total was -21.000000. loss_func: 0.763644\n",
      "resetting env. episode 353 reward total was -21.000000. loss_func: 0.706059\n",
      "resetting env. episode 354 reward total was -21.000000. loss_func: 0.686131\n",
      "resetting env. episode 355 reward total was -21.000000. loss_func: 0.740337\n",
      "resetting env. episode 356 reward total was -20.000000. loss_func: 0.945611\n",
      "resetting env. episode 357 reward total was -21.000000. loss_func: 0.677953\n",
      "resetting env. episode 358 reward total was -21.000000. loss_func: 0.742621\n",
      "resetting env. episode 359 reward total was -21.000000. loss_func: 0.710583\n",
      "resetting env. episode 360 reward total was -21.000000. loss_func: 0.723907\n",
      "resetting env. episode 361 reward total was -21.000000. loss_func: 0.795704\n",
      "resetting env. episode 362 reward total was -21.000000. loss_func: 0.757422\n",
      "resetting env. episode 363 reward total was -21.000000. loss_func: 0.750433\n",
      "resetting env. episode 364 reward total was -21.000000. loss_func: 0.677646\n",
      "resetting env. episode 365 reward total was -20.000000. loss_func: 0.945805\n",
      "resetting env. episode 366 reward total was -20.000000. loss_func: 0.935228\n",
      "resetting env. episode 367 reward total was -21.000000. loss_func: 0.763239\n",
      "resetting env. episode 368 reward total was -21.000000. loss_func: 0.740047\n",
      "resetting env. episode 369 reward total was -19.000000. loss_func: 1.329549\n",
      "resetting env. episode 370 reward total was -21.000000. loss_func: 0.742712\n",
      "resetting env. episode 371 reward total was -21.000000. loss_func: 0.709699\n",
      "resetting env. episode 372 reward total was -20.000000. loss_func: 0.931702\n",
      "resetting env. episode 373 reward total was -21.000000. loss_func: 0.738675\n",
      "resetting env. episode 374 reward total was -21.000000. loss_func: 0.740836\n",
      "resetting env. episode 375 reward total was -21.000000. loss_func: 0.694771\n",
      "resetting env. episode 376 reward total was -21.000000. loss_func: 0.711171\n",
      "resetting env. episode 377 reward total was -21.000000. loss_func: 0.738371\n",
      "resetting env. episode 378 reward total was -17.000000. loss_func: 1.453104\n",
      "resetting env. episode 379 reward total was -21.000000. loss_func: 0.737173\n",
      "resetting env. episode 380 reward total was -21.000000. loss_func: 0.700828\n",
      "resetting env. episode 381 reward total was -20.000000. loss_func: 0.937889\n",
      "resetting env. episode 382 reward total was -20.000000. loss_func: 0.940721\n",
      "resetting env. episode 383 reward total was -21.000000. loss_func: 0.772189\n",
      "resetting env. episode 384 reward total was -21.000000. loss_func: 0.704455\n",
      "resetting env. episode 385 reward total was -21.000000. loss_func: 0.686467\n",
      "resetting env. episode 386 reward total was -20.000000. loss_func: 1.124266\n",
      "resetting env. episode 387 reward total was -21.000000. loss_func: 0.747030\n",
      "resetting env. episode 388 reward total was -21.000000. loss_func: 0.745706\n",
      "resetting env. episode 389 reward total was -21.000000. loss_func: 0.695878\n",
      "resetting env. episode 390 reward total was -21.000000. loss_func: 0.705566\n",
      "resetting env. episode 391 reward total was -21.000000. loss_func: 0.772943\n",
      "resetting env. episode 392 reward total was -21.000000. loss_func: 0.712781\n",
      "resetting env. episode 393 reward total was -20.000000. loss_func: 0.942183\n",
      "resetting env. episode 394 reward total was -19.000000. loss_func: 1.105598\n",
      "resetting env. episode 395 reward total was -21.000000. loss_func: 0.677348\n",
      "resetting env. episode 396 reward total was -20.000000. loss_func: 0.934054\n",
      "resetting env. episode 397 reward total was -19.000000. loss_func: 1.259984\n",
      "resetting env. episode 398 reward total was -21.000000. loss_func: 0.700459\n",
      "resetting env. episode 399 reward total was -20.000000. loss_func: 1.027714\n",
      "resetting env. episode 400 reward total was -21.000000. loss_func: 0.745560\n",
      "resetting env. episode 401 reward total was -20.000000. loss_func: 0.933860\n",
      "resetting env. episode 402 reward total was -20.000000. loss_func: 1.093285\n",
      "resetting env. episode 403 reward total was -21.000000. loss_func: 0.698426\n",
      "resetting env. episode 404 reward total was -21.000000. loss_func: 0.733813\n",
      "resetting env. episode 405 reward total was -20.000000. loss_func: 1.129769\n",
      "resetting env. episode 406 reward total was -21.000000. loss_func: 0.701464\n",
      "resetting env. episode 407 reward total was -20.000000. loss_func: 0.924855\n",
      "resetting env. episode 408 reward total was -21.000000. loss_func: 0.725203\n",
      "resetting env. episode 409 reward total was -21.000000. loss_func: 0.770513\n",
      "resetting env. episode 410 reward total was -21.000000. loss_func: 0.726605\n",
      "resetting env. episode 411 reward total was -21.000000. loss_func: 0.751835\n",
      "resetting env. episode 412 reward total was -21.000000. loss_func: 0.720859\n",
      "resetting env. episode 413 reward total was -20.000000. loss_func: 1.108627\n",
      "resetting env. episode 414 reward total was -19.000000. loss_func: 1.200812\n",
      "resetting env. episode 415 reward total was -21.000000. loss_func: 0.716560\n",
      "resetting env. episode 416 reward total was -21.000000. loss_func: 0.720783\n",
      "resetting env. episode 417 reward total was -20.000000. loss_func: 1.383778\n",
      "resetting env. episode 418 reward total was -21.000000. loss_func: 0.785932\n",
      "resetting env. episode 419 reward total was -19.000000. loss_func: 1.338848\n",
      "resetting env. episode 420 reward total was -21.000000. loss_func: 0.720099\n",
      "resetting env. episode 421 reward total was -19.000000. loss_func: 1.074228\n",
      "resetting env. episode 422 reward total was -20.000000. loss_func: 0.921210\n",
      "resetting env. episode 423 reward total was -20.000000. loss_func: 1.115876\n",
      "resetting env. episode 424 reward total was -21.000000. loss_func: 0.714350\n",
      "resetting env. episode 425 reward total was -19.000000. loss_func: 1.136173\n",
      "resetting env. episode 426 reward total was -21.000000. loss_func: 0.719194\n",
      "resetting env. episode 427 reward total was -21.000000. loss_func: 0.690965\n",
      "resetting env. episode 428 reward total was -21.000000. loss_func: 0.739002\n",
      "resetting env. episode 429 reward total was -21.000000. loss_func: 0.741397\n",
      "resetting env. episode 430 reward total was -20.000000. loss_func: 1.245012\n",
      "resetting env. episode 431 reward total was -21.000000. loss_func: 0.710651\n",
      "resetting env. episode 432 reward total was -20.000000. loss_func: 1.335927\n",
      "resetting env. episode 433 reward total was -21.000000. loss_func: 0.731026\n",
      "resetting env. episode 434 reward total was -19.000000. loss_func: 1.108335\n",
      "resetting env. episode 435 reward total was -21.000000. loss_func: 0.752460\n",
      "resetting env. episode 436 reward total was -19.000000. loss_func: 1.072341\n",
      "resetting env. episode 437 reward total was -21.000000. loss_func: 0.717446\n",
      "resetting env. episode 438 reward total was -21.000000. loss_func: 0.726445\n",
      "resetting env. episode 439 reward total was -19.000000. loss_func: 1.183687\n",
      "resetting env. episode 440 reward total was -21.000000. loss_func: 0.703140\n",
      "resetting env. episode 441 reward total was -20.000000. loss_func: 0.924824\n",
      "resetting env. episode 442 reward total was -21.000000. loss_func: 0.698232\n",
      "resetting env. episode 443 reward total was -19.000000. loss_func: 1.168205\n",
      "resetting env. episode 444 reward total was -21.000000. loss_func: 0.672127\n",
      "resetting env. episode 445 reward total was -20.000000. loss_func: 0.940116\n",
      "resetting env. episode 446 reward total was -21.000000. loss_func: 0.787601\n",
      "resetting env. episode 447 reward total was -21.000000. loss_func: 0.710136\n",
      "resetting env. episode 448 reward total was -21.000000. loss_func: 0.699418\n",
      "resetting env. episode 449 reward total was -20.000000. loss_func: 0.918404\n",
      "resetting env. episode 450 reward total was -21.000000. loss_func: 0.721969\n",
      "resetting env. episode 451 reward total was -21.000000. loss_func: 0.666591\n",
      "resetting env. episode 452 reward total was -21.000000. loss_func: 0.759701\n",
      "resetting env. episode 453 reward total was -20.000000. loss_func: 0.920389\n",
      "resetting env. episode 454 reward total was -17.000000. loss_func: 1.643503\n",
      "resetting env. episode 455 reward total was -21.000000. loss_func: 0.704898\n",
      "resetting env. episode 456 reward total was -21.000000. loss_func: 0.697838\n",
      "resetting env. episode 457 reward total was -18.000000. loss_func: 1.300598\n",
      "resetting env. episode 458 reward total was -21.000000. loss_func: 0.706589\n",
      "resetting env. episode 459 reward total was -20.000000. loss_func: 0.909885\n",
      "resetting env. episode 460 reward total was -19.000000. loss_func: 1.092904\n",
      "resetting env. episode 461 reward total was -20.000000. loss_func: 0.911819\n",
      "resetting env. episode 462 reward total was -21.000000. loss_func: 0.717619\n",
      "resetting env. episode 463 reward total was -21.000000. loss_func: 0.715316\n",
      "resetting env. episode 464 reward total was -21.000000. loss_func: 0.734307\n",
      "resetting env. episode 465 reward total was -21.000000. loss_func: 0.696506\n",
      "resetting env. episode 466 reward total was -20.000000. loss_func: 0.920923\n",
      "resetting env. episode 467 reward total was -21.000000. loss_func: 0.720311\n",
      "resetting env. episode 468 reward total was -20.000000. loss_func: 0.919413\n",
      "resetting env. episode 469 reward total was -20.000000. loss_func: 1.066388\n",
      "resetting env. episode 470 reward total was -21.000000. loss_func: 0.729659\n",
      "resetting env. episode 471 reward total was -21.000000. loss_func: 0.699535\n",
      "resetting env. episode 472 reward total was -21.000000. loss_func: 0.685542\n",
      "resetting env. episode 473 reward total was -18.000000. loss_func: 1.588152\n",
      "resetting env. episode 474 reward total was -21.000000. loss_func: 0.698714\n",
      "resetting env. episode 475 reward total was -20.000000. loss_func: 0.919077\n",
      "resetting env. episode 476 reward total was -20.000000. loss_func: 0.899804\n",
      "resetting env. episode 477 reward total was -21.000000. loss_func: 0.681702\n",
      "resetting env. episode 478 reward total was -21.000000. loss_func: 0.723326\n",
      "resetting env. episode 479 reward total was -20.000000. loss_func: 0.906196\n",
      "resetting env. episode 480 reward total was -21.000000. loss_func: 0.693776\n",
      "resetting env. episode 481 reward total was -21.000000. loss_func: 0.689606\n",
      "resetting env. episode 482 reward total was -21.000000. loss_func: 0.731391\n",
      "resetting env. episode 483 reward total was -21.000000. loss_func: 0.703807\n",
      "resetting env. episode 484 reward total was -20.000000. loss_func: 0.927035\n",
      "resetting env. episode 485 reward total was -21.000000. loss_func: 0.729404\n",
      "resetting env. episode 486 reward total was -21.000000. loss_func: 0.732805\n",
      "resetting env. episode 487 reward total was -21.000000. loss_func: 0.713928\n",
      "resetting env. episode 488 reward total was -21.000000. loss_func: 0.725453\n",
      "resetting env. episode 489 reward total was -20.000000. loss_func: 0.926951\n",
      "resetting env. episode 490 reward total was -21.000000. loss_func: 0.686061\n",
      "resetting env. episode 491 reward total was -21.000000. loss_func: 0.738597\n",
      "resetting env. episode 492 reward total was -21.000000. loss_func: 0.673055\n",
      "resetting env. episode 493 reward total was -21.000000. loss_func: 0.661746\n",
      "resetting env. episode 494 reward total was -21.000000. loss_func: 0.716341\n",
      "resetting env. episode 495 reward total was -21.000000. loss_func: 0.706574\n",
      "resetting env. episode 496 reward total was -20.000000. loss_func: 0.919912\n",
      "resetting env. episode 497 reward total was -21.000000. loss_func: 0.705347\n",
      "resetting env. episode 498 reward total was -21.000000. loss_func: 0.662440\n",
      "resetting env. episode 499 reward total was -21.000000. loss_func: 0.639176\n",
      "resetting env. episode 500 reward total was -20.000000. loss_func: 0.923275\n",
      "resetting env. episode 501 reward total was -21.000000. loss_func: 0.686656\n",
      "resetting env. episode 502 reward total was -20.000000. loss_func: 0.922647\n",
      "resetting env. episode 503 reward total was -21.000000. loss_func: 0.707825\n",
      "resetting env. episode 504 reward total was -18.000000. loss_func: 1.361638\n",
      "resetting env. episode 505 reward total was -20.000000. loss_func: 0.921729\n",
      "resetting env. episode 506 reward total was -21.000000. loss_func: 0.692915\n",
      "resetting env. episode 507 reward total was -20.000000. loss_func: 0.935236\n",
      "resetting env. episode 508 reward total was -19.000000. loss_func: 1.226512\n",
      "resetting env. episode 509 reward total was -21.000000. loss_func: 0.690951\n",
      "resetting env. episode 510 reward total was -20.000000. loss_func: 0.910850\n",
      "resetting env. episode 511 reward total was -21.000000. loss_func: 0.678214\n",
      "resetting env. episode 512 reward total was -20.000000. loss_func: 0.921501\n",
      "resetting env. episode 513 reward total was -21.000000. loss_func: 0.660853\n",
      "resetting env. episode 514 reward total was -21.000000. loss_func: 0.705115\n",
      "resetting env. episode 515 reward total was -20.000000. loss_func: 1.059911\n",
      "resetting env. episode 516 reward total was -20.000000. loss_func: 0.909792\n",
      "resetting env. episode 517 reward total was -20.000000. loss_func: 1.072271\n",
      "resetting env. episode 518 reward total was -19.000000. loss_func: 1.109432\n",
      "resetting env. episode 519 reward total was -19.000000. loss_func: 1.095387\n",
      "resetting env. episode 520 reward total was -21.000000. loss_func: 0.682216\n",
      "resetting env. episode 521 reward total was -18.000000. loss_func: 1.223786\n",
      "resetting env. episode 522 reward total was -20.000000. loss_func: 1.076833\n",
      "resetting env. episode 523 reward total was -21.000000. loss_func: 0.716611\n",
      "resetting env. episode 524 reward total was -20.000000. loss_func: 0.918038\n",
      "resetting env. episode 525 reward total was -21.000000. loss_func: 0.694898\n",
      "resetting env. episode 526 reward total was -21.000000. loss_func: 0.712495\n",
      "resetting env. episode 527 reward total was -21.000000. loss_func: 0.710152\n",
      "resetting env. episode 528 reward total was -21.000000. loss_func: 0.735753\n",
      "resetting env. episode 529 reward total was -20.000000. loss_func: 0.905112\n",
      "resetting env. episode 530 reward total was -19.000000. loss_func: 1.105893\n",
      "resetting env. episode 531 reward total was -21.000000. loss_func: 0.727306\n",
      "resetting env. episode 532 reward total was -20.000000. loss_func: 0.904151\n",
      "resetting env. episode 533 reward total was -20.000000. loss_func: 0.914898\n",
      "resetting env. episode 534 reward total was -20.000000. loss_func: 0.896984\n",
      "resetting env. episode 535 reward total was -21.000000. loss_func: 0.706795\n",
      "resetting env. episode 536 reward total was -21.000000. loss_func: 0.705063\n",
      "resetting env. episode 537 reward total was -21.000000. loss_func: 0.728964\n",
      "resetting env. episode 538 reward total was -21.000000. loss_func: 0.724716\n",
      "resetting env. episode 539 reward total was -21.000000. loss_func: 0.672281\n",
      "resetting env. episode 540 reward total was -19.000000. loss_func: 1.164882\n",
      "resetting env. episode 541 reward total was -21.000000. loss_func: 0.684528\n",
      "resetting env. episode 542 reward total was -21.000000. loss_func: 0.713337\n",
      "resetting env. episode 543 reward total was -21.000000. loss_func: 0.689717\n",
      "resetting env. episode 544 reward total was -21.000000. loss_func: 0.684186\n",
      "resetting env. episode 545 reward total was -21.000000. loss_func: 0.707053\n",
      "resetting env. episode 546 reward total was -20.000000. loss_func: 0.917758\n",
      "resetting env. episode 547 reward total was -18.000000. loss_func: 1.429852\n",
      "resetting env. episode 548 reward total was -21.000000. loss_func: 0.653799\n",
      "resetting env. episode 549 reward total was -19.000000. loss_func: 1.063513\n",
      "resetting env. episode 550 reward total was -20.000000. loss_func: 0.911721\n",
      "resetting env. episode 551 reward total was -20.000000. loss_func: 1.001058\n",
      "resetting env. episode 552 reward total was -20.000000. loss_func: 0.929099\n",
      "resetting env. episode 553 reward total was -21.000000. loss_func: 0.682444\n",
      "resetting env. episode 554 reward total was -21.000000. loss_func: 0.686578\n",
      "resetting env. episode 555 reward total was -21.000000. loss_func: 0.719147\n",
      "resetting env. episode 556 reward total was -19.000000. loss_func: 1.192120\n",
      "resetting env. episode 557 reward total was -21.000000. loss_func: 0.683149\n",
      "resetting env. episode 558 reward total was -21.000000. loss_func: 0.688440\n",
      "resetting env. episode 559 reward total was -21.000000. loss_func: 0.744765\n",
      "resetting env. episode 560 reward total was -21.000000. loss_func: 0.680176\n",
      "resetting env. episode 561 reward total was -20.000000. loss_func: 0.919959\n",
      "resetting env. episode 562 reward total was -21.000000. loss_func: 0.729786\n",
      "resetting env. episode 563 reward total was -21.000000. loss_func: 0.716489\n",
      "resetting env. episode 564 reward total was -20.000000. loss_func: 0.902085\n",
      "resetting env. episode 565 reward total was -21.000000. loss_func: 0.685745\n",
      "resetting env. episode 566 reward total was -21.000000. loss_func: 0.703443\n",
      "resetting env. episode 567 reward total was -21.000000. loss_func: 0.683258\n",
      "resetting env. episode 568 reward total was -21.000000. loss_func: 0.695191\n",
      "resetting env. episode 569 reward total was -20.000000. loss_func: 0.922054\n",
      "resetting env. episode 570 reward total was -20.000000. loss_func: 0.909739\n",
      "resetting env. episode 571 reward total was -20.000000. loss_func: 0.894523\n",
      "resetting env. episode 572 reward total was -19.000000. loss_func: 1.069789\n",
      "resetting env. episode 573 reward total was -20.000000. loss_func: 0.908455\n",
      "resetting env. episode 574 reward total was -19.000000. loss_func: 1.092971\n",
      "resetting env. episode 575 reward total was -21.000000. loss_func: 0.716145\n",
      "resetting env. episode 576 reward total was -18.000000. loss_func: 1.209712\n",
      "resetting env. episode 577 reward total was -21.000000. loss_func: 0.669571\n",
      "resetting env. episode 578 reward total was -21.000000. loss_func: 0.724936\n",
      "resetting env. episode 579 reward total was -21.000000. loss_func: 0.701425\n",
      "resetting env. episode 580 reward total was -19.000000. loss_func: 1.078488\n",
      "resetting env. episode 581 reward total was -21.000000. loss_func: 0.666280\n",
      "resetting env. episode 582 reward total was -20.000000. loss_func: 0.898946\n",
      "resetting env. episode 583 reward total was -20.000000. loss_func: 1.096851\n",
      "resetting env. episode 584 reward total was -20.000000. loss_func: 0.895209\n",
      "resetting env. episode 585 reward total was -20.000000. loss_func: 1.000794\n",
      "resetting env. episode 586 reward total was -21.000000. loss_func: 0.702918\n",
      "resetting env. episode 587 reward total was -19.000000. loss_func: 1.057715\n",
      "resetting env. episode 588 reward total was -21.000000. loss_func: 0.711829\n",
      "resetting env. episode 589 reward total was -21.000000. loss_func: 0.711132\n",
      "resetting env. episode 590 reward total was -21.000000. loss_func: 0.717490\n",
      "resetting env. episode 591 reward total was -21.000000. loss_func: 0.707927\n",
      "resetting env. episode 592 reward total was -20.000000. loss_func: 0.912988\n",
      "resetting env. episode 593 reward total was -21.000000. loss_func: 0.719058\n",
      "resetting env. episode 594 reward total was -20.000000. loss_func: 0.906045\n",
      "resetting env. episode 595 reward total was -20.000000. loss_func: 1.004009\n",
      "resetting env. episode 596 reward total was -21.000000. loss_func: 0.651964\n",
      "resetting env. episode 597 reward total was -20.000000. loss_func: 0.894369\n",
      "resetting env. episode 598 reward total was -21.000000. loss_func: 0.666764\n",
      "resetting env. episode 599 reward total was -20.000000. loss_func: 0.910529\n",
      "resetting env. episode 600 reward total was -20.000000. loss_func: 0.907382\n",
      "resetting env. episode 601 reward total was -21.000000. loss_func: 0.724792\n",
      "resetting env. episode 602 reward total was -20.000000. loss_func: 0.904706\n",
      "resetting env. episode 603 reward total was -19.000000. loss_func: 1.087866\n",
      "resetting env. episode 604 reward total was -21.000000. loss_func: 0.702054\n",
      "resetting env. episode 605 reward total was -20.000000. loss_func: 0.913653\n",
      "resetting env. episode 606 reward total was -21.000000. loss_func: 0.701258\n",
      "resetting env. episode 607 reward total was -21.000000. loss_func: 0.701566\n",
      "resetting env. episode 608 reward total was -21.000000. loss_func: 0.700834\n",
      "resetting env. episode 609 reward total was -19.000000. loss_func: 1.071764\n",
      "resetting env. episode 610 reward total was -21.000000. loss_func: 0.702049\n",
      "resetting env. episode 611 reward total was -21.000000. loss_func: 0.663599\n",
      "resetting env. episode 612 reward total was -20.000000. loss_func: 0.905668\n",
      "resetting env. episode 613 reward total was -21.000000. loss_func: 0.690560\n",
      "resetting env. episode 614 reward total was -21.000000. loss_func: 0.682148\n",
      "resetting env. episode 615 reward total was -21.000000. loss_func: 0.680148\n",
      "resetting env. episode 616 reward total was -21.000000. loss_func: 0.685250\n",
      "resetting env. episode 617 reward total was -21.000000. loss_func: 0.681550\n",
      "resetting env. episode 618 reward total was -21.000000. loss_func: 0.706407\n",
      "resetting env. episode 619 reward total was -20.000000. loss_func: 1.004661\n",
      "resetting env. episode 620 reward total was -21.000000. loss_func: 0.720396\n",
      "resetting env. episode 621 reward total was -21.000000. loss_func: 0.687295\n",
      "resetting env. episode 622 reward total was -21.000000. loss_func: 0.665449\n",
      "resetting env. episode 623 reward total was -21.000000. loss_func: 0.661871\n",
      "resetting env. episode 624 reward total was -21.000000. loss_func: 0.709850\n",
      "resetting env. episode 625 reward total was -21.000000. loss_func: 0.657285\n",
      "resetting env. episode 626 reward total was -19.000000. loss_func: 1.075833\n",
      "resetting env. episode 627 reward total was -20.000000. loss_func: 0.906778\n",
      "resetting env. episode 628 reward total was -19.000000. loss_func: 1.111113\n",
      "resetting env. episode 629 reward total was -21.000000. loss_func: 0.706530\n",
      "resetting env. episode 630 reward total was -21.000000. loss_func: 0.720819\n",
      "resetting env. episode 631 reward total was -21.000000. loss_func: 0.659816\n",
      "resetting env. episode 632 reward total was -20.000000. loss_func: 0.906510\n",
      "resetting env. episode 633 reward total was -21.000000. loss_func: 0.706601\n",
      "resetting env. episode 634 reward total was -20.000000. loss_func: 0.901663\n",
      "resetting env. episode 635 reward total was -21.000000. loss_func: 0.727959\n",
      "resetting env. episode 636 reward total was -21.000000. loss_func: 0.679696\n",
      "resetting env. episode 637 reward total was -20.000000. loss_func: 1.113071\n",
      "resetting env. episode 638 reward total was -21.000000. loss_func: 0.690222\n",
      "resetting env. episode 639 reward total was -21.000000. loss_func: 0.677155\n",
      "resetting env. episode 640 reward total was -21.000000. loss_func: 0.678125\n",
      "resetting env. episode 641 reward total was -20.000000. loss_func: 0.910298\n",
      "resetting env. episode 642 reward total was -21.000000. loss_func: 0.641070\n",
      "resetting env. episode 643 reward total was -20.000000. loss_func: 1.135458\n",
      "resetting env. episode 644 reward total was -21.000000. loss_func: 0.691348\n",
      "resetting env. episode 645 reward total was -20.000000. loss_func: 0.905973\n",
      "resetting env. episode 646 reward total was -21.000000. loss_func: 0.676586\n",
      "resetting env. episode 647 reward total was -21.000000. loss_func: 0.694889\n",
      "resetting env. episode 648 reward total was -21.000000. loss_func: 0.743977\n",
      "resetting env. episode 649 reward total was -21.000000. loss_func: 0.699607\n",
      "resetting env. episode 650 reward total was -21.000000. loss_func: 0.681020\n",
      "resetting env. episode 651 reward total was -21.000000. loss_func: 0.681526\n",
      "resetting env. episode 652 reward total was -21.000000. loss_func: 0.708515\n",
      "resetting env. episode 653 reward total was -21.000000. loss_func: 0.728200\n",
      "resetting env. episode 654 reward total was -21.000000. loss_func: 0.657368\n",
      "resetting env. episode 655 reward total was -21.000000. loss_func: 0.658434\n",
      "resetting env. episode 656 reward total was -21.000000. loss_func: 0.690543\n",
      "resetting env. episode 657 reward total was -21.000000. loss_func: 0.659922\n",
      "resetting env. episode 658 reward total was -19.000000. loss_func: 1.220583\n",
      "resetting env. episode 659 reward total was -21.000000. loss_func: 0.698811\n",
      "resetting env. episode 660 reward total was -21.000000. loss_func: 0.699703\n",
      "resetting env. episode 661 reward total was -18.000000. loss_func: 1.264692\n",
      "resetting env. episode 662 reward total was -21.000000. loss_func: 0.714061\n",
      "resetting env. episode 663 reward total was -21.000000. loss_func: 0.690055\n",
      "resetting env. episode 664 reward total was -20.000000. loss_func: 0.901988\n",
      "resetting env. episode 665 reward total was -21.000000. loss_func: 0.649491\n",
      "resetting env. episode 666 reward total was -21.000000. loss_func: 0.676322\n",
      "resetting env. episode 667 reward total was -18.000000. loss_func: 1.541317\n",
      "resetting env. episode 668 reward total was -18.000000. loss_func: 1.264018\n",
      "resetting env. episode 669 reward total was -21.000000. loss_func: 0.729867\n",
      "resetting env. episode 670 reward total was -20.000000. loss_func: 0.906039\n",
      "resetting env. episode 671 reward total was -21.000000. loss_func: 0.720586\n",
      "resetting env. episode 672 reward total was -21.000000. loss_func: 0.718719\n",
      "resetting env. episode 673 reward total was -18.000000. loss_func: 1.276808\n",
      "resetting env. episode 674 reward total was -20.000000. loss_func: 0.905200\n",
      "resetting env. episode 675 reward total was -20.000000. loss_func: 1.007958\n",
      "resetting env. episode 676 reward total was -19.000000. loss_func: 1.112237\n",
      "resetting env. episode 677 reward total was -21.000000. loss_func: 0.678377\n",
      "resetting env. episode 678 reward total was -21.000000. loss_func: 0.688895\n",
      "resetting env. episode 679 reward total was -21.000000. loss_func: 0.713373\n",
      "resetting env. episode 680 reward total was -20.000000. loss_func: 0.900573\n",
      "resetting env. episode 681 reward total was -20.000000. loss_func: 0.899479\n",
      "resetting env. episode 682 reward total was -21.000000. loss_func: 0.722003\n",
      "resetting env. episode 683 reward total was -20.000000. loss_func: 0.903524\n",
      "resetting env. episode 684 reward total was -19.000000. loss_func: 1.080303\n",
      "resetting env. episode 685 reward total was -21.000000. loss_func: 0.705385\n",
      "resetting env. episode 686 reward total was -21.000000. loss_func: 0.693090\n",
      "resetting env. episode 687 reward total was -21.000000. loss_func: 0.655437\n",
      "resetting env. episode 688 reward total was -21.000000. loss_func: 0.640691\n",
      "resetting env. episode 689 reward total was -20.000000. loss_func: 0.996495\n",
      "resetting env. episode 690 reward total was -21.000000. loss_func: 0.707334\n",
      "resetting env. episode 691 reward total was -20.000000. loss_func: 0.910590\n",
      "resetting env. episode 692 reward total was -21.000000. loss_func: 0.677189\n",
      "resetting env. episode 693 reward total was -21.000000. loss_func: 0.677821\n",
      "resetting env. episode 694 reward total was -21.000000. loss_func: 0.725599\n",
      "resetting env. episode 695 reward total was -21.000000. loss_func: 0.639290\n",
      "resetting env. episode 696 reward total was -20.000000. loss_func: 0.916619\n",
      "resetting env. episode 697 reward total was -21.000000. loss_func: 0.714978\n",
      "resetting env. episode 698 reward total was -21.000000. loss_func: 0.658250\n",
      "resetting env. episode 699 reward total was -21.000000. loss_func: 0.705502\n",
      "resetting env. episode 700 reward total was -19.000000. loss_func: 1.215476\n",
      "resetting env. episode 701 reward total was -20.000000. loss_func: 0.914827\n",
      "resetting env. episode 702 reward total was -21.000000. loss_func: 0.686895\n",
      "resetting env. episode 703 reward total was -21.000000. loss_func: 0.706463\n",
      "resetting env. episode 704 reward total was -21.000000. loss_func: 0.697995\n",
      "resetting env. episode 705 reward total was -21.000000. loss_func: 0.661537\n",
      "resetting env. episode 706 reward total was -20.000000. loss_func: 0.920022\n",
      "resetting env. episode 707 reward total was -21.000000. loss_func: 0.673920\n",
      "resetting env. episode 708 reward total was -21.000000. loss_func: 0.704412\n",
      "resetting env. episode 709 reward total was -21.000000. loss_func: 0.670796\n",
      "resetting env. episode 710 reward total was -21.000000. loss_func: 0.687409\n",
      "resetting env. episode 711 reward total was -21.000000. loss_func: 0.657169\n",
      "resetting env. episode 712 reward total was -21.000000. loss_func: 0.693628\n",
      "resetting env. episode 713 reward total was -21.000000. loss_func: 0.658542\n",
      "resetting env. episode 714 reward total was -20.000000. loss_func: 0.916914\n",
      "resetting env. episode 715 reward total was -21.000000. loss_func: 0.651871\n",
      "resetting env. episode 716 reward total was -21.000000. loss_func: 0.710330\n",
      "resetting env. episode 717 reward total was -20.000000. loss_func: 0.919178\n",
      "resetting env. episode 718 reward total was -21.000000. loss_func: 0.654749\n",
      "resetting env. episode 719 reward total was -20.000000. loss_func: 1.086113\n",
      "resetting env. episode 720 reward total was -20.000000. loss_func: 0.922966\n",
      "resetting env. episode 721 reward total was -21.000000. loss_func: 0.675344\n",
      "resetting env. episode 722 reward total was -21.000000. loss_func: 0.648698\n",
      "resetting env. episode 723 reward total was -21.000000. loss_func: 0.677801\n",
      "resetting env. episode 724 reward total was -21.000000. loss_func: 0.654871\n",
      "resetting env. episode 725 reward total was -19.000000. loss_func: 1.288986\n",
      "resetting env. episode 726 reward total was -19.000000. loss_func: 1.064646\n",
      "resetting env. episode 727 reward total was -21.000000. loss_func: 0.656404\n",
      "resetting env. episode 728 reward total was -21.000000. loss_func: 0.688115\n",
      "resetting env. episode 729 reward total was -21.000000. loss_func: 0.698380\n",
      "resetting env. episode 730 reward total was -21.000000. loss_func: 0.713920\n",
      "resetting env. episode 731 reward total was -21.000000. loss_func: 0.701021\n",
      "resetting env. episode 732 reward total was -21.000000. loss_func: 0.699226\n",
      "resetting env. episode 733 reward total was -19.000000. loss_func: 1.094272\n",
      "resetting env. episode 734 reward total was -21.000000. loss_func: 0.684969\n",
      "resetting env. episode 735 reward total was -20.000000. loss_func: 0.990792\n",
      "resetting env. episode 736 reward total was -21.000000. loss_func: 0.670201\n",
      "resetting env. episode 737 reward total was -20.000000. loss_func: 0.937497\n",
      "resetting env. episode 738 reward total was -21.000000. loss_func: 0.678432\n",
      "resetting env. episode 739 reward total was -19.000000. loss_func: 1.096444\n",
      "resetting env. episode 740 reward total was -21.000000. loss_func: 0.655391\n",
      "resetting env. episode 741 reward total was -20.000000. loss_func: 1.003047\n",
      "resetting env. episode 742 reward total was -21.000000. loss_func: 0.718218\n",
      "resetting env. episode 743 reward total was -18.000000. loss_func: 1.297171\n",
      "resetting env. episode 744 reward total was -18.000000. loss_func: 1.376787\n",
      "resetting env. episode 745 reward total was -21.000000. loss_func: 0.722152\n",
      "resetting env. episode 746 reward total was -19.000000. loss_func: 1.071542\n",
      "resetting env. episode 747 reward total was -21.000000. loss_func: 0.706603\n",
      "resetting env. episode 748 reward total was -20.000000. loss_func: 0.896487\n",
      "resetting env. episode 749 reward total was -21.000000. loss_func: 0.688155\n",
      "resetting env. episode 750 reward total was -20.000000. loss_func: 0.897871\n",
      "resetting env. episode 751 reward total was -21.000000. loss_func: 0.693556\n",
      "resetting env. episode 752 reward total was -21.000000. loss_func: 0.686733\n",
      "resetting env. episode 753 reward total was -21.000000. loss_func: 0.688411\n",
      "resetting env. episode 754 reward total was -20.000000. loss_func: 0.906992\n",
      "resetting env. episode 755 reward total was -20.000000. loss_func: 0.909507\n",
      "resetting env. episode 756 reward total was -21.000000. loss_func: 0.690086\n",
      "resetting env. episode 757 reward total was -21.000000. loss_func: 0.705435\n",
      "resetting env. episode 758 reward total was -21.000000. loss_func: 0.689906\n",
      "resetting env. episode 759 reward total was -21.000000. loss_func: 0.657122\n",
      "resetting env. episode 760 reward total was -21.000000. loss_func: 0.733743\n",
      "resetting env. episode 761 reward total was -20.000000. loss_func: 1.093350\n",
      "resetting env. episode 762 reward total was -20.000000. loss_func: 0.898256\n",
      "resetting env. episode 763 reward total was -20.000000. loss_func: 0.916931\n",
      "resetting env. episode 764 reward total was -21.000000. loss_func: 0.658008\n",
      "resetting env. episode 765 reward total was -20.000000. loss_func: 0.899002\n",
      "resetting env. episode 766 reward total was -20.000000. loss_func: 1.057003\n",
      "resetting env. episode 767 reward total was -21.000000. loss_func: 0.722292\n",
      "resetting env. episode 768 reward total was -20.000000. loss_func: 1.046228\n",
      "resetting env. episode 769 reward total was -19.000000. loss_func: 1.102328\n",
      "resetting env. episode 770 reward total was -20.000000. loss_func: 0.915292\n",
      "resetting env. episode 771 reward total was -20.000000. loss_func: 0.896825\n",
      "resetting env. episode 772 reward total was -21.000000. loss_func: 0.722223\n",
      "resetting env. episode 773 reward total was -21.000000. loss_func: 0.642603\n",
      "resetting env. episode 774 reward total was -21.000000. loss_func: 0.704241\n",
      "resetting env. episode 775 reward total was -20.000000. loss_func: 0.920072\n",
      "resetting env. episode 776 reward total was -21.000000. loss_func: 0.704535\n",
      "resetting env. episode 777 reward total was -20.000000. loss_func: 0.898747\n",
      "resetting env. episode 778 reward total was -21.000000. loss_func: 0.673831\n",
      "resetting env. episode 779 reward total was -20.000000. loss_func: 0.909456\n",
      "resetting env. episode 780 reward total was -20.000000. loss_func: 0.912384\n",
      "resetting env. episode 781 reward total was -21.000000. loss_func: 0.689838\n",
      "resetting env. episode 782 reward total was -21.000000. loss_func: 0.713518\n",
      "resetting env. episode 783 reward total was -21.000000. loss_func: 0.729989\n",
      "resetting env. episode 784 reward total was -21.000000. loss_func: 0.677188\n",
      "resetting env. episode 785 reward total was -20.000000. loss_func: 0.888401\n",
      "resetting env. episode 786 reward total was -19.000000. loss_func: 1.231401\n",
      "resetting env. episode 787 reward total was -20.000000. loss_func: 1.082423\n",
      "resetting env. episode 788 reward total was -21.000000. loss_func: 0.676445\n",
      "resetting env. episode 789 reward total was -19.000000. loss_func: 1.171610\n",
      "resetting env. episode 790 reward total was -20.000000. loss_func: 0.904805\n",
      "resetting env. episode 791 reward total was -20.000000. loss_func: 0.907828\n",
      "resetting env. episode 792 reward total was -21.000000. loss_func: 0.729239\n",
      "resetting env. episode 793 reward total was -21.000000. loss_func: 0.675770\n",
      "resetting env. episode 794 reward total was -20.000000. loss_func: 0.909953\n",
      "resetting env. episode 795 reward total was -20.000000. loss_func: 0.891413\n",
      "resetting env. episode 796 reward total was -20.000000. loss_func: 1.275562\n",
      "resetting env. episode 797 reward total was -21.000000. loss_func: 0.679859\n",
      "resetting env. episode 798 reward total was -20.000000. loss_func: 0.905514\n",
      "resetting env. episode 799 reward total was -21.000000. loss_func: 0.678906\n",
      "resetting env. episode 800 reward total was -21.000000. loss_func: 0.681129\n",
      "resetting env. episode 801 reward total was -21.000000. loss_func: 0.657069\n",
      "resetting env. episode 802 reward total was -21.000000. loss_func: 0.690218\n",
      "resetting env. episode 803 reward total was -21.000000. loss_func: 0.676307\n",
      "resetting env. episode 804 reward total was -20.000000. loss_func: 0.903652\n",
      "resetting env. episode 805 reward total was -21.000000. loss_func: 0.705950\n",
      "resetting env. episode 806 reward total was -21.000000. loss_func: 0.741540\n",
      "resetting env. episode 807 reward total was -21.000000. loss_func: 0.694244\n",
      "resetting env. episode 808 reward total was -21.000000. loss_func: 0.675822\n",
      "resetting env. episode 809 reward total was -20.000000. loss_func: 0.913162\n",
      "resetting env. episode 810 reward total was -20.000000. loss_func: 1.079509\n",
      "resetting env. episode 811 reward total was -21.000000. loss_func: 0.662440\n",
      "resetting env. episode 812 reward total was -21.000000. loss_func: 0.701275\n",
      "resetting env. episode 813 reward total was -21.000000. loss_func: 0.678449\n",
      "resetting env. episode 814 reward total was -20.000000. loss_func: 1.113817\n",
      "resetting env. episode 815 reward total was -19.000000. loss_func: 1.091338\n",
      "resetting env. episode 816 reward total was -21.000000. loss_func: 0.658014\n",
      "resetting env. episode 817 reward total was -21.000000. loss_func: 0.688155\n",
      "resetting env. episode 818 reward total was -21.000000. loss_func: 0.674890\n",
      "resetting env. episode 819 reward total was -19.000000. loss_func: 1.201178\n",
      "resetting env. episode 820 reward total was -21.000000. loss_func: 0.659716\n",
      "resetting env. episode 821 reward total was -19.000000. loss_func: 1.245568\n",
      "resetting env. episode 822 reward total was -19.000000. loss_func: 1.074588\n",
      "resetting env. episode 823 reward total was -20.000000. loss_func: 0.912014\n",
      "resetting env. episode 824 reward total was -20.000000. loss_func: 1.071037\n",
      "resetting env. episode 825 reward total was -20.000000. loss_func: 0.899896\n",
      "resetting env. episode 826 reward total was -21.000000. loss_func: 0.680908\n",
      "resetting env. episode 827 reward total was -21.000000. loss_func: 0.693902\n",
      "resetting env. episode 828 reward total was -20.000000. loss_func: 0.903176\n",
      "resetting env. episode 829 reward total was -21.000000. loss_func: 0.705646\n",
      "resetting env. episode 830 reward total was -21.000000. loss_func: 0.638125\n",
      "resetting env. episode 831 reward total was -21.000000. loss_func: 0.682102\n",
      "resetting env. episode 832 reward total was -21.000000. loss_func: 0.718524\n",
      "resetting env. episode 833 reward total was -20.000000. loss_func: 0.910709\n",
      "resetting env. episode 834 reward total was -19.000000. loss_func: 1.172066\n",
      "resetting env. episode 835 reward total was -19.000000. loss_func: 1.197273\n",
      "resetting env. episode 836 reward total was -21.000000. loss_func: 0.710011\n",
      "resetting env. episode 837 reward total was -21.000000. loss_func: 0.789864\n",
      "resetting env. episode 838 reward total was -21.000000. loss_func: 0.704155\n",
      "resetting env. episode 839 reward total was -21.000000. loss_func: 0.706702\n",
      "resetting env. episode 840 reward total was -20.000000. loss_func: 0.903408\n",
      "resetting env. episode 841 reward total was -20.000000. loss_func: 0.899125\n",
      "resetting env. episode 842 reward total was -21.000000. loss_func: 0.660830\n",
      "resetting env. episode 843 reward total was -21.000000. loss_func: 0.678136\n",
      "resetting env. episode 844 reward total was -20.000000. loss_func: 0.898889\n",
      "resetting env. episode 845 reward total was -21.000000. loss_func: 0.704774\n",
      "resetting env. episode 846 reward total was -16.000000. loss_func: 1.496158\n",
      "resetting env. episode 847 reward total was -20.000000. loss_func: 0.912366\n",
      "resetting env. episode 848 reward total was -20.000000. loss_func: 0.906402\n",
      "resetting env. episode 849 reward total was -21.000000. loss_func: 0.677647\n",
      "resetting env. episode 850 reward total was -20.000000. loss_func: 1.083990\n",
      "resetting env. episode 851 reward total was -21.000000. loss_func: 0.708966\n",
      "resetting env. episode 852 reward total was -18.000000. loss_func: 1.274941\n",
      "resetting env. episode 853 reward total was -20.000000. loss_func: 0.903142\n",
      "resetting env. episode 854 reward total was -20.000000. loss_func: 4.347814\n",
      "resetting env. episode 855 reward total was -21.000000. loss_func: 0.704212\n",
      "resetting env. episode 856 reward total was -21.000000. loss_func: 0.691980\n",
      "resetting env. episode 857 reward total was -17.000000. loss_func: 1.477341\n",
      "resetting env. episode 858 reward total was -21.000000. loss_func: 0.661905\n",
      "resetting env. episode 859 reward total was -21.000000. loss_func: 0.709335\n",
      "resetting env. episode 860 reward total was -21.000000. loss_func: 0.716501\n",
      "resetting env. episode 861 reward total was -20.000000. loss_func: 0.894674\n",
      "resetting env. episode 862 reward total was -21.000000. loss_func: 0.683349\n",
      "resetting env. episode 863 reward total was -20.000000. loss_func: 0.910424\n",
      "resetting env. episode 864 reward total was -21.000000. loss_func: 0.663307\n",
      "resetting env. episode 865 reward total was -21.000000. loss_func: 0.739075\n",
      "resetting env. episode 866 reward total was -21.000000. loss_func: 0.708489\n",
      "resetting env. episode 867 reward total was -21.000000. loss_func: 0.690146\n",
      "resetting env. episode 868 reward total was -20.000000. loss_func: 0.912660\n",
      "resetting env. episode 869 reward total was -20.000000. loss_func: 0.987755\n",
      "resetting env. episode 870 reward total was -18.000000. loss_func: 1.336197\n",
      "resetting env. episode 871 reward total was -21.000000. loss_func: 0.681313\n",
      "resetting env. episode 872 reward total was -21.000000. loss_func: 0.697381\n",
      "resetting env. episode 873 reward total was -21.000000. loss_func: 0.716671\n",
      "resetting env. episode 874 reward total was -19.000000. loss_func: 1.167052\n",
      "resetting env. episode 875 reward total was -21.000000. loss_func: 0.696381\n",
      "resetting env. episode 876 reward total was -20.000000. loss_func: 0.897702\n",
      "resetting env. episode 877 reward total was -18.000000. loss_func: 1.378499\n",
      "resetting env. episode 878 reward total was -21.000000. loss_func: 0.680118\n",
      "resetting env. episode 879 reward total was -20.000000. loss_func: 0.905952\n",
      "resetting env. episode 880 reward total was -19.000000. loss_func: 1.077697\n",
      "resetting env. episode 881 reward total was -21.000000. loss_func: 0.676627\n",
      "resetting env. episode 882 reward total was -20.000000. loss_func: 0.904747\n",
      "resetting env. episode 883 reward total was -21.000000. loss_func: 0.642306\n",
      "resetting env. episode 884 reward total was -20.000000. loss_func: 0.903953\n",
      "resetting env. episode 885 reward total was -21.000000. loss_func: 0.694515\n",
      "resetting env. episode 886 reward total was -21.000000. loss_func: 0.712272\n",
      "resetting env. episode 887 reward total was -19.000000. loss_func: 1.178445\n",
      "resetting env. episode 888 reward total was -20.000000. loss_func: 0.884332\n",
      "resetting env. episode 889 reward total was -21.000000. loss_func: 0.681895\n",
      "resetting env. episode 890 reward total was -19.000000. loss_func: 1.067962\n",
      "resetting env. episode 891 reward total was -18.000000. loss_func: 1.242595\n",
      "resetting env. episode 892 reward total was -19.000000. loss_func: 1.056169\n",
      "resetting env. episode 893 reward total was -21.000000. loss_func: 0.698764\n",
      "resetting env. episode 894 reward total was -21.000000. loss_func: 0.721330\n",
      "resetting env. episode 895 reward total was -21.000000. loss_func: 0.662993\n",
      "resetting env. episode 896 reward total was -20.000000. loss_func: 0.886300\n",
      "resetting env. episode 897 reward total was -20.000000. loss_func: 1.028552\n",
      "resetting env. episode 898 reward total was -21.000000. loss_func: 0.705792\n",
      "resetting env. episode 899 reward total was -21.000000. loss_func: 0.711957\n",
      "resetting env. episode 900 reward total was -21.000000. loss_func: 0.731191\n",
      "resetting env. episode 901 reward total was -21.000000. loss_func: 0.684998\n",
      "resetting env. episode 902 reward total was -21.000000. loss_func: 0.682289\n",
      "resetting env. episode 903 reward total was -21.000000. loss_func: 0.678820\n",
      "resetting env. episode 904 reward total was -21.000000. loss_func: 0.689185\n",
      "resetting env. episode 905 reward total was -20.000000. loss_func: 0.851984\n",
      "resetting env. episode 906 reward total was -19.000000. loss_func: 0.990925\n",
      "resetting env. episode 907 reward total was -20.000000. loss_func: 0.902569\n",
      "resetting env. episode 908 reward total was -21.000000. loss_func: 0.812476\n",
      "resetting env. episode 909 reward total was -21.000000. loss_func: 0.638500\n",
      "resetting env. episode 910 reward total was -19.000000. loss_func: 1.076889\n",
      "resetting env. episode 911 reward total was -21.000000. loss_func: 0.620802\n",
      "resetting env. episode 912 reward total was -20.000000. loss_func: 0.821334\n",
      "resetting env. episode 913 reward total was -19.000000. loss_func: 0.948759\n",
      "resetting env. episode 914 reward total was -21.000000. loss_func: 0.613995\n",
      "resetting env. episode 915 reward total was -20.000000. loss_func: 0.923081\n",
      "resetting env. episode 916 reward total was -18.000000. loss_func: 1.013646\n",
      "resetting env. episode 917 reward total was -20.000000. loss_func: 1.501771\n",
      "resetting env. episode 918 reward total was -21.000000. loss_func: 0.653222\n",
      "resetting env. episode 919 reward total was -21.000000. loss_func: 0.569265\n",
      "resetting env. episode 920 reward total was -21.000000. loss_func: 0.586984\n",
      "resetting env. episode 921 reward total was -20.000000. loss_func: 0.785613\n",
      "resetting env. episode 922 reward total was -21.000000. loss_func: 0.597242\n",
      "resetting env. episode 923 reward total was -19.000000. loss_func: 0.901632\n",
      "resetting env. episode 924 reward total was -21.000000. loss_func: 0.569805\n",
      "resetting env. episode 925 reward total was -20.000000. loss_func: 1.117470\n",
      "resetting env. episode 926 reward total was -21.000000. loss_func: 0.569615\n",
      "resetting env. episode 927 reward total was -21.000000. loss_func: 0.582750\n",
      "resetting env. episode 928 reward total was -21.000000. loss_func: 0.486545\n",
      "resetting env. episode 929 reward total was -21.000000. loss_func: 0.534810\n",
      "resetting env. episode 930 reward total was -21.000000. loss_func: 0.524078\n",
      "resetting env. episode 931 reward total was -21.000000. loss_func: 0.604911\n",
      "resetting env. episode 932 reward total was -21.000000. loss_func: 0.529716\n",
      "resetting env. episode 933 reward total was -21.000000. loss_func: 0.539240\n",
      "resetting env. episode 934 reward total was -20.000000. loss_func: 0.726488\n",
      "resetting env. episode 935 reward total was -20.000000. loss_func: 0.696748\n",
      "resetting env. episode 936 reward total was -20.000000. loss_func: 0.686617\n",
      "resetting env. episode 937 reward total was -21.000000. loss_func: 0.535818\n",
      "resetting env. episode 938 reward total was -20.000000. loss_func: 0.844479\n",
      "resetting env. episode 939 reward total was -20.000000. loss_func: 0.718749\n",
      "resetting env. episode 940 reward total was -21.000000. loss_func: 0.474378\n",
      "resetting env. episode 941 reward total was -21.000000. loss_func: 0.432557\n",
      "resetting env. episode 942 reward total was -20.000000. loss_func: 0.692630\n",
      "resetting env. episode 943 reward total was -21.000000. loss_func: 0.411531\n",
      "resetting env. episode 944 reward total was -21.000000. loss_func: 0.486729\n",
      "resetting env. episode 945 reward total was -21.000000. loss_func: 0.504875\n",
      "resetting env. episode 946 reward total was -21.000000. loss_func: 0.385344\n",
      "resetting env. episode 947 reward total was -20.000000. loss_func: 0.837257\n",
      "resetting env. episode 948 reward total was -20.000000. loss_func: 0.634629\n",
      "resetting env. episode 949 reward total was -21.000000. loss_func: 0.408531\n",
      "resetting env. episode 950 reward total was -19.000000. loss_func: 0.831996\n",
      "resetting env. episode 951 reward total was -20.000000. loss_func: 0.611376\n",
      "resetting env. episode 952 reward total was -20.000000. loss_func: 0.625575\n",
      "resetting env. episode 953 reward total was -21.000000. loss_func: 0.510846\n",
      "resetting env. episode 954 reward total was -21.000000. loss_func: 0.442349\n",
      "resetting env. episode 955 reward total was -21.000000. loss_func: 0.348495\n",
      "resetting env. episode 956 reward total was -21.000000. loss_func: 0.410318\n",
      "resetting env. episode 957 reward total was -20.000000. loss_func: 0.665739\n",
      "resetting env. episode 958 reward total was -21.000000. loss_func: 0.417630\n",
      "resetting env. episode 959 reward total was -21.000000. loss_func: 0.295880\n",
      "resetting env. episode 960 reward total was -21.000000. loss_func: 0.348203\n",
      "resetting env. episode 961 reward total was -21.000000. loss_func: 0.353487\n",
      "resetting env. episode 962 reward total was -21.000000. loss_func: 0.406868\n",
      "resetting env. episode 963 reward total was -21.000000. loss_func: 0.341371\n",
      "resetting env. episode 964 reward total was -21.000000. loss_func: 0.410228\n",
      "resetting env. episode 965 reward total was -21.000000. loss_func: 0.429318\n",
      "resetting env. episode 966 reward total was -20.000000. loss_func: 0.709156\n",
      "resetting env. episode 967 reward total was -20.000000. loss_func: 0.798695\n",
      "resetting env. episode 968 reward total was -20.000000. loss_func: 0.660696\n",
      "resetting env. episode 969 reward total was -21.000000. loss_func: 0.391640\n",
      "resetting env. episode 970 reward total was -20.000000. loss_func: 0.607160\n",
      "resetting env. episode 971 reward total was -21.000000. loss_func: 0.365731\n",
      "resetting env. episode 972 reward total was -21.000000. loss_func: 0.310123\n",
      "resetting env. episode 973 reward total was -18.000000. loss_func: 1.090280\n",
      "resetting env. episode 974 reward total was -20.000000. loss_func: 0.677260\n",
      "resetting env. episode 975 reward total was -21.000000. loss_func: 0.303834\n",
      "resetting env. episode 976 reward total was -21.000000. loss_func: 0.440056\n",
      "resetting env. episode 977 reward total was -20.000000. loss_func: 1.068716\n",
      "resetting env. episode 978 reward total was -18.000000. loss_func: 1.205088\n",
      "resetting env. episode 979 reward total was -20.000000. loss_func: 0.542629\n",
      "resetting env. episode 980 reward total was -21.000000. loss_func: 4.225705\n",
      "resetting env. episode 981 reward total was -21.000000. loss_func: 0.465509\n",
      "resetting env. episode 982 reward total was -19.000000. loss_func: 2.679365\n",
      "resetting env. episode 983 reward total was -20.000000. loss_func: 0.729280\n",
      "resetting env. episode 984 reward total was -20.000000. loss_func: 0.729432\n",
      "resetting env. episode 985 reward total was -21.000000. loss_func: 0.490088\n",
      "resetting env. episode 986 reward total was -15.000000. loss_func: 1.202133\n",
      "resetting env. episode 987 reward total was -20.000000. loss_func: 0.597451\n",
      "resetting env. episode 988 reward total was -21.000000. loss_func: 0.373997\n",
      "resetting env. episode 989 reward total was -21.000000. loss_func: 0.385521\n",
      "resetting env. episode 990 reward total was -19.000000. loss_func: 0.690781\n",
      "resetting env. episode 991 reward total was -21.000000. loss_func: 0.490081\n",
      "resetting env. episode 992 reward total was -21.000000. loss_func: 0.402673\n",
      "resetting env. episode 993 reward total was -21.000000. loss_func: 0.364660\n",
      "resetting env. episode 994 reward total was -21.000000. loss_func: 0.264300\n",
      "resetting env. episode 995 reward total was -21.000000. loss_func: 0.298556\n",
      "resetting env. episode 996 reward total was -20.000000. loss_func: 0.719007\n",
      "resetting env. episode 997 reward total was -21.000000. loss_func: 0.250494\n",
      "resetting env. episode 998 reward total was -19.000000. loss_func: 0.838972\n",
      "resetting env. episode 999 reward total was -21.000000. loss_func: 0.271630\n",
      "resetting env. episode 1000 reward total was -21.000000. loss_func: 0.231931\n",
      "resetting env. episode 1001 reward total was -21.000000. loss_func: 0.306625\n",
      "resetting env. episode 1002 reward total was -21.000000. loss_func: 0.327859\n",
      "resetting env. episode 1003 reward total was -21.000000. loss_func: 0.276316\n",
      "resetting env. episode 1004 reward total was -21.000000. loss_func: 0.283286\n",
      "resetting env. episode 1005 reward total was -21.000000. loss_func: 0.258963\n",
      "resetting env. episode 1006 reward total was -21.000000. loss_func: 0.257963\n",
      "resetting env. episode 1007 reward total was -21.000000. loss_func: 0.238682\n",
      "resetting env. episode 1008 reward total was -20.000000. loss_func: 0.587691\n",
      "resetting env. episode 1009 reward total was -21.000000. loss_func: 0.325368\n",
      "resetting env. episode 1010 reward total was -21.000000. loss_func: 0.354531\n",
      "resetting env. episode 1011 reward total was -18.000000. loss_func: 0.924366\n",
      "resetting env. episode 1012 reward total was -20.000000. loss_func: 0.599582\n",
      "resetting env. episode 1013 reward total was -21.000000. loss_func: 0.243508\n",
      "resetting env. episode 1014 reward total was -21.000000. loss_func: 0.292949\n",
      "resetting env. episode 1015 reward total was -21.000000. loss_func: 0.331337\n",
      "resetting env. episode 1016 reward total was -21.000000. loss_func: 0.216208\n",
      "resetting env. episode 1017 reward total was -21.000000. loss_func: 0.309436\n",
      "resetting env. episode 1018 reward total was -20.000000. loss_func: 0.549963\n",
      "resetting env. episode 1019 reward total was -20.000000. loss_func: 0.635955\n",
      "resetting env. episode 1020 reward total was -21.000000. loss_func: 0.283599\n",
      "resetting env. episode 1021 reward total was -20.000000. loss_func: 0.590851\n",
      "resetting env. episode 1022 reward total was -21.000000. loss_func: 0.459417\n",
      "resetting env. episode 1023 reward total was -21.000000. loss_func: 0.275974\n",
      "resetting env. episode 1024 reward total was -21.000000. loss_func: 0.263206\n",
      "resetting env. episode 1025 reward total was -20.000000. loss_func: 0.479997\n",
      "resetting env. episode 1026 reward total was -21.000000. loss_func: 0.248143\n",
      "resetting env. episode 1027 reward total was -21.000000. loss_func: 0.282687\n",
      "resetting env. episode 1028 reward total was -21.000000. loss_func: 0.278926\n",
      "resetting env. episode 1029 reward total was -19.000000. loss_func: 0.738246\n",
      "resetting env. episode 1030 reward total was -20.000000. loss_func: 0.493746\n",
      "resetting env. episode 1031 reward total was -21.000000. loss_func: 0.289820\n",
      "resetting env. episode 1032 reward total was -21.000000. loss_func: 0.304084\n",
      "resetting env. episode 1033 reward total was -21.000000. loss_func: 0.275781\n",
      "resetting env. episode 1034 reward total was -21.000000. loss_func: 0.342411\n",
      "resetting env. episode 1035 reward total was -20.000000. loss_func: 0.827357\n",
      "resetting env. episode 1036 reward total was -21.000000. loss_func: 0.292114\n",
      "resetting env. episode 1037 reward total was -18.000000. loss_func: 1.091532\n",
      "resetting env. episode 1038 reward total was -20.000000. loss_func: 0.693191\n",
      "resetting env. episode 1039 reward total was -20.000000. loss_func: 0.642669\n",
      "resetting env. episode 1040 reward total was -21.000000. loss_func: 0.269143\n",
      "resetting env. episode 1041 reward total was -21.000000. loss_func: 0.407643\n",
      "resetting env. episode 1042 reward total was -21.000000. loss_func: 0.295261\n",
      "resetting env. episode 1043 reward total was -21.000000. loss_func: 0.285505\n",
      "resetting env. episode 1044 reward total was -19.000000. loss_func: 0.677210\n",
      "resetting env. episode 1045 reward total was -20.000000. loss_func: 0.453740\n",
      "resetting env. episode 1046 reward total was -21.000000. loss_func: 0.200953\n",
      "resetting env. episode 1047 reward total was -20.000000. loss_func: 0.536825\n",
      "resetting env. episode 1048 reward total was -17.000000. loss_func: 1.068728\n",
      "resetting env. episode 1049 reward total was -21.000000. loss_func: 0.657609\n",
      "resetting env. episode 1050 reward total was -21.000000. loss_func: 0.317136\n",
      "resetting env. episode 1051 reward total was -20.000000. loss_func: 0.743768\n",
      "resetting env. episode 1052 reward total was -20.000000. loss_func: 0.611466\n",
      "resetting env. episode 1053 reward total was -21.000000. loss_func: 0.295284\n",
      "resetting env. episode 1054 reward total was -20.000000. loss_func: 0.480607\n",
      "resetting env. episode 1055 reward total was -21.000000. loss_func: 0.390641\n",
      "resetting env. episode 1056 reward total was -19.000000. loss_func: 0.830439\n",
      "resetting env. episode 1057 reward total was -21.000000. loss_func: 0.270755\n",
      "resetting env. episode 1058 reward total was -21.000000. loss_func: 0.246231\n",
      "resetting env. episode 1059 reward total was -20.000000. loss_func: 0.524723\n",
      "resetting env. episode 1060 reward total was -20.000000. loss_func: 0.751945\n",
      "resetting env. episode 1061 reward total was -20.000000. loss_func: 0.462108\n",
      "resetting env. episode 1062 reward total was -20.000000. loss_func: 0.556729\n",
      "resetting env. episode 1063 reward total was -21.000000. loss_func: 0.272113\n",
      "resetting env. episode 1064 reward total was -21.000000. loss_func: 0.212143\n",
      "resetting env. episode 1065 reward total was -20.000000. loss_func: 0.428887\n",
      "resetting env. episode 1066 reward total was -19.000000. loss_func: 0.637548\n",
      "resetting env. episode 1067 reward total was -21.000000. loss_func: 0.485613\n",
      "resetting env. episode 1068 reward total was -19.000000. loss_func: 0.731593\n",
      "resetting env. episode 1069 reward total was -21.000000. loss_func: 0.276401\n",
      "resetting env. episode 1070 reward total was -20.000000. loss_func: 0.541280\n",
      "resetting env. episode 1071 reward total was -21.000000. loss_func: 0.252950\n",
      "resetting env. episode 1072 reward total was -21.000000. loss_func: 0.192258\n",
      "resetting env. episode 1073 reward total was -20.000000. loss_func: 0.706787\n",
      "resetting env. episode 1074 reward total was -21.000000. loss_func: 0.233065\n",
      "resetting env. episode 1075 reward total was -21.000000. loss_func: 0.287449\n",
      "resetting env. episode 1076 reward total was -21.000000. loss_func: 0.277661\n",
      "resetting env. episode 1077 reward total was -21.000000. loss_func: 0.213120\n",
      "resetting env. episode 1078 reward total was -20.000000. loss_func: 0.562522\n",
      "resetting env. episode 1079 reward total was -21.000000. loss_func: 0.369036\n",
      "resetting env. episode 1080 reward total was -20.000000. loss_func: 0.505921\n",
      "resetting env. episode 1081 reward total was -20.000000. loss_func: 0.503323\n",
      "resetting env. episode 1082 reward total was -20.000000. loss_func: 0.485875\n",
      "resetting env. episode 1083 reward total was -21.000000. loss_func: 0.190262\n",
      "resetting env. episode 1084 reward total was -21.000000. loss_func: 0.225337\n",
      "resetting env. episode 1085 reward total was -21.000000. loss_func: 0.256352\n",
      "resetting env. episode 1086 reward total was -21.000000. loss_func: 0.543705\n",
      "resetting env. episode 1087 reward total was -21.000000. loss_func: 0.338061\n",
      "resetting env. episode 1088 reward total was -21.000000. loss_func: 0.273597\n",
      "resetting env. episode 1089 reward total was -21.000000. loss_func: 0.324861\n",
      "resetting env. episode 1090 reward total was -21.000000. loss_func: 0.182008\n",
      "resetting env. episode 1091 reward total was -21.000000. loss_func: 0.208273\n",
      "resetting env. episode 1092 reward total was -21.000000. loss_func: 0.193964\n",
      "resetting env. episode 1093 reward total was -18.000000. loss_func: 0.973873\n",
      "resetting env. episode 1094 reward total was -21.000000. loss_func: 0.420370\n",
      "resetting env. episode 1095 reward total was -21.000000. loss_func: 0.300402\n",
      "resetting env. episode 1096 reward total was -20.000000. loss_func: 0.484502\n",
      "resetting env. episode 1097 reward total was -20.000000. loss_func: 0.473659\n",
      "resetting env. episode 1098 reward total was -20.000000. loss_func: 0.451158\n",
      "resetting env. episode 1099 reward total was -20.000000. loss_func: 0.449083\n",
      "resetting env. episode 1100 reward total was -20.000000. loss_func: 0.450191\n",
      "resetting env. episode 1101 reward total was -21.000000. loss_func: 0.145123\n",
      "resetting env. episode 1102 reward total was -21.000000. loss_func: 0.164818\n",
      "resetting env. episode 1103 reward total was -19.000000. loss_func: 0.894216\n",
      "resetting env. episode 1104 reward total was -21.000000. loss_func: 0.349785\n",
      "resetting env. episode 1105 reward total was -20.000000. loss_func: 0.486379\n",
      "resetting env. episode 1106 reward total was -19.000000. loss_func: 0.993348\n",
      "resetting env. episode 1107 reward total was -21.000000. loss_func: 0.225030\n",
      "resetting env. episode 1108 reward total was -21.000000. loss_func: 0.237556\n",
      "resetting env. episode 1109 reward total was -21.000000. loss_func: 0.255351\n",
      "resetting env. episode 1110 reward total was -21.000000. loss_func: 0.353441\n",
      "resetting env. episode 1111 reward total was -21.000000. loss_func: 0.311822\n",
      "resetting env. episode 1112 reward total was -20.000000. loss_func: 0.497887\n",
      "resetting env. episode 1113 reward total was -21.000000. loss_func: 0.246563\n",
      "resetting env. episode 1114 reward total was -19.000000. loss_func: 0.610597\n",
      "resetting env. episode 1115 reward total was -20.000000. loss_func: 0.452075\n",
      "resetting env. episode 1116 reward total was -21.000000. loss_func: 0.175835\n",
      "resetting env. episode 1117 reward total was -20.000000. loss_func: 0.594358\n",
      "resetting env. episode 1118 reward total was -20.000000. loss_func: 0.427226\n",
      "resetting env. episode 1119 reward total was -20.000000. loss_func: 0.408997\n",
      "resetting env. episode 1120 reward total was -20.000000. loss_func: 0.454861\n",
      "resetting env. episode 1121 reward total was -21.000000. loss_func: 0.276326\n",
      "resetting env. episode 1122 reward total was -21.000000. loss_func: 0.320294\n",
      "resetting env. episode 1123 reward total was -19.000000. loss_func: 0.619092\n",
      "resetting env. episode 1124 reward total was -21.000000. loss_func: 0.270087\n",
      "resetting env. episode 1125 reward total was -21.000000. loss_func: 0.238105\n",
      "resetting env. episode 1126 reward total was -20.000000. loss_func: 0.455525\n",
      "resetting env. episode 1127 reward total was -21.000000. loss_func: 0.286438\n",
      "resetting env. episode 1128 reward total was -21.000000. loss_func: 0.202252\n",
      "resetting env. episode 1129 reward total was -19.000000. loss_func: 2.048229\n",
      "resetting env. episode 1130 reward total was -21.000000. loss_func: 0.633676\n",
      "resetting env. episode 1131 reward total was -21.000000. loss_func: 0.580367\n",
      "resetting env. episode 1132 reward total was -19.000000. loss_func: 0.919081\n",
      "resetting env. episode 1133 reward total was -21.000000. loss_func: 0.352378\n",
      "resetting env. episode 1134 reward total was -21.000000. loss_func: 0.406874\n",
      "resetting env. episode 1135 reward total was -20.000000. loss_func: 0.456082\n",
      "resetting env. episode 1136 reward total was -21.000000. loss_func: 0.164441\n",
      "resetting env. episode 1137 reward total was -20.000000. loss_func: 0.487312\n",
      "resetting env. episode 1138 reward total was -19.000000. loss_func: 0.627600\n",
      "resetting env. episode 1139 reward total was -20.000000. loss_func: 0.746083\n",
      "resetting env. episode 1140 reward total was -20.000000. loss_func: 0.434668\n",
      "resetting env. episode 1141 reward total was -18.000000. loss_func: 0.764861\n",
      "resetting env. episode 1142 reward total was -21.000000. loss_func: 0.474682\n",
      "resetting env. episode 1143 reward total was -21.000000. loss_func: 0.204405\n",
      "resetting env. episode 1144 reward total was -20.000000. loss_func: 0.542738\n",
      "resetting env. episode 1145 reward total was -21.000000. loss_func: 0.443173\n",
      "resetting env. episode 1146 reward total was -19.000000. loss_func: 0.601116\n",
      "resetting env. episode 1147 reward total was -21.000000. loss_func: 0.236952\n",
      "resetting env. episode 1148 reward total was -21.000000. loss_func: 0.211704\n",
      "resetting env. episode 1149 reward total was -21.000000. loss_func: 0.263357\n",
      "resetting env. episode 1150 reward total was -21.000000. loss_func: 0.116118\n",
      "resetting env. episode 1151 reward total was -20.000000. loss_func: 0.739655\n",
      "resetting env. episode 1152 reward total was -21.000000. loss_func: 0.240391\n",
      "resetting env. episode 1153 reward total was -18.000000. loss_func: 1.029932\n",
      "resetting env. episode 1154 reward total was -19.000000. loss_func: 0.642898\n",
      "resetting env. episode 1155 reward total was -21.000000. loss_func: 0.268227\n",
      "resetting env. episode 1156 reward total was -21.000000. loss_func: 0.277799\n",
      "resetting env. episode 1157 reward total was -21.000000. loss_func: 0.245467\n",
      "resetting env. episode 1158 reward total was -19.000000. loss_func: 0.637933\n",
      "resetting env. episode 1159 reward total was -19.000000. loss_func: 1.038146\n",
      "resetting env. episode 1160 reward total was -20.000000. loss_func: 1.934911\n",
      "resetting env. episode 1161 reward total was -21.000000. loss_func: 0.500688\n",
      "resetting env. episode 1162 reward total was -21.000000. loss_func: 0.345944\n",
      "resetting env. episode 1163 reward total was -21.000000. loss_func: 0.164526\n",
      "resetting env. episode 1164 reward total was -21.000000. loss_func: 0.263316\n",
      "resetting env. episode 1165 reward total was -21.000000. loss_func: 0.290287\n",
      "resetting env. episode 1166 reward total was -20.000000. loss_func: 0.435028\n",
      "resetting env. episode 1167 reward total was -21.000000. loss_func: 0.240438\n",
      "resetting env. episode 1168 reward total was -21.000000. loss_func: 0.213446\n",
      "resetting env. episode 1169 reward total was -21.000000. loss_func: 0.222383\n",
      "resetting env. episode 1170 reward total was -21.000000. loss_func: 0.222932\n",
      "resetting env. episode 1171 reward total was -20.000000. loss_func: 0.572074\n",
      "resetting env. episode 1172 reward total was -21.000000. loss_func: 0.196923\n",
      "resetting env. episode 1173 reward total was -21.000000. loss_func: 0.302145\n",
      "resetting env. episode 1174 reward total was -21.000000. loss_func: 0.350750\n",
      "resetting env. episode 1175 reward total was -21.000000. loss_func: 0.182843\n",
      "resetting env. episode 1176 reward total was -21.000000. loss_func: 0.351320\n",
      "resetting env. episode 1177 reward total was -19.000000. loss_func: 1.053822\n",
      "resetting env. episode 1178 reward total was -21.000000. loss_func: 0.226187\n",
      "resetting env. episode 1179 reward total was -19.000000. loss_func: 0.862599\n",
      "resetting env. episode 1180 reward total was -19.000000. loss_func: 0.771143\n",
      "resetting env. episode 1181 reward total was -20.000000. loss_func: 0.429684\n",
      "resetting env. episode 1182 reward total was -19.000000. loss_func: 0.527929\n",
      "resetting env. episode 1183 reward total was -21.000000. loss_func: 0.157166\n",
      "resetting env. episode 1184 reward total was -21.000000. loss_func: 0.204063\n",
      "resetting env. episode 1185 reward total was -21.000000. loss_func: 0.207146\n",
      "resetting env. episode 1186 reward total was -21.000000. loss_func: 0.366716\n",
      "resetting env. episode 1187 reward total was -20.000000. loss_func: 0.454065\n",
      "resetting env. episode 1188 reward total was -21.000000. loss_func: 0.167789\n",
      "resetting env. episode 1189 reward total was -21.000000. loss_func: 0.273862\n",
      "resetting env. episode 1190 reward total was -20.000000. loss_func: 0.414160\n",
      "resetting env. episode 1191 reward total was -18.000000. loss_func: 1.238856\n",
      "resetting env. episode 1192 reward total was -21.000000. loss_func: 0.405747\n",
      "resetting env. episode 1193 reward total was -20.000000. loss_func: 0.418239\n",
      "resetting env. episode 1194 reward total was -20.000000. loss_func: 0.482311\n",
      "resetting env. episode 1195 reward total was -20.000000. loss_func: 0.747009\n",
      "resetting env. episode 1196 reward total was -21.000000. loss_func: 0.478203\n",
      "resetting env. episode 1197 reward total was -20.000000. loss_func: 0.718297\n",
      "resetting env. episode 1198 reward total was -20.000000. loss_func: 0.395688\n",
      "resetting env. episode 1199 reward total was -21.000000. loss_func: 0.324908\n",
      "resetting env. episode 1200 reward total was -21.000000. loss_func: 0.254120\n",
      "resetting env. episode 1201 reward total was -21.000000. loss_func: 0.280412\n",
      "resetting env. episode 1202 reward total was -21.000000. loss_func: 0.301919\n",
      "resetting env. episode 1203 reward total was -20.000000. loss_func: 0.570842\n",
      "resetting env. episode 1204 reward total was -19.000000. loss_func: 0.637576\n",
      "resetting env. episode 1205 reward total was -21.000000. loss_func: 0.281466\n",
      "resetting env. episode 1206 reward total was -21.000000. loss_func: 0.214408\n",
      "resetting env. episode 1207 reward total was -19.000000. loss_func: 0.685352\n",
      "resetting env. episode 1208 reward total was -21.000000. loss_func: 0.163468\n",
      "resetting env. episode 1209 reward total was -21.000000. loss_func: 0.296676\n",
      "resetting env. episode 1210 reward total was -20.000000. loss_func: 0.473318\n",
      "resetting env. episode 1211 reward total was -21.000000. loss_func: 0.154739\n",
      "resetting env. episode 1212 reward total was -21.000000. loss_func: 0.225154\n",
      "resetting env. episode 1213 reward total was -21.000000. loss_func: 0.479728\n",
      "resetting env. episode 1214 reward total was -21.000000. loss_func: 0.466669\n",
      "resetting env. episode 1215 reward total was -21.000000. loss_func: 0.338160\n",
      "resetting env. episode 1216 reward total was -21.000000. loss_func: 0.170607\n",
      "resetting env. episode 1217 reward total was -21.000000. loss_func: 0.193209\n",
      "resetting env. episode 1218 reward total was -20.000000. loss_func: 0.447252\n",
      "resetting env. episode 1219 reward total was -21.000000. loss_func: 0.183042\n",
      "resetting env. episode 1220 reward total was -21.000000. loss_func: 0.212229\n",
      "resetting env. episode 1221 reward total was -21.000000. loss_func: 0.254567\n",
      "resetting env. episode 1222 reward total was -21.000000. loss_func: 0.228109\n",
      "resetting env. episode 1223 reward total was -20.000000. loss_func: 0.480853\n",
      "resetting env. episode 1224 reward total was -20.000000. loss_func: 0.394606\n",
      "resetting env. episode 1225 reward total was -21.000000. loss_func: 0.217113\n",
      "resetting env. episode 1226 reward total was -21.000000. loss_func: 0.163139\n",
      "resetting env. episode 1227 reward total was -21.000000. loss_func: 0.149403\n",
      "resetting env. episode 1228 reward total was -21.000000. loss_func: 0.444307\n",
      "resetting env. episode 1229 reward total was -19.000000. loss_func: 1.053743\n",
      "resetting env. episode 1230 reward total was -21.000000. loss_func: 0.439386\n",
      "resetting env. episode 1231 reward total was -21.000000. loss_func: 0.281172\n",
      "resetting env. episode 1232 reward total was -20.000000. loss_func: 0.451308\n",
      "resetting env. episode 1233 reward total was -20.000000. loss_func: 0.437370\n",
      "resetting env. episode 1234 reward total was -21.000000. loss_func: 0.262291\n",
      "resetting env. episode 1235 reward total was -21.000000. loss_func: 0.230437\n",
      "resetting env. episode 1236 reward total was -21.000000. loss_func: 0.163734\n",
      "resetting env. episode 1237 reward total was -21.000000. loss_func: 0.142489\n",
      "resetting env. episode 1238 reward total was -18.000000. loss_func: 0.805269\n",
      "resetting env. episode 1239 reward total was -20.000000. loss_func: 0.497803\n",
      "resetting env. episode 1240 reward total was -21.000000. loss_func: 0.183782\n",
      "resetting env. episode 1241 reward total was -21.000000. loss_func: 0.198839\n",
      "resetting env. episode 1242 reward total was -21.000000. loss_func: 0.198605\n",
      "resetting env. episode 1243 reward total was -21.000000. loss_func: 0.261037\n",
      "resetting env. episode 1244 reward total was -19.000000. loss_func: 0.864843\n",
      "resetting env. episode 1245 reward total was -21.000000. loss_func: 0.238516\n",
      "resetting env. episode 1246 reward total was -20.000000. loss_func: 0.403174\n",
      "resetting env. episode 1247 reward total was -21.000000. loss_func: 0.275137\n",
      "resetting env. episode 1248 reward total was -21.000000. loss_func: 0.169095\n",
      "resetting env. episode 1249 reward total was -21.000000. loss_func: 0.142604\n",
      "resetting env. episode 1250 reward total was -20.000000. loss_func: 0.450620\n",
      "resetting env. episode 1251 reward total was -20.000000. loss_func: 0.377703\n",
      "resetting env. episode 1252 reward total was -21.000000. loss_func: 0.157081\n",
      "resetting env. episode 1253 reward total was -21.000000. loss_func: 0.171268\n",
      "resetting env. episode 1254 reward total was -20.000000. loss_func: 0.998940\n",
      "resetting env. episode 1255 reward total was -21.000000. loss_func: 0.444522\n",
      "resetting env. episode 1256 reward total was -21.000000. loss_func: 0.326392\n",
      "resetting env. episode 1257 reward total was -21.000000. loss_func: 0.232378\n",
      "resetting env. episode 1258 reward total was -19.000000. loss_func: 0.521752\n",
      "resetting env. episode 1259 reward total was -21.000000. loss_func: 0.226137\n",
      "resetting env. episode 1260 reward total was -20.000000. loss_func: 0.372699\n",
      "resetting env. episode 1261 reward total was -21.000000. loss_func: 0.193789\n",
      "resetting env. episode 1262 reward total was -20.000000. loss_func: 0.401234\n",
      "resetting env. episode 1263 reward total was -19.000000. loss_func: 0.504314\n",
      "resetting env. episode 1264 reward total was -21.000000. loss_func: 0.215735\n",
      "resetting env. episode 1265 reward total was -18.000000. loss_func: 0.848114\n",
      "resetting env. episode 1266 reward total was -21.000000. loss_func: 0.261865\n",
      "resetting env. episode 1267 reward total was -20.000000. loss_func: 0.516475\n",
      "resetting env. episode 1268 reward total was -21.000000. loss_func: 0.431812\n",
      "resetting env. episode 1269 reward total was -21.000000. loss_func: 0.168787\n",
      "resetting env. episode 1270 reward total was -21.000000. loss_func: 0.187818\n",
      "resetting env. episode 1271 reward total was -21.000000. loss_func: 0.270001\n",
      "resetting env. episode 1272 reward total was -20.000000. loss_func: 0.450211\n",
      "resetting env. episode 1273 reward total was -21.000000. loss_func: 0.185551\n",
      "resetting env. episode 1274 reward total was -20.000000. loss_func: 0.367745\n",
      "resetting env. episode 1275 reward total was -20.000000. loss_func: 0.431108\n",
      "resetting env. episode 1276 reward total was -21.000000. loss_func: 0.234406\n",
      "resetting env. episode 1277 reward total was -19.000000. loss_func: 0.578654\n",
      "resetting env. episode 1278 reward total was -19.000000. loss_func: 0.488698\n",
      "resetting env. episode 1279 reward total was -21.000000. loss_func: 0.163438\n",
      "resetting env. episode 1280 reward total was -21.000000. loss_func: 0.156246\n",
      "resetting env. episode 1281 reward total was -19.000000. loss_func: 0.602466\n",
      "resetting env. episode 1282 reward total was -21.000000. loss_func: 0.386645\n",
      "resetting env. episode 1283 reward total was -21.000000. loss_func: 0.270915\n",
      "resetting env. episode 1284 reward total was -20.000000. loss_func: 0.418712\n",
      "resetting env. episode 1285 reward total was -19.000000. loss_func: 0.590060\n",
      "resetting env. episode 1286 reward total was -20.000000. loss_func: 0.430514\n",
      "resetting env. episode 1287 reward total was -21.000000. loss_func: 0.247277\n",
      "resetting env. episode 1288 reward total was -20.000000. loss_func: 0.415542\n",
      "resetting env. episode 1289 reward total was -21.000000. loss_func: 0.294047\n",
      "resetting env. episode 1290 reward total was -20.000000. loss_func: 0.680330\n",
      "resetting env. episode 1291 reward total was -21.000000. loss_func: 0.289409\n",
      "resetting env. episode 1292 reward total was -17.000000. loss_func: 0.822888\n",
      "resetting env. episode 1293 reward total was -21.000000. loss_func: 0.289629\n",
      "resetting env. episode 1294 reward total was -20.000000. loss_func: 0.399256\n",
      "resetting env. episode 1295 reward total was -21.000000. loss_func: 0.337830\n",
      "resetting env. episode 1296 reward total was -21.000000. loss_func: 0.147542\n",
      "resetting env. episode 1297 reward total was -21.000000. loss_func: 0.216724\n",
      "resetting env. episode 1298 reward total was -21.000000. loss_func: 0.215507\n",
      "resetting env. episode 1299 reward total was -20.000000. loss_func: 0.629850\n",
      "resetting env. episode 1300 reward total was -21.000000. loss_func: 0.233659\n",
      "resetting env. episode 1301 reward total was -19.000000. loss_func: 1.097261\n",
      "resetting env. episode 1302 reward total was -21.000000. loss_func: 0.356550\n",
      "resetting env. episode 1303 reward total was -21.000000. loss_func: 0.367187\n",
      "resetting env. episode 1304 reward total was -21.000000. loss_func: 0.145214\n",
      "resetting env. episode 1305 reward total was -21.000000. loss_func: 0.128464\n",
      "resetting env. episode 1306 reward total was -20.000000. loss_func: 0.524347\n",
      "resetting env. episode 1307 reward total was -21.000000. loss_func: 0.334150\n",
      "resetting env. episode 1308 reward total was -21.000000. loss_func: 0.174708\n",
      "resetting env. episode 1309 reward total was -21.000000. loss_func: 0.189308\n",
      "resetting env. episode 1310 reward total was -21.000000. loss_func: 0.163840\n",
      "resetting env. episode 1311 reward total was -21.000000. loss_func: 0.175444\n",
      "resetting env. episode 1312 reward total was -21.000000. loss_func: 0.263863\n",
      "resetting env. episode 1313 reward total was -20.000000. loss_func: 0.572961\n",
      "resetting env. episode 1314 reward total was -20.000000. loss_func: 0.438606\n",
      "resetting env. episode 1315 reward total was -17.000000. loss_func: 0.996283\n",
      "resetting env. episode 1316 reward total was -20.000000. loss_func: 0.594687\n",
      "resetting env. episode 1317 reward total was -20.000000. loss_func: 0.578394\n",
      "resetting env. episode 1318 reward total was -21.000000. loss_func: 0.183493\n",
      "resetting env. episode 1319 reward total was -20.000000. loss_func: 0.362977\n",
      "resetting env. episode 1320 reward total was -21.000000. loss_func: 0.161017\n",
      "resetting env. episode 1321 reward total was -21.000000. loss_func: 0.396088\n",
      "resetting env. episode 1322 reward total was -21.000000. loss_func: 0.153296\n",
      "resetting env. episode 1323 reward total was -21.000000. loss_func: 0.209397\n",
      "resetting env. episode 1324 reward total was -21.000000. loss_func: 0.271630\n",
      "resetting env. episode 1325 reward total was -21.000000. loss_func: 0.389435\n",
      "resetting env. episode 1326 reward total was -20.000000. loss_func: 0.436977\n",
      "resetting env. episode 1327 reward total was -19.000000. loss_func: 0.557584\n",
      "resetting env. episode 1328 reward total was -21.000000. loss_func: 0.132165\n",
      "resetting env. episode 1329 reward total was -21.000000. loss_func: 0.257769\n",
      "resetting env. episode 1330 reward total was -18.000000. loss_func: 0.846235\n",
      "resetting env. episode 1331 reward total was -21.000000. loss_func: 0.213931\n",
      "resetting env. episode 1332 reward total was -20.000000. loss_func: 0.417034\n",
      "resetting env. episode 1333 reward total was -21.000000. loss_func: 0.198525\n",
      "resetting env. episode 1334 reward total was -21.000000. loss_func: 0.177616\n",
      "resetting env. episode 1335 reward total was -21.000000. loss_func: 0.093417\n",
      "resetting env. episode 1336 reward total was -21.000000. loss_func: 0.242497\n",
      "resetting env. episode 1337 reward total was -20.000000. loss_func: 0.514799\n",
      "resetting env. episode 1338 reward total was -19.000000. loss_func: 0.780986\n",
      "resetting env. episode 1339 reward total was -21.000000. loss_func: 0.251810\n",
      "resetting env. episode 1340 reward total was -19.000000. loss_func: 0.565124\n",
      "resetting env. episode 1341 reward total was -21.000000. loss_func: 0.186800\n",
      "resetting env. episode 1342 reward total was -21.000000. loss_func: 0.260238\n",
      "resetting env. episode 1343 reward total was -21.000000. loss_func: 0.162216\n",
      "resetting env. episode 1344 reward total was -21.000000. loss_func: 0.252428\n",
      "resetting env. episode 1345 reward total was -20.000000. loss_func: 0.417653\n",
      "resetting env. episode 1346 reward total was -21.000000. loss_func: 0.270936\n",
      "resetting env. episode 1347 reward total was -21.000000. loss_func: 0.431241\n",
      "resetting env. episode 1348 reward total was -21.000000. loss_func: 0.244279\n",
      "resetting env. episode 1349 reward total was -20.000000. loss_func: 0.399537\n",
      "resetting env. episode 1350 reward total was -19.000000. loss_func: 0.784710\n",
      "resetting env. episode 1351 reward total was -19.000000. loss_func: 0.514345\n",
      "resetting env. episode 1352 reward total was -21.000000. loss_func: 0.177073\n",
      "resetting env. episode 1353 reward total was -20.000000. loss_func: 0.659865\n",
      "resetting env. episode 1354 reward total was -21.000000. loss_func: 0.237398\n",
      "resetting env. episode 1355 reward total was -21.000000. loss_func: 0.313337\n",
      "resetting env. episode 1356 reward total was -20.000000. loss_func: 0.338321\n",
      "resetting env. episode 1357 reward total was -20.000000. loss_func: 0.481127\n",
      "resetting env. episode 1358 reward total was -21.000000. loss_func: 0.247573\n",
      "resetting env. episode 1359 reward total was -21.000000. loss_func: 0.324547\n",
      "resetting env. episode 1360 reward total was -21.000000. loss_func: 0.329516\n",
      "resetting env. episode 1361 reward total was -21.000000. loss_func: 0.169823\n",
      "resetting env. episode 1362 reward total was -20.000000. loss_func: 0.821145\n",
      "resetting env. episode 1363 reward total was -19.000000. loss_func: 0.541412\n",
      "resetting env. episode 1364 reward total was -21.000000. loss_func: 0.172411\n",
      "resetting env. episode 1365 reward total was -21.000000. loss_func: 0.289768\n",
      "resetting env. episode 1366 reward total was -19.000000. loss_func: 1.416849\n",
      "resetting env. episode 1367 reward total was -20.000000. loss_func: 0.489135\n",
      "resetting env. episode 1368 reward total was -21.000000. loss_func: 0.184124\n",
      "resetting env. episode 1369 reward total was -19.000000. loss_func: 0.757419\n",
      "resetting env. episode 1370 reward total was -21.000000. loss_func: 0.245128\n",
      "resetting env. episode 1371 reward total was -21.000000. loss_func: 0.175887\n",
      "resetting env. episode 1372 reward total was -20.000000. loss_func: 0.647377\n",
      "resetting env. episode 1373 reward total was -21.000000. loss_func: 0.209971\n",
      "resetting env. episode 1374 reward total was -20.000000. loss_func: 0.843797\n",
      "resetting env. episode 1375 reward total was -20.000000. loss_func: 0.528606\n",
      "resetting env. episode 1376 reward total was -21.000000. loss_func: 0.350917\n",
      "resetting env. episode 1377 reward total was -19.000000. loss_func: 0.496383\n",
      "resetting env. episode 1378 reward total was -19.000000. loss_func: 0.659195\n",
      "resetting env. episode 1379 reward total was -21.000000. loss_func: 0.257681\n",
      "resetting env. episode 1380 reward total was -21.000000. loss_func: 0.186899\n",
      "resetting env. episode 1381 reward total was -21.000000. loss_func: 0.582222\n",
      "resetting env. episode 1382 reward total was -18.000000. loss_func: 0.859507\n",
      "resetting env. episode 1383 reward total was -21.000000. loss_func: 0.487280\n",
      "resetting env. episode 1384 reward total was -21.000000. loss_func: 0.268615\n",
      "resetting env. episode 1385 reward total was -21.000000. loss_func: 0.175921\n",
      "resetting env. episode 1386 reward total was -21.000000. loss_func: 0.309200\n",
      "resetting env. episode 1387 reward total was -21.000000. loss_func: 0.142214\n",
      "resetting env. episode 1388 reward total was -20.000000. loss_func: 0.487204\n",
      "resetting env. episode 1389 reward total was -21.000000. loss_func: 0.183156\n",
      "resetting env. episode 1390 reward total was -21.000000. loss_func: 0.195835\n",
      "resetting env. episode 1391 reward total was -21.000000. loss_func: 0.269105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f261f6f6843b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0maprob\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;31m# roll the dice!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;31m# a \"fake label\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# record various intermediates (needed later for backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ModelName = 'signabs_RL_pong_RMSprop'\n",
    "# ModelName = 'expolike_RL_pong_RMSprop';\n",
    "ModelFile = 'Models/'+ModelName+'.ckpt';\n",
    "render = False;\n",
    "resume = 1;\n",
    "batch_size=1;\n",
    "learning_rate = 10e-2\n",
    "\n",
    "# ModelName='save.ckpt'\n",
    "# fname = \n",
    "# with tf.Session() as sess:\n",
    "# gdcmd='grad_dict = {'+','.join(['Grad%d: gradBuffer[%d]'%(i,i) for i,k in enumerate(tvars)])+'}';\n",
    "while True:\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True));\n",
    "    print(sess.run(tf.constant('init')))\n",
    "#     gradBuffer = [np.zeros_like(v) for v in sess.run(tvars)]\n",
    "#     grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "    if resume: \n",
    "#         tf.train.import_meta_graph(ModelFile+'.meta')    \n",
    "        oSaver = tf.train.Saver()\n",
    "#         oSaver.restore(sess,ModelFile)\n",
    "#         oSaver = tf.train.import_meta_graph(ModelFile+'.meta')    \n",
    "        oSaver.restore(sess,ModelFile)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    while True:\n",
    "        if render: env.render()\n",
    "\n",
    "        # preprocess the observation, set input to network to be difference image\n",
    "        cur_x = prepro(observation)\n",
    "        diff_x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "        x = np.reshape(diff_x,[1,D1,D2,1]);\n",
    "#         aprob = sess.run(probability,feed_dict={observations: x})\n",
    "        aprob = 0.5;\n",
    "        action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "        y = 1. if action == 2 else 0. # a \"fake label\"\n",
    "        tpred = sess.run(score,feed_dict={observations: x, input_y:np.array([[y]])})    \n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) # observation\n",
    "        ys.append(y)\n",
    "        tpreds.append(tpred);\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys);\n",
    "            epr = np.vstack(drs)\n",
    "            eptpred=np.vstack(tpreds);\n",
    "            xs,hs,dlogps,drs,ys,tpreds = [],[],[],[],[],[] # reset array memory\n",
    "            \n",
    "#             # compute the discounted reward backwards through time\n",
    "#             discounted_epr = discount_rewards(epr)\n",
    "#             # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "#             discounted_epr -= np.mean(discounted_epr)\n",
    "#             discounted_epr /= np.std(discounted_epr)\n",
    "#             discounted_epr = discount_rewards(epr);\n",
    "            time_epr=time_rewards(epr);\n",
    "#            curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "            lst =  sess.run([loss,updateGrads],feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "            curr_loss = lst[0];\n",
    "#             curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "            \n",
    "#             tGrad = sess.run([loss,newGrads,updateGrads],feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "#             for ix,grad in enumerate(tGrad):\n",
    "#                 gradBuffer[ix] += grad\n",
    "            \n",
    "            # perform rmsprop parameter update every batch_size episodes\n",
    "            if episode_number % batch_size == 0:\n",
    "                pass\n",
    "#                 grad_dict={varname:}\n",
    "#                 exec(gdcmd);\n",
    "#                 sess.run(updateGrads,feed_dict={Grad0: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "#                 sess.run(updateGrads)\n",
    "#                 sess.run(updateGrads,feed_dict={newGrads:tGrad})\n",
    "#                 sess.run(updateGrads,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "             \n",
    "                \n",
    "#                 gradBuffer = [np.zeros_like(v) for v in sess.run(tvars)]\n",
    "    \n",
    "            # boring book-keeping\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print 'resetting env. episode %d reward total was %f. loss_func: %f' % (episode_number, reward_sum, curr_loss)\n",
    "            if episode_number % 10  == 9: \n",
    "                oSaver = tf.train.Saver()\n",
    "                oSess = sess\n",
    "                oSaver.save(oSess, ModelFile) \n",
    "#                 pickle.dump(tf, open('save.p', 'wb'))\n",
    "            reward_sum = 0\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "\n",
    "        if reward != 0: # Pong has either +1 or -1 reward exactly when game ends. \n",
    "            bys=lookback(ys);\n",
    "            byss.append(bys);\n",
    "            rss.append(reward);\n",
    "            if len(byss)-1 == 100:\n",
    "                    byss.pop(0);\n",
    "                    rss.pop(0);\n",
    "\n",
    "            pass;\n",
    "    #     print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
