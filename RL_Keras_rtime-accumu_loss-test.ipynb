{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "# gamma = 1-0.\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "# resume = False;\n",
    "render = False\n",
    "# render = True\n",
    "backlen=20;\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "def lookback(lst):\n",
    "    lst = lst[-backlen:];\n",
    "#     np.pad(lst,(20-lst.size,), 'constant', constant_values=0);\n",
    "    if len(lst) != backlen:\n",
    "        lst = [None]*(backlen-len(lst)) + lst;\n",
    "    return(lst)\n",
    "\n",
    "def time_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    grad = 0;\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "#         grad = grad * gamma + r[t] ;\n",
    "        if r[t] != 0: \n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "            grad = 2 * (r[t] > 0) - 1; \n",
    "        running_add = running_add + grad;\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "def quickax():\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initiliase topology\n",
    "* set input and output\n",
    "* set optimiser\n",
    "* set routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "resume = 1;\n",
    "# resume = True;\n",
    "render = False;\n",
    "render = True;\n",
    "H=5;\n",
    "k=4;\n",
    "gamma = 0.99;\n",
    "D1=80;D2=80;\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "from keras.layers import Input, Dense, convolutional,core,concatenate,Flatten\n",
    "from keras.models import Model,load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "def quickax():\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    return ax1\n",
    "def savemodel(m,ModelFile):\n",
    "    # serialize model to JSON\n",
    "    model_json = m.to_json()\n",
    "    with open(ModelFile+'.json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(ModelFile+'.h5')\n",
    "    pickle.dump(losshist,open(ModelFile+'.p', \"wb\"))\n",
    "    print(\"Saved model to disk at \"+ModelFile)\n",
    "# savemodel(model,ModelFile)\n",
    "\n",
    "# load json and create model\n",
    "def loadmodel(ModelFile):\n",
    "    global losshist,episode_number\n",
    "    json_file = open(ModelFile+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(ModelFile+'.h5')\n",
    "#     print(\"Saved model to disk at \"+ModelFile)\n",
    "    losshist = pickle.load(open(ModelFile+'.p', 'rb'))\n",
    "    episode_number = len(losshist.losses);\n",
    "    print(\"Model loaded from disk at \"+ModelFile)\n",
    "    return(loaded_model)\n",
    "# model = loadmodel(ModelFile)\n",
    "class LossCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "class LossHist():\n",
    "    def __init__(self):\n",
    "        self.losses=[];\n",
    "    def add(self,n,loss):\n",
    "        l = len(self.losses);\n",
    "        if n == l+1:\n",
    "            self.losses.append(loss);\n",
    "        else:\n",
    "            self.losses += [0]*(n-l);\n",
    "            self.losses = self.losses[:n];\n",
    "            self.losses[n-1] = loss;\n",
    "            print('Loss history has changed ')\n",
    "    def vis(self,ax):\n",
    "        ax.plot(self.losses,'-');\n",
    "        pass\n",
    "\n",
    "def lossfunc(y_true,y_pred):\n",
    "    return K.mean(K.mean( K.square( y_pred / (K.abs(y_true)+1) - 1)  )); \n",
    "# tpr=time_epr;\n",
    "def decouple(tpr):\n",
    "    tpr1=np.maximum(tpr,0);\n",
    "    tpr2=np.minimum(tpr,0);\n",
    "    tpr1[tpr1==0]=np.maximum(np.max(tpr1),20);\n",
    "    tpr2[tpr2==0]=np.minimum(np.min(tpr2),-20);\n",
    "    tpr2=-tpr2;\n",
    "    time_epr = np.hstack([tpr1,tpr2])\n",
    "    return(time_epr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_input = Input(shape=(D1,D2,1,))\n",
    "po_input = Input(shape=(1,));\n",
    "conv1 = convolutional.Conv2D(filters=H,\n",
    "                             kernel_size=(k,k),\n",
    "                            strides=(1,1),\n",
    "                            padding='same',\n",
    "                            activation='relu')(x_input)\n",
    "den1 = Flatten()(Dense(units=5*H,\n",
    "             activation='relu')(conv1))\n",
    "\n",
    "den1c = concatenate([den1,po_input]);\n",
    "score = Dense(units=2,\n",
    "              activation = 'relu')(den1c)\n",
    "\n",
    "model = Model(inputs=[x_input,po_input], outputs=[score])\n",
    "\n",
    "optimiser = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=lossfunc)\n",
    "history = LossCallback()\n",
    "# log = model.fit([epx,epy], time_epr,callbacks=[history])\n",
    "# loss = model.train_on_batch([epx,epy], time_epr)\n",
    "# print(history.losses[-1])\n",
    "# x = Dense(64, activation='relu')(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xinput.shape\n",
    "del model\n",
    "del losshist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.0300502315163613)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAADoCAYAAAA659JnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmJVKEf8fdsWlBbJsyXEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f323c0bbf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# losshist_temp=losshist;\n",
    "\n",
    "ax = quickax();\n",
    "losses = losshist.losses;\n",
    "losshist = LossHist();\n",
    "losshist.losses = losses;\n",
    "losshist.vis(ax);\n",
    "ax.set_ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-429f7fced1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# discount_rewards(epdlogp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# discount_rewards(epdlogp)\n",
    "x.size\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig=plt.figure(figsize=[5,5])\n",
    "ax1=plt.subplot()\n",
    "# ax1.plot(time_epr)\n",
    "# ax1.plot(discounted_epr)\n",
    "ax1.scatter(abs(time_epr),eptpred)\n",
    "# ax1.set_xlim([0, 200])\n",
    "# ax1.set_xlim([800, 1000])\n",
    "# ax1.set_ylim([-10, 000])\n",
    "# ax1.imshow(eph[:500,:500].T)\n",
    "time_epr.size\n",
    "# tpreds.size\n",
    "D1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-05 19:14:52,604] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk at Models/H5k4_RL_pong_RMSprop_accumulloss_2event.ckpt\n",
      "[0.59371918, 0.45037723, 0.17339683, 0.20524687, 0.43743935]\n",
      "309\n",
      "Loss history has changed \n",
      "resetting env. episode 1 reward total was -20.000000. loss_func: 0.431941\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "ys=[];byss=[];rss=[];tpreds=[];\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0;\n",
    "\n",
    "ModelName = 'H5k4_RL_pong_RMSprop_accumulloss_2event'\n",
    "ModelFile = 'Models/'+ModelName+'.ckpt';\n",
    "render = False;\n",
    "resume = 1;\n",
    "batch_size=1;\n",
    "learning_rate = 1e-2\n",
    "optimiser = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# episode_number = 0;\n",
    "\n",
    "if 'losshist' in locals():\n",
    "    pass\n",
    "else:\n",
    "    losshist = LossHist();\n",
    "\n",
    "if True:\n",
    "    if resume:\n",
    "        model = loadmodel(ModelFile)    \n",
    "#         losshist=losshist_temp;\n",
    "        print(losshist.losses[-5:])\n",
    "        print(len(losshist.losses))\n",
    "        \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss=lossfunc)\n",
    "    episode_number = 0;\n",
    "    while episode_number<1:\n",
    "        if render: env.render()\n",
    "\n",
    "        # preprocess the observation, set input to network to be difference image\n",
    "        cur_x = prepro(observation)\n",
    "        diff_x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "        x = np.reshape(diff_x,[1,D1,D2,1]);\n",
    "        \n",
    "        # Sample action and label it \n",
    "        aprob = 0.5;\n",
    "        action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "        y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "        \n",
    "        # Run predictor\n",
    "        tpred = model.predict([x,np.array([[y]])]);\n",
    "#         tpred = model.predict({'input_1':x,'input_2':np.array([[y]])});\n",
    "        \n",
    "    \n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) # observation\n",
    "        ys.append(y)\n",
    "        tpreds.append(tpred);\n",
    "#         spreds.append(spred);\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "        \n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys);\n",
    "            epr = np.vstack(drs)\n",
    "            eptpred=np.vstack(tpreds);\n",
    "#             epspred=np.vstack()\n",
    "            xs,hs,dlogps,drs,ys,tpreds = [],[],[],[],[],[] # reset array memory\n",
    "            \n",
    "#             time_epr=time_rewards(epr);\n",
    "            tpr=time_rewards(epr);\n",
    "            time_epr = decouple(tpr);\n",
    "            \n",
    "            ## train the critic networks \n",
    "            curr_loss = model.train_on_batch([epx,epy], time_epr)\n",
    "            losshist.add(episode_number,curr_loss);\n",
    "            \n",
    "            ## train the actor network, with signal from critics weighted by the loss.\n",
    "#             critic = calc_expt(eptpred);\n",
    "#             rsignal = merge_signal(epr,eptpred)\n",
    "#             model_actor.train_on_batch([epx,epy],rsignal); ### finish this\n",
    "            \n",
    "            if episode_number % batch_size == 0:\n",
    "                #accumulate gradient over batch where appropriate\n",
    "                pass\n",
    "   \n",
    "            # boring book-keeping\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print 'resetting env. episode %d reward total was %f. loss_func: %f' % (episode_number, reward_sum, curr_loss)\n",
    "            if episode_number % 10  == 9: \n",
    "                savemodel(model,ModelFile)\n",
    "\n",
    "            reward_sum = 0\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "            #     print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39363493, -0.39362618, -0.39364489, ..., -0.39366304,\n",
       "       -0.39366303, -1.00000001])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discount_rewards(epdlogp)\n",
    "\n",
    "# z=x;\n",
    "# z[:,0]= np.convolve(x[:,0], [1,-2,1], mode='same')\n",
    "# z[:,1]= np.convolve(x[:,1], [1,-2,1], mode='same')\n",
    "# eptpred = z;\n",
    "deptpred = np.copy(eptpred);\n",
    "x= np.copy(eptpred);\n",
    "\n",
    "deptpred[:,0] = np.convolve(x[:,0], [0,-1,1], mode='same');\n",
    "deptpred[:,1] = np.convolve(x[:,1], [0,-1,1], mode='same');\n",
    "x[:,0] = np.convolve(x[:,0], [1./3,1./3,1./3], mode='same');\n",
    "x[:,1] = np.convolve(x[:,1], [1./3,1./3,1./3], mode='same');\n",
    "# deptpred[:,0] = np.convolve(abs(deptpred[:,0]), [1./3,1./3,1./3], mode='same');\n",
    "# deptpred[:,1] = np.convolve(abs(deptpred[:,1]), [1./3,1./3,1./3], mode='same');\n",
    "\n",
    "# deptpred[:,1] = np.convolve(x[:,1], [0,-1,1], mode='same');\n",
    "\n",
    "\n",
    "\n",
    "def calc_expt(eptpred,rd=np.array([[1,-1]])):\n",
    "    prep = np.exp(-eptpred)\n",
    "    z = np.expand_dims(np.sum(prep,axis = 1),1);\n",
    "    rz = np.reciprocal(z)\n",
    "#     p = prep * z,;\n",
    "#     rd = np.array([[1,-1]]);\n",
    "    expt = (np.sum(prep*rd,axis = 1)*rz.T).T;\n",
    "    return expt\n",
    "\n",
    "\n",
    "def merge_signal(epr,eptpred):\n",
    "    rd = discount_rewards(epr);\n",
    "    critic = calc_expt(eptpred);\n",
    "    wt =np.expand_dims(np.array( [1, np.exp(-curr_loss)]),1);\n",
    "    vct = np.array([epr,critic]).squeeze();\n",
    "    signal = np.sum(vct*wt,axis=0)/np.sum(wt,axis=0);\n",
    "    return signal \n",
    "# signal\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-539efbf64dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# wt.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# epr.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "vct.shape\n",
    "# wt.shape\n",
    "# epr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-210-bf60e0d4a0ab>, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-210-bf60e0d4a0ab>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    wt =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig=plt.figure(figsize=[10,10])\n",
    "ax1=plt.subplot(221)\n",
    "\n",
    "ax1.plot(time_epr)\n",
    "ax1b=ax1.twinx()\n",
    "ax1b.plot(critic)\n",
    "\n",
    "\n",
    "# ax1b.plot(expt)\n",
    "\n",
    "# xcritic= np.convolve(critic, [0,-1,1], mode='same');\n",
    "\n",
    "# ax1.plot(xcritic)\n",
    "\n",
    "# ax1.plot(eptpred,'-')\n",
    "# ax1.plot(x,'-')\n",
    "\n",
    "\n",
    "ax2=plt.subplot(222)\n",
    "ax2b=ax2.twinx();\n",
    "# ax2.plot(eptpred[1:,: ])\n",
    "# ax2b.plot(np.diff(eptpred[:,:],axis=0,n=1)/eptpred[:-1,],'--')\n",
    "# ax1.plot(eptpred[:,1])\n",
    "\n",
    "ax2.plot(eptpred[:-1,1])\n",
    "ax2b.plot(abs(deptpred[:,1])/x[:,1],'--r')\n",
    "# ax2.plot(eptpred[:-1,0])\n",
    "# ax2b.plot(abs(deptpred[:,0])/x[:,0],'--r')\n",
    "\n",
    "# ax1.plot(np.sign(-(epspred-.5))*2*(eptpred-1))\n",
    "\n",
    "# ax1.plot(discounted_epr)\n",
    "# ax1.scatter(abs(time_epr),eptpred)\n",
    "# ax1.set_xlim([0, 500])\n",
    "# ax1.set_xlim([500,1000])\n",
    "ax1.set_xlim([800,1200])\n",
    "ax1.set_ylim(top = 20)\n",
    "ax2.set_xlim([600, 850])\n",
    "ax2b.set_ylim(top =2.5)\n",
    "\n",
    "\n",
    "# ax1.imshow(eph[:500,:500].T)\n",
    "# tpreds.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1314)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prep\n",
    "# np.min(prep)\n",
    "np.multiply(np.sum(prep*rd,axis = 1),rz.T).shape\n",
    "# (rz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1631, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_epr.shape\n",
    "# print(time_epr.ravel())\n",
    "# time_epr\n",
    "# tpreds\n",
    "# H\n",
    "# np.expand_dims(epx,1).shape\n",
    "# curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "# epx.shape\n",
    "# tf.reshape(epy);\n",
    "# D*H\n",
    "# oSaver = tf.train.Saver()\n",
    "# print(sess.run(score,feed_dict={xinput: epx, input_y: epy, rtime: time_epr}))\n",
    "# oSess = sess\n",
    "# oSaver.save(oSess, ModelFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
