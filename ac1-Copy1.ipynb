{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, Flatten\n",
    "from keras.layers import Input, Dense, convolutional,core,Flatten,Reshape,Concatenate\n",
    "from keras.models import Model,load_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "standardise = lambda rewards: (rewards - np.mean(rewards,keepdims =1)) / np.std(rewards ) ;\n",
    "\n",
    "\n",
    "class temp:\n",
    "    def __init__(self):\n",
    "        self.clean();\n",
    "    def clean(self):\n",
    "        self.states = []\n",
    "        self.gradients = []\n",
    "        self.rewards = []\n",
    "        self.probs = []\n",
    "        self.tpreds = [];\n",
    "        self.aprobs = [];\n",
    "        self.actions = [];\n",
    "    def remember(self,state,action,prob,reward,tpred):\n",
    "        self.states.append(state);\n",
    "        self.probs.append(prob)\n",
    "        self.rewards.append(reward);\n",
    "        self.tpreds.append(tpred);\n",
    "        y = np.zeros([action_size])\n",
    "        y[action] = 1\n",
    "        self.actions.append(y);\n",
    "        self.gradients.append(np.array(y).astype('float32') - prob)\n",
    "        \n",
    "class PGAgent:\n",
    "    def __init__(self, state_size, action_size, tmp):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "#         self.states = []\n",
    "#         self.gradients = []\n",
    "#         self.rewards = []\n",
    "#         self.probs = []\n",
    "        self.tmp = tmp;\n",
    "        self.model = self._build_model()\n",
    "        self.summary = self.model.summary;\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((80, 80, 1), input_shape=(self.state_size,)))\n",
    "        model.add(Convolution2D(5, (6, 6), subsample=(3, 3), border_mode='same',\n",
    "                                activation='relu', init='he_uniform'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(20, activation='relu', init='he_uniform'))\n",
    "        model.add(Dense(20, activation='relu', init='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        opt = Adam(lr=self.learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, prob, reward):\n",
    "        y = np.zeros([self.action_size])\n",
    "        y[action] = 1\n",
    "        self.gradients.append(np.array(y).astype('float32') - prob)\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def act(self, state):\n",
    "        # state = state.reshape([1, state.shape[0]])\n",
    "        aprob = self.model.predict(state, batch_size=1).flatten()\n",
    "        self.tmp.aprobs.append(aprob)\n",
    "        prob = aprob / np.sum(aprob)\n",
    "        action = np.random.choice(self.action_size, 1, p=prob)[0]\n",
    "        return action, prob\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, rewards.size)):\n",
    "            if rewards[t] != 0:\n",
    "                running_add = 0\n",
    "            running_add = running_add * self.gamma + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "\n",
    "    def train(self,rewards):\n",
    "        gradients = np.vstack(self.tmp.gradients)\n",
    "        # rewards = np.vstack(self.rewards)\n",
    "        # rewards = self.discount_rewards(rewards)\n",
    "#         rewards = (rewards - np.mean(rewards,keepdims=1)) / np.std(rewards)\n",
    "        gradients *= rewards\n",
    "        X = np.squeeze(np.vstack([self.tmp.states]))\n",
    "        Y = self.tmp.aprobs + self.learning_rate * np.squeeze(np.vstack([gradients]))\n",
    "        self.model.train_on_batch(X, Y)\n",
    "\n",
    "    def clean(self): \n",
    "        self.tmp.clean();\n",
    "#         self.states, self.probs, self.gradients, self.rewards = [], [], [], []\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "def preprocess(I):\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(np.float).ravel()\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "\n",
    "    state_size = 80 * 80\n",
    "    action_size = env.action_space.n\n",
    "    agent = PGAgent(state_size, action_size,temp())\n",
    "    \n",
    "agent.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class critic:\n",
    "    def __init__(self,state_size,action_size,tmp):\n",
    "        self.D = state_size;\n",
    "        self.action_size = action_size;\n",
    "        self.H = 5;\n",
    "        self.tmp = tmp;\n",
    "        self.model = self.model_init();\n",
    "        self.predict = self.model.predict;\n",
    "        self.summary = self.model.summary;\n",
    "        \n",
    "    def model_init(self):\n",
    "        def lossfunc(y_true,y_pred):\n",
    "            return K.mean(K.mean( K.square( y_pred / (K.abs(y_true)+1) - 1)  )); \n",
    "\n",
    "        D = self.D;\n",
    "        H = self.H;\n",
    "        x_input = Input(shape=(D,));\n",
    "#         x_input = Input();        \n",
    "        po_input = Input(shape=(self.action_size,));\n",
    "        conv1 = convolutional.Conv2D(filters=H,\n",
    "                             kernel_size=(6,6),\n",
    "                            strides=(3,3),\n",
    "                            padding='same',\n",
    "                            activation='relu')(Reshape((80, 80,1))(x_input))\n",
    "        den1 = Flatten()(\n",
    "            Dense(units=5*H,\n",
    "             activation='relu')(conv1)\n",
    "            )\n",
    "        den1c = Concatenate()([den1,po_input]);\n",
    "        \n",
    "        score = Dense(units=2,\n",
    "              activation = 'relu')(den1c)\n",
    "    \n",
    "        model = Model(inputs=[x_input,po_input], outputs=[score])\n",
    "#         optimiser = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "        model.compile(optimizer='rmsprop',\n",
    "              loss=lossfunc)\n",
    "        return model\n",
    "    def time_rewards(self,r):\n",
    "        \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        grad = 0;\n",
    "        for t in reversed(xrange(0, r.size)):\n",
    "    #         grad = grad * gamma + r[t] ;\n",
    "            if r[t] != 0: \n",
    "                running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "                grad = 2 * (r[t] > 0) - 1; \n",
    "            running_add = running_add + grad;\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r\n",
    "    \n",
    "    def decouple(self,tpr):\n",
    "        tpr1=np.maximum(tpr,0);\n",
    "        tpr2=np.minimum(tpr,0);\n",
    "        tpr1[tpr1==0]=np.maximum(np.max(tpr1),20);\n",
    "        tpr2[tpr2==0]=np.minimum(np.min(tpr2),-20);\n",
    "        tpr2=-tpr2;\n",
    "        time_epr = np.hstack([tpr1,tpr2])\n",
    "        return(time_epr)\n",
    "    \n",
    "    def vari_signal(self):\n",
    "        x = eptpred;\n",
    "        x_diff = convolve2d(x,[[0.],[-1.],[1.]],mode='same');\n",
    "        x_mean = convolve2d(x,[[0.],[.5],[.5]],mode='same');\n",
    "        x_vari = np.mean(abs(x_diff)/x_mean,axis = 1,keepdims = 1);\n",
    "        return(x_vari);\n",
    "    #     sx_avg = x_avg * rsign;\n",
    "    return x_vari\n",
    "    def train(self):\n",
    "        epx = np.vstack(self.tmp.states);\n",
    "        epy = np.vstack(self.tmp.actions);\n",
    "        epr = np.vstack(self.tmp.rewards);\n",
    "        tpr=self.time_rewards(epr);\n",
    "        time_epr = self.decouple(tpr);\n",
    "        curr_loss = self.model.train_on_batch([epx,epy], time_epr);\n",
    "        return curr_loss\n",
    "        \n",
    "    def save(self,name):\n",
    "        print('no method to save');\n",
    "    def load(self,name):\n",
    "        print('no method to load')\n",
    "cagent = critic(state_size,action_size,temp());\n",
    "cagent.model_init();\n",
    "cagent.summary();\n",
    "pbatch = lambda x: np.reshape(x,(1,-1))\n",
    "# tpred = cagent.predict([pbatch(x),pbatch(prob)])\n",
    "# print(tpred)\n",
    "# tpred = cagent.predict([x,prob]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-12 13:58:58,969] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_14 (Reshape)         (None, 80, 80, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 27, 27, 5)         185       \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 3645)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 20)                72920     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 73,651\n",
      "Trainable params: 73,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(3, 3))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6400, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "\n",
    "    state_size = 80 * 80\n",
    "    action_size = env.action_space.n\n",
    "    agent = PGAgent(state_size, action_size,temp())\n",
    "    \n",
    "agent.summary();\n",
    "state_size,action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-12 14:00:55,116] Making new env: Pong-v0\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(5, (6, 6), kernel_initializer=\"he_uniform\", activation=\"relu\", padding=\"same\", strides=(3, 3))`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n",
      "/home/shouldsee/.local/lib/python2.7/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, kernel_initializer=\"he_uniform\", activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode\t1\tScore\t-21.000000\tLoss\t0.999420\n",
      "Episode\t2\tScore\t-21.000000\tLoss\t0.992095\n",
      "Episode\t3\tScore\t-21.000000\tLoss\t0.965931\n",
      "Episode\t4\tScore\t-20.000000\tLoss\t0.938416\n",
      "Episode\t5\tScore\t-20.000000\tLoss\t0.891511\n",
      "Episode\t6\tScore\t-21.000000\tLoss\t0.778717\n",
      "Episode\t7\tScore\t-20.000000\tLoss\t0.793197\n",
      "Episode\t8\tScore\t-20.000000\tLoss\t0.761182\n",
      "Episode\t9\tScore\t-20.000000\tLoss\t0.735654\n",
      "Episode\t10\tScore\t-21.000000\tLoss\t0.564067\n",
      "no method to save\n",
      "Episode\t11\tScore\t-21.000000\tLoss\t0.525580\n",
      "Episode\t12\tScore\t-19.000000\tLoss\t0.718815\n"
     ]
    }
   ],
   "source": [
    "AgentName = 'ac1'\n",
    "AgentFile = 'Models/%s.h5'%AgentName;\n",
    "cAgentFile = 'Models/%s-critic.h5'%AgentName;\n",
    "\n",
    "LogName = 'Models/%s.log'%AgentName;\n",
    "# LogFile = open(LogName,'a');\n",
    "resume = 0;\n",
    "render = 0;\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    state = env.reset()\n",
    "    prev_x = None\n",
    "    score = 0\n",
    "    episode = 0\n",
    "\n",
    "    state_size = 80 * 80\n",
    "    action_size = env.action_space.n\n",
    "    tmp = temp();\n",
    "    agent = PGAgent(state_size, action_size,tmp)\n",
    "    cagent = critic(state_size,action_size,tmp);\n",
    "#     agent.summary();\n",
    "#     cagent.summary();\n",
    "    \n",
    "\n",
    "    if resume:\n",
    "        agent.load(AgentFile)\n",
    "        cagent.load(cAgentFile)\n",
    "    while episode > -1:\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        cur_x = preprocess(state)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(state_size)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        action, prob = agent.act(np.expand_dims(x,1).T)\n",
    "        avct = np.zeros([action_size])\n",
    "        avct[action] = 1\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        ### manage the list \n",
    "#         agent.remember(x, action, prob, reward)\n",
    "        tpred = cagent.predict([pbatch(x),pbatch(avct)]);\n",
    "        tmp.remember(x,action,prob,reward,tpred);\n",
    "#         cagent.remember(x, action, prob, reward, tpred);\n",
    "        \n",
    "        if done:\n",
    "            episode += 1\n",
    "            rewards = np.vstack(tmp.rewards)\n",
    "            \n",
    "            \n",
    "            ## process reward on a per event basis\n",
    "            curr_loss = cagent.train();\n",
    "#             losshist.add(episode_number,curr_loss);                        \n",
    "#             rsignal = merge_signal(epr,eptpred,.5);\n",
    "            rewards = standardise(agent.discount_rewards(rewards))\n",
    "            \n",
    "            agent.train(rewards)\n",
    "            tmp.clean();\n",
    "#             agent.record()\n",
    "            msg = '%s\\t%d\\t%s\\t%f\\t%s\\t%f' % ('Episode',episode,'Score', score,'Loss',curr_loss);\n",
    "            print(msg);\n",
    "            if episode == 1  and resume == 0:\n",
    "                open(LogName,'w').close();\n",
    "            with open(LogName,'a+') as LogFile:\n",
    "                LogFile.write(msg+'\\n');\n",
    "            \n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            prev_x = None\n",
    "            if episode > 1 and episode % 10 == 0:\n",
    "                agent.save(AgentFile)\n",
    "                cagent.save(cAgentFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print >> LogFile, 'Hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogFile = open('Models/%s.log'%AgentName,'a')\n",
    "LogFile.write('Hi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
