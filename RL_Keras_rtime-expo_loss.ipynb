{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "# gamma = 1-0.\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "# resume = False;\n",
    "render = False\n",
    "# render = True\n",
    "backlen=20;\n",
    "\n",
    "from keras.layers import Input, Dense, convolutional,core,concatenate,Flatten\n",
    "from keras.models import Model,load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "def lookback(lst):\n",
    "    lst = lst[-backlen:];\n",
    "#     np.pad(lst,(20-lst.size,), 'constant', constant_values=0);\n",
    "    if len(lst) != backlen:\n",
    "        lst = [None]*(backlen-len(lst)) + lst;\n",
    "    return(lst)\n",
    "\n",
    "def time_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    grad = 0;\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "#         grad = grad * gamma + r[t] ;\n",
    "        if r[t] != 0: \n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "            grad = 2 * (r[t] > 0) - 1; \n",
    "        running_add = running_add + grad;\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "def quickax():\n",
    "    fig = plt.figure(figsize=[8,8])\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initiliase topology\n",
    "* set input and output\n",
    "* set optimiser\n",
    "* set routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "resume = 1;\n",
    "# resume = True;\n",
    "render = False;\n",
    "render = True;\n",
    "H=5;\n",
    "k=4;\n",
    "gamma = 0.99;\n",
    "D1=80;D2=80;\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "def savemodel(m,ModelFile):\n",
    "    # serialize model to JSON\n",
    "    model_json = m.to_json()\n",
    "    with open(ModelFile+'.json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(ModelFile+'.h5')\n",
    "    pickle.dump(losshist,open(ModelFile+'.p', \"wb\"))\n",
    "    print(\"Saved model to disk at \"+ModelFile)\n",
    "# savemodel(model,ModelFile)\n",
    "\n",
    "# load json and create model\n",
    "def loadmodel(ModelFile):\n",
    "    global losshist,episode_number\n",
    "    json_file = open(ModelFile+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(ModelFile+'.h5')\n",
    "#     print(\"Saved model to disk at \"+ModelFile)\n",
    "    losshist = pickle.load(open(ModelFile+'.p', 'rb'))\n",
    "    episode_number = len(losshist.losses);\n",
    "    print(\"Model loaded from disk at \"+ModelFile)\n",
    "    return(loaded_model)\n",
    "# model = loadmodel(ModelFile)\n",
    "class LossCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "class LossHist():\n",
    "    def __init__(self):\n",
    "        self.losses=[];\n",
    "    def add(self,n,loss):\n",
    "        l = len(self.losses);\n",
    "        if n == l+1:\n",
    "            self.losses.append(loss);\n",
    "        else:\n",
    "            self.losses += [0]*(n-l);\n",
    "            self.losses[n-1] = loss;\n",
    "            print('Loss history has changed ')\n",
    "    def vis(self,ax):\n",
    "        ax.plot(self.losses,'-');\n",
    "        pass\n",
    "\n",
    "def lossfunc(y_true,y_pred):\n",
    "    return K.mean( K.square( y_pred / (K.abs(y_true)+1) - 1)  ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x7f8987863950>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input = Input(shape=(D1,D2,1,))\n",
    "po_input = Input(shape=(1,));\n",
    "conv1 = convolutional.Conv2D(filters=H,\n",
    "                             kernel_size=(k,k),\n",
    "                            strides=(1,1),\n",
    "                            padding='same',\n",
    "                            activation='relu')(x_input)\n",
    "den1 = Flatten()(Dense(units=5*H,\n",
    "             activation='relu')(conv1))\n",
    "\n",
    "den1c = concatenate([den1,po_input]);\n",
    "score = Dense(units=1,\n",
    "              activation = 'relu')(den1c)\n",
    "\n",
    "model = Model(inputs=[x_input,po_input], outputs=[score])\n",
    "\n",
    "\n",
    "optimiser = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=keras.losses.mean_squared_error)\n",
    "history = LossCallback()\n",
    "\n",
    "tf.InteractiveSession()\n",
    "\n",
    "# log = model.fit([epx,epy], time_epr,callbacks=[history])\n",
    "# loss = model.train_on_batch([epx,epy], time_epr)\n",
    "# print(history.losses[-1])\n",
    "# x = Dense(64, activation='relu')(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xinput.shape\n",
    "del model\n",
    "del losshist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1.0300502315163613)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAADoCAYAAAA659JnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4W9d9//H3FwDBBW6QFClKIiVRgxq2bFmWIjtecjwS\n2x1OYzeu0zxp3DSj2fnFv6Ru6jRPR5rxa5omdVabUa/EcRRbjveIHQ1LlrUXRS2SEjfBPXF+f9wL\nECQBEDJBEYC/r+fRYwK4BA5gfnDGPedcMcaglEp+jtkugFIqPjTMSqUIDbNSKULDrFSK0DArlSI0\nzEqlCA2zUilCw6xUitAwK5UiXLP1wl6v11RWVs7WyyuVNHbt2tVqjCme6rhZC3NlZSU7d+6crZdX\nKmmIyKlYjpuymS0iPxaRZhHZH+FxEZF/F5FaEdkrIpecb2GVUtMXS5/5v4Ebozx+E1Bt/7sH+N70\ni6WUOl9ThtkY8wrQHuWQ24CfGss2IF9EyuJVQKVUbOIxmj0XOBNyu96+b1qe2NvIHQ9sxe/XJZpK\nxeKCnpoSkXtEZKeI7GxpaYl6bP/QKNvq2qlr7b1ApVMqucUjzA3AvJDbFfZ9kxhjHjDGrDXGrC0u\njj7SvroiH4B9DZ1xKKJSqS8eYd4M3G2Paq8HfMaYs9N90kXF2WSmOdlb75t+CZV6G5jyPLOIPAhc\nDXhFpB74eyANwBjzfWALcDNQC/QBH4xLwZwOVpTnsk/DrFRMpgyzMebOKR43wMfiVqIQqyryeGjH\nGUZG/bicOvNUqWgSOiGrK/LoHx7leIsOgik1lYQO86q51iDY3nodBFNqKgkd5oXebLLdTvY1aL9Z\nqakkdJgdDmHF3DwNs1IxSOgwA6yem8fBxi6GR/2zXRSlElrCh3lVRR6DI36ONfXMdlGUSmgJH+aV\nc/MAOHi2a5ZLolRiS/gwF+ekA9DROzTLJVEqsSV8mD1uFw4BX//wbBdFqYSW8GF2OITczDQ6+7Vm\nViqahA8zQF5mGr7+kdkuhlIJLYnCrM1spaLRMCuVIpImzF0aZqWiSpowa82sVHRJFWZr6bRSKpyk\nCfOo39A7NDrbRVEqYSVNmAE6+/Rcs1KRJFWYtd+sVGTJEeYsDbNSU0mOMNs1s56eUiqypAqz1sxK\nRaZhVipFJEWYPekunA7RMCsVRVKEWUTIzXBpmJWKIinCDJCf5aazT8OsVCRJE+ZcnZ+tVFRJE2Zd\nOaVUdEkVZq2ZlYosicKsA2BKRRNTmEXkRhE5IiK1IvLFMI/PF5EXRWS3iOwVkZvjXdC8zDS6BkZ0\nGaRSEUwZZhFxAt8FbgJqgDtFpGbCYV8GHjHGrAHuAP4z3gUNLIPsGdSN/ZQKJ5aaeR1Qa4ypM8YM\nAQ8Bt004xgC59s95QGP8imjJz3QDOgtMqUhiCfNc4EzI7Xr7vlBfAe4SkXpgC/CJcE8kIveIyE4R\n2dnS0nJeBc0NrmnWMCsVTrwGwO4E/tsYUwHcDPxMRCY9tzHmAWPMWmPM2uLi4vN6AV05pVR0sYS5\nAZgXcrvCvi/Uh4BHAIwxW4EMwBuPAgboYgulooslzK8D1SJSJSJurAGuzROOOQ1cByAiy7HCfH7t\n6CnoBgVKRTdlmI0xI8DHgaeBQ1ij1gdE5H4RudU+7LPAh0VkD/Ag8JcmzueQtGZWKjpXLAcZY7Zg\nDWyF3ndfyM8HgY3xLdp42W4nLl0GqVRESTMDTER0SqdSUSRNmEHnZysVTVKFuSDbTVuP7p2tVDhJ\nFeb5hVmcbu+b7WIolZCSKswLirJo9PUzMKyXqVFqoqQKc5U3G2PgjNbOSk2SVGGuLMoG4ERr7yyX\nRKnEk5RhPtmmYVZqoqQKc15WGgVZaZxs02a2UhMlVZgBKr3ZnNRmtlKTJF+Yi7I5pTWzUpMkZZj1\n9JRSkyVfmL1ZGINOHlFqguQLc2BEW/vNSo2TvGHW01NKjZN0YQ6cnjrRqs1spUIlXZjBOj116m1a\nMx9r6uZzj+7RAUA1SVKGuarowpxr/o8XjvGDV+ri8ly/3l1PR29syzeHR/28frKdnSfb2d/gY9Q/\ntgPT7/af45e76nn6wLm4lEuljqQMc6U3m0bfAAcafTEd/8Vf7eWrTxw8r9do6hrg288d4/svH8fv\nn952ZnUtPXz64T38cld9TMf/fNsp3vv9rdz+/a285zuv8sjOsW3LT9gtkodfPxPp19XbVFKG+b1r\nKyjPy+AvfrSDI+e6ox7r9xue3HeWl4+e32ahv9h+mhG/oa13iINnu2L6ndEIod/XYH3pNPr6Y3qe\np/afY1FxNj/70DpyMlzjvrQCLZI/HG9723Y1VHhJGeayvEz+98PrcTmE9/9w27g/6r6hEXadag/e\nPtXeR/fACKfb+iKGbaLBkVH+d/spLp6XD8Crta1T/s6BRh8X3/8Mv9t/dtJj++0wn+0cmPJ5OnqH\n2HmynZtXlXFldTHVJR5qm3uCj59s6+PaZSU4hHE1tlJJGWawmtoP3rOewWE/9/92rAn95V/v5/bv\nb6Wx06oF99Z3AjA06udc19RhAnhy71lae4b47LuWsLQ0h98fi16r+/qG+cjPd9E9MMLrJzsmPb6/\nwarZz8ZQM79wuBm/gU3LSwFYVOyhttn6svL1D9PeO8TlVYVcs7SER3fWMzLqj+k9qdSXtGEG6w/9\no9cs5vnDzfyhtpXXT7bz2O4GjIHX7Np0b/3kJmo0xhj++w8nWVzi4YrFXq6s9vL6yQ76h8KPHvv9\nhk89vJtzvgGKc9I52tQ96fn2283ks76pv0yeO9REaW46q+bmAbC4xENrzyC+vuFgC6TSm837LptH\nc/cgzx9unvI51dtDUocZ4IMbK5mbn8k/PnmI+35zgPK8DIqy3cEw76v3UZ6XAcQ20aS+o5+99T7u\nXDcfEeGKai9DI352nGwPe/wjO8/w4pEW7rtlBVcs9o5rEgOcae+ne2CEObkZtPQMMjQSuSYdGB7l\n5aMtXLe8FIdDACvMALUtPcFNGaq82VyzrIR5hZl8+7lj0x6gU6kh6cOckebkCzcu5eDZLg6d7eLL\n76lh42Ivrx1vY2TUz/5GH5tqSnG7HDGttgrUrBfPs2rGy6uKcDsd/D7CANqDO06zbE4Od10+n8Ul\nHs76BugeGNsOODD4tammBGOsUfJItta10Tc0yvU1pcH7AmE+3tzDydY+RKyNDdOcDj73rqUcOtvF\n429OvPSXejtK+jAD3HpROVdWe7lhRSk3rZzDFYu9tHQP8rsD5+gbGuWiinwWFGbF1Mw+Ztesi0ty\nAMh0O7msqoAXDjezZd9ZHnujnl77gu9Hm7rZU+/j9ksrEBGWlFq/E1o772/0keYUrlpSAhC13/7c\nwSay3E42LCwK3ldRkIXb5aC2pYeTbb2U5WaQkeYE4JbV5ayam8c3njmqk0hUaoRZRPifD67j+3dd\nioiwsdq6AOX3XjoOwOqKPHvW2NQ187GmHkpz04PXtgK4ZmkJda29fPQXb/CZR/bwhV/uBeBXu+px\nOYQ/WmNdrrrarkWPNYWEucHHktIcKouyAIIDcxMZY3jhcDNXVnuDYQVwOoSF3mxqm61mdqU3O/iY\nwyHce9MyGjr7+enWk1O+N5XaUiLMYP1hi1j9zLn5mVR5sznQ2EWW28nCYg+VRVmcau+dsn95rLmb\nartWDrh7QyWPfmQDT33ySv722sU8ue8sT+49y2O7G7h6aQleTzoA8wqtWvRYs9VUN8awv8HHyvI8\nyvIzgciDYEeaujnrG+DaZSWTHltU4uF4Sw+n2saHGeAd9iDdD35/guELPLJ96GwXf/Gj7doqSBAp\nE+aJNi62mqor5+bhdAgLirIZGPbT3D0YPMYYw6vHWjnYaJ068vsNtc09VJd6xj2X2+XgsspClpfl\n8onrqllRnsunH36Tlu5Bbr+0Inic0yEsKvYEm+qNvgE6+oZZOTcXT7qLnHQX5yKE+aUjVp880BwP\ntajYw6m2Pjr6hqkqyp70+Ac3VtLSPchzB5vO5yOatm11bfz+WKtufZwgYgqziNwoIkdEpFZEvhjh\nmD8TkYMickBE/je+xTx/Vyy2mtqr7VM8E7fpPdrUzV/8aAd3/Wg7n374TcCaodU3NDqpZg6V5nTw\nr7evxm8MBVlpk2rSJaWeYDN7n31abIVdhrL8jIjN7BcPN7O8LJc59sh7qMAgGDCpZgbrC2BufiY/\n334qYrlnQmCueWeU638ZY/iH3x4ITpxRM2fKMIuIE/gucBNQA9wpIjUTjqkG7gU2GmNWAJ+agbKe\nl3cs9rJsTk5wZHiB3Wc91dbLidZe3vOdV9nX4OPKai9Hmrpp7OwPhnBizTzRivI8vvFnF/G1P16F\n2zX+I6wu8dDQ2U/v4AiP7jxDflYaNWW5gDVzLVwzu2tgmJ2nOrh6aXHY11tcHBJm+32EcjqEO9fN\n47XaNupaeiY9PlPa+6ww+/oih7mzb5ifvHaSX2w/HbzPGKO1+QyIpWZeB9QaY+qMMUPAQ8BtE475\nMPBdY0wHgDFm1mcy5Gak8btPvZPL7ZHh8vxM0pzCybY+vvPCMRwCv/vUlXz53db30stHW4J93eqS\n6GEGuO3iudy8qmzS/YFR8MffbOD5w8381RVVwQGt8vyMsGF+7Vgro37DNUsnN7EBFhZnIwIiVr88\nnD+7bB4uh/DgjtNhH58JHb1WiKNdmTNQa+8MOU//2BsNXP1vL0Xsckzk9xuM0XPpU4klzHOB0EnA\n9fZ9oZYAS0TkNRHZJiI3xquA8eJ0CPMKs/j9sRZ+82Yjd12+gLK8TJaUeijLy+ClI80ca+qhOCed\n/Cz3W36dJXat/k9bDpOb4eLud1QGH5uTm0lrzyCDI+MHjF480kxOhotL5ueHfc6MNCcVBZmU52WO\nG+kOVZKTwQ0r5vDorvpJzz9TOgI1c5QwBx471twTbJY/e7CJUb+JeaHIvY/t40P/s3OapU19rjg+\nTzVwNVABvCIiq4wxnaEHicg9wD0A8+fPj9NLx66yKJsXDjeT7nJwz1ULA2Xi6qXF/HbPWRYUZQXD\n+FbNL8zC7XTQMzjCpzctITdj7BRXWb7VH27yDVLf0cc//PYg5fkZvHG6k3dWF+NyRv5uvX75nClH\nq69bXsKT+87S0NHPwuLpvY9YtPdOHebOvrE13DtPdXDN0mJeO27Nzotlrrzfb3jm4LmIX2JqTCw1\ncwMwL+R2hX1fqHpgszFm2BhzAjiKFe5xjDEPGGPWGmPWFheH7x/OpEC/+a71CyjJGRtoumpJCT2D\nIxxo7Io6+BULl9PBwuJsctJd/OXGynGPlecFTk/1841nj9LcPcC5rkH8xnDbxeVRn/e+W2r46h+t\njHpM4D21hIzYz6TzqZnBamq/eaaT7gFr0k0szexjzT109A3T1jOkTe0pxFIzvw5Ui0gVVojvAP58\nwjGPA3cCPxERL1azOz5bdMTRpQsK2PxmI39t18oBGxcX4XIII34zbuT4rbr35uX4/WbcxBMgOFK9\nZd9Zdp3q4Cu31PCXG6um/XoBxTnW+e7WC3BBemNMsM/cFUOYK4uyeP1kO26XA4dYZwViqZm3n2gD\nrFVvXQMjkz5TNWbKmtkYMwJ8HHgaOAQ8Yow5ICL3i8it9mFPA20ichB4Efi8MaZtpgr9Vr1ndTmv\nf2nTuFoZICcjjbWVBQDBKZnTcdWSYq4JM/mj3G5m/3z7aXIzXLx37bxJx0xHIMwt3ZFDMjgyyl0/\n3M62uun97+kdGmXIbvZHrZntke5Ny0vZ1+Dj2YNNXDwvn4qCzJhq5u11YwNnrT0XpsWRrGI6z2yM\n2WKMWWKMWWSM+Zp9333GmM32z8YY8xljTI0xZpUx5qGZLPR0BFYjTXTDijmkuxzT7jNHk+V2kZeZ\nxqjf8P71C8hOj9eQhSU/Mw2nQ6LWzPsbuni1tpUXp7l0MnQ/s6ma2dluJxsWFTE8ajh8rpt3Lilm\nTl7GlDWzMYbtJ9oosb+k2sK8r39//hiff3TPW3wXqSVlZ4Cdr7s3VPLi566e1kh2LMryMnA5hA9s\nqIz7czscgtfjjtpn3n3a2jwh1n3HjTHBhSWhAoNfbpdjylNTeZlprF1QGLzvqiXFzMnNpGmKmvl4\nSy+tPUPBU4ATa2ZjDA/tOM2ju+qDs/jezjTMNqdDKLfnT8+k2y+t4NPXLwk70ysevJ50WqI0R3ed\nssMcYd/xrcfbxoX3qf3nuPQfn6W+Y/zxgQkjCwqzpqyZczPTyMtKY2lpDvlZaayuyGdOXjpN3YNR\nt3IK9JffvTp8mE+399FofyH86NUTEZ/nQtvf4OOqr79Ic5juzptnOrnlO6/ys60no65tfys0zBfY\nX125kI9ds3jGnr84Jz1i39IYwxt2zRxu0Ulz9wB//sNt/NfLx4P3PX+omYFhP795s3HcsYFTTpXe\n7Cn7zPlZ1qDV529Yyt/fUoPTIczJy2TUb2iL8sWzva6dkpx01szLRwRaJ7Q4th63wn5ltZfNexpo\nntBsf622lSf3Tt6TbabtPtPJqba+cf19sDZ8/L+P7ePQ2S7+7jcH2PTNl3kmjlsma5hTjNeTHrGZ\n3egboKlrkCWlHgaG/TRNqDn2N/gwBl45NraBYaB2fHx3w7hTQ+32SHaVN5vBEX/ElVM+u5kNsKmm\nlD9eYy1MmZNrtUwirSIL9JcvX1iEy+mgMMtN64R9x7fWtVGck85Xb1vJiN/w063j56Z/7clD573F\nsvV7B/nAj3ec9+8FBLoPgf3nAh5+/QwHz3bxzfddzE8+eBnZ6a64nnnQMKeYQM0c7pzsG3YTOxCo\niU3tA/bGg3vrO/H1DVPf0Ud9Rz/L5uRwrLln3JbDHb1D1qy6AqtrEun0VGf/EPmZk8chAmGONAh2\nur2Ppq5B1lVZfe0ij3tczWyMYevxNtYvLKLSm827akr5+fZTwb3a2u0tks91DUxqOexv8PGn3/sD\nf/PzXWFf++WjLbx8tCXiopipBHaTCd1/ztc3zNefPsy6qkJuWV3GNUtLePITV/C+y+J3RkPDnGKK\nPekMj5qwTd9dpzrISHNw08o5wORBsP2NPlwOwW+sWi/QTLzvlhpcDmFzSFO7vW+I/Mw08uwBw0hN\nbV//MHlZk88NB8YMIm2jtP2E9drr7TB7PeO7D3WtvTR3DwZ3Zbl7QyWdfcO8YI/Sh556q20e22Tx\nG88c4db/eJVdpzp47lDTpBbF0Iifuhbrc3n2LS4pDXxBhV6N5D9fqsXXP8xXblkRXHfvcAjOCGdX\n3goNc4rxBs81T25q7z7dwUUV+dYmCk7H5DA3dLFpeSlZbiev1raw/UQb+VlprK8q4qolxWze0xjs\nZ3f0DlGQ7Q42ocMtgxwYHmVg2B92okdRtps0p0RsZu840U5htjs4iccK81iTNNBf3rDICvP6hUV4\nPW627LP6yK/VtgaDctReDdfUNcB3XqjlxpVz+PrtqxkeNRyYMAp+vKWHEfs9PnPwrfVnm7sGcTqE\n3qFR6lp68PsNv97dwPU1pdSU576l54yFhjnFFNu7nkwc0R4YHuVAYxeXLCiwF51kjtsTraN3iIbO\nftbMz2f9wiJePdbKtrp2LqssxOEQblszl7O+gWCN2d47RGHWWJjDLYMMNL3DhdnhEEpyMiKentp+\noo3LKguCtZjXkz5usGxrXRtzcjOCS0KdDuGGFXN44XAz/UOjbD3exjurvWSmOYObNAZOy334yoW8\nc4k1nfjNM+P7tYErpGxaXsK2uvZxc8tjda5rgHWVVotiT72P3Wc6aO4eDLvKLp40zCmmOMdq9k6s\nmd8808mI33DJfGumW9WEPdEC/eEV5XlcsdjLybY+Trf3sd5uxm5aXoLLIbxiXxCgo2+Iguw08gNh\nDlMzB2rr/DDNbCDixJHGzn7OtPezrmpsY8Mij5veoVH6h0atwbG6NjYsKgqGHeDdq8roHx7lwR2n\nqWvtZeNiL0tKPWNhPtOJ2+mgpjyX0twMyvMyggEPOHyumzSn8JGrFjHqN8Fme6wGhkfx9Q+zYVER\n2W4ne+s7eWrfOdxOR9gtoeJJw5xiij2TF1s8c+Acf/PzXXjSXaxdYIV5QVE2J9t6gwNlgZ1AVpTn\ncqW9ISLA5XafNcvtYklpTvC4jr5hCkOa2YEwv3y0JXiJHl+UmhmsQbBwUzpft9c+B17bel+BeeeD\n1Hf009ozFJyCG7CuqpCibDffeu4oAO9Y5KW6NCfYzN59upOa8lzSXdYKrIvn54epmbtYVOzhkvkF\nlOam88yB8f3mffW+qFc4ae6yPveyvAxWzM1jT72Pp/af44pqLzkZMzuvXMOcYnIzXbidjmD/8j9f\nquWen+2iPD+Txz+2kYJsq+au9Fp7ojXZf3z7G7uYm59Jgd1PLc1NJyfDxfKysT7eqrl59ukrY/WZ\ns9zkTgjzt549yj8/dRiwdhkBwo5mw1jNPHHkffuJdjzp41/bG2hx9AwGv1ACV/0IcDkd3LByDt0D\nIxRmu1k2J4clpR5augdp7RlkX70veP0wgDXzCqjv6B/3xXfkXDdL5+TgcAjvqpnDy0dbxg2SfW3L\nQf7u8f0RP/9AS6M0N4OLKvLYc6aThs5+brQHHWeShjnFiIxN6TTG8NM/nGLj4iJ+/dGN4/cSs/ua\ngUGwA40+VtiDMyLCR69ezEeuWjRutHXl3Fw6+oY50tTNiN9QkOXG6RBy0l34+ocxxnC8pYczHf0M\nj/pjqpn7hkbpnjBddMeJdtZWFox77cAOqG09Q+xrsEbdl86ZvCjm3Xa/dMPCIhwOodpeOPPEnkb6\nh0dZE7IBxMX2z4Ha2dc/TKNvIPi81y4roX94NHhKb9Rv2FfvizpdNjA6Pycvg9UV1vM7HcL1y0sj\n/k68aJhTUOBc86m2Ps51DXDjyrJJe5UFNjg82dpLz+AIJ1p7WRlS033gHZWTZqoFHn/FvrpHoJbP\nzUyjq3+Ylp5BugdGGPUb6jv6g4NHEcNsn54KbWq39gxS29wTPL8c4A1pZu9v7GJJaU6wuRzq8qpC\nrq8pDZ6/DayCe3indW3sNfPGmuYry/NwOSTYbw70rZfZYb7E7pIEpsDWtfTQOzRK79AofUOT56vD\nWJhLczK4yA7z+oWFwc9qJsV32Y5KCF5Puj3ybJ++WVg46Zjy/Ez79FQfb5zqwBir5o1meVkuTofw\ne3uGWGG2FdK8zDR8/cPjLz3b2ktX/zAikJMR/s8sNMyB0L1qP/flE8JcaIehtXuQAw0+rlsefjDJ\n5XTwg7vXjr3PvAw86S4One2iKNvNvMKx+feZbifLynKCNfNhexBw6Zzc4PtaUuphlx32PSGTQFq7\nh5hfNPl9NXUNkJHmIDfTRW6mi1suKudPLpm4y9bM0Jo5BRXnWIstttW14/W4WRRmC6HA6alfbDvF\nB36yA0+6K1iTRJKR5qS6xBM8PVVgTxgJhPl4y9iprhOtvdYii4y0iMtOg7PA7Jp5ZNTPd144xuIS\nDxfPGz+4lZHmJCfDxf5GH229Q+NaEdGISHC31Yvn5Y8b/Qarpt5zppNRv7U8MyfDFbzQIFgbWrxx\nqgO/34ybnhlpMcu5rkFKczMQsS7K8J0710TcqDHeNMwpqDgnnfbeIbYeb+PyqqJJf8ABV1YXU5Dt\n5hPXVvPUJ6+kyG7KRrNybl5wtU+gtgyGubmHbLcVupNtvXT2D0c8LQXWIFFOhoufbjtJ/9Aov3qj\nnuMtvXzuXUvDzowq9qTzWq3V2lhRHluYAZbYW0GtCbNh4rqqQnqHRvnrn+1i16kOlpbmjPu8Lplf\nQNfACMdbethT7wt2GSItZmnqGqA0d2ZWxE1Fw5yCvJ50Rv2Gc10DrA/TxA74yq0reOUL1/CZ65dE\n3MJ3opUhM5gKJoa5pYdFJR6qvNnBmjnaNj9ul4Nvv+9iDjR28dlH3+Tbzx3j4nn53LAi/GCR15NO\nz+AIDiG4F3ksxmrmgkmPvXtVGffetIxXa1s4bI9kh7rU7jdvq2vjUGMX19h7m2uY1QUR2D4ICE76\niJdVFVaN6LJHsQHyssZq5kXFHiqLrDB39kUPM8B1y0v5wg3L2LLvHGd9A/yfG5dFbEkUeawvj8Ul\nHjLdse/W+e7VZdy9YQGXVU0Os8Mh/PVVi3j201dx57r5kxY+VHmzKchK4xfbTzM06g9uB9XaPXlm\nmDGGpq4B5uRO3cKZCToAloICI79FIXOb42V5WS4OgfwsdzB0eZlpDI74afQNsKg4m6FRwxN7GzEG\nKgqm3vDhI1ctpLVnkP7h0eBc63AC72vleTSxwbqSyP23Rd/ZdF5hFv/0J6sm3S8iXLqggOcOWTPB\nLplfQH5WGi09Ya5M0j/CwLB/1mpmDXMKCtTM6xdG7i+/VVluF4uKPYQ+bW5I7bu4xEP/8Ch+Aw2d\n/VyzbOotlUWEv3tPzZTHBcK8IsbBr3i5xA5zYbabioJMa9FHmJo5sD68RMOs4qUsL4PCbDfvitD3\nnK6PXLWIvpBZUaFN6UXFHnpCJoHEc2vcwCywlTO48iicS+357Ksr8hARij3hd3MJThjRMKt4yUhz\nsvNLmyKeEpquPw25jC2MBTZw6dzQCRWRpnK+FZuWl1LX0sua+ZP7vjNpdUU+ORmu4PiDNyedfRN2\nEYGxU2yl2mdW8TRTQQ4nEOYF9sXm3S43+VlpMQ2AnY/S3IyYmuPxlul28tLnrg52J7wed9jtfgLX\n/tbRbJW0AoFdOO7Ss9Z00dw4hnk2FXnSSbOvBRY4RRbYoiigvqOfvMy0WbsuloZZTVsgzKEj51X2\nReGjTRpJVqHLMQHeON3BB368gwd3nJ5ySuxM0ma2mraCrDQ+c/0S3rN6bCeNQM2citeGCl2OWeRx\n8+c/2IYn3cXnb1jKXesXzFq5NMxq2kSEv71u/EU/Ny4u4te7s2I6z5xsAhtAtHYPsmfYz8Cwn++9\n/6Kw1xe7kDTMakasrSzkpc9fM9vFmBGBmrm1Z4hj9kqxcPO+LzQNs1LnqSh7rM+8t76ThcXZM36N\nsljoAJhS58ntcpCXmUZL9yBvnO4ct+HBbIopzCJyo4gcEZFaEflilOP+VESMiKyNdIxSqcDrcbPr\nVAftvUMM/MVUAAAHi0lEQVRcsmD2m9gQQ5hFxAl8F7gJqAHuFJFJZ+5FJAf4JLA93oVUKtF4PenB\n7YkvucAz0iKJpWZeB9QaY+qMMUPAQ8BtYY77KvAvQPSL7iqVAgKLWbLdzuCWR7MtljDPBc6E3K63\n7wsSkUuAecaYJ6M9kYjcIyI7RWRnS0vkvYeVSnSBFVwXzcuP6/WipmPaA2Ai4gC+CXx2qmONMQ8Y\nY9YaY9YWF0+9NE6pRBWomROliQ2xhbkBCN1+ocK+LyAHWAm8JCIngfXAZh0EU6nMa+96kiiDXxBb\nmF8HqkWkSkTcwB3A5sCDxhifMcZrjKk0xlQC24BbjTE7Z6TESiWAdy4p5r2XVrBhoXfqgy+QKcNs\njBkBPg48DRwCHjHGHBCR+0Xk1pkuoFKJqCwvk6+/96Lz2otspsU0A8wYswXYMuG++yIce/X0i6WU\nOl86A0ypFKFhVipFaJiVShEaZqVShIZZqRShYVYqRWiYlUoRGmalUoSGWakUoWFWKkVomJVKERpm\npVKEhlmpFKFhVipFaJiVShEaZqVShIZZqRShYVYqRWiYlUoRGmalUoSGWakUoWFWKkVomJVKERpm\npVKEhlmpFKFhVipFaJiVShEaZqVShIZZqRQRU5hF5EYROSIitSLyxTCPf0ZEDorIXhF5XkQWxL+o\nSqlopgyziDiB7wI3ATXAnSJSM+Gw3cBaY8xq4JfAv8a7oEqp6GKpmdcBtcaYOmPMEPAQcFvoAcaY\nF40xffbNbUBFfIuplJpKLGGeC5wJuV1v3xfJh4Cnwj0gIveIyE4R2dnS0hJ7KZVSU4rrAJiI3AWs\nBb4e7nFjzAPGmLXGmLXFxcXxfGml3vZcMRzTAMwLuV1h3zeOiGwCvgRcZYwZjE/xlFKxiqVmfh2o\nFpEqEXEDdwCbQw8QkTXAfwG3GmOa419MpdRUpgyzMWYE+DjwNHAIeMQYc0BE7heRW+3Dvg54gEdF\n5E0R2Rzh6ZRSMySWZjbGmC3Algn33Rfy86Y4l0spdZ50BphSKULDrFSK0DArlSI0zEqlCA2zUilC\nw6xUitAwK5UiNMxKpQgNs1IpQsOsVIrQMCuVIjTMSqUIDbNSKULDrFSK0DArlSI0zEqlCA2zUilC\nw6xUitAwK5UiNMxKpQgNs1IpQsOsVIrQMCuVIjTMSqUIDbNSKULDrFSK0DArlSI0zEqlCA2zUiki\npjCLyI0ickREakXki2EeTxeRh+3Ht4tIZbwLqpSKbsowi4gT+C5wE1AD3CkiNRMO+xDQYYxZDHwL\n+Jd4F1QpFV0sNfM6oNYYU2eMGQIeAm6bcMxtwP/YP/8SuE5EJH7FVEpNJZYwzwXOhNyut+8Le4wx\nZgTwAUXxKKBSKjauC/liInIPcI99s0dEjkzxK16gdWZLNWO07BdespYbopd9QSxPEEuYG4B5Ibcr\n7PvCHVMvIi4gD2ib+ETGmAeAB2IpGICI7DTGrI31+ESiZb/wkrXcEJ+yx9LMfh2oFpEqEXEDdwCb\nJxyzGfiA/fPtwAvGGDOdgimlzs+UNbMxZkREPg48DTiBHxtjDojI/cBOY8xm4EfAz0SkFmjHCrxS\n6gKKqc9sjNkCbJlw330hPw8A741v0YDzaJInIC37hZes5YY4lF20NaxUatDpnEqliIQN81RTSBOF\niMwTkRdF5KCIHBCRT9r3F4rIsyJyzP5vwWyXNRIRcYrIbhF5wr5dZU/LrbWn6bpnu4zhiEi+iPxS\nRA6LyCER2ZAMn7uIfNr+W9kvIg+KSEY8PvOEDHOMU0gTxQjwWWNMDbAe+Jhd1i8CzxtjqoHn7duJ\n6pPAoZDb/wJ8y56e24E1XTcR/T/gd8aYZcBFWO8hoT93EZkL/C2w1hizEmtQ+Q7i8ZkbYxLuH7AB\neDrk9r3AvbNdrhjL/hvgeuAIUGbfVwYcme2yRShvBdYf/bXAE4BgTV5whft/kSj/sOYynMAe9wm5\nP6E/d8ZmSxZiDUA/AdwQj888IWtmYptCmnDs1WJrgO1AqTHmrP3QOaB0loo1lW8DXwD89u0ioNNY\n03IhcT/7KqAF+IndRfihiGST4J+7MaYB+DfgNHAWa+rzLuLwmSdqmJOOiHiAXwGfMsZ0hT5mrK/b\nhDttICLvAZqNMbtmuyxvgQu4BPieMWYN0MuEJnUifu52H/42rC+jciAbuDEez52oYY5lCmnCEJE0\nrCD/whjzmH13k4iU2Y+XAc2zVb4oNgK3ishJrNVw12L1Q/PtabmQuJ99PVBvjNlu3/4lVrgT/XPf\nBJwwxrQYY4aBx7D+P0z7M0/UMMcyhTQh2Es9fwQcMsZ8M+Sh0CmuH8DqSycUY8y9xpgKY0wl1mf8\ngjHm/cCLWNNyIXHLfg44IyJL7buuAw6S+J/7aWC9iGTZfzuBck//M5/tAYEoAwU3A0eB48CXZrs8\nUcp5BVZTbi/wpv3vZqy+5/PAMeA5oHC2yzrF+7gaeML+eSGwA6gFHgXSZ7t8Ecp8MbDT/uwfBwqS\n4XMH/gE4DOwHfgakx+Mz1xlgSqWIRG1mK6XOk4ZZqRShYVYqRWiYlUoRGmalUoSGWakUoWFWKkVo\nmJVKEf8fdsWlBbJsyXEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f323c0bbf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# losshist_temp=losshist;\n",
    "\n",
    "ax = quickax();\n",
    "losses = losshist.losses;\n",
    "losshist = LossHist();\n",
    "losshist.losses = losses;\n",
    "losshist.vis(ax);\n",
    "ax.set_ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAEyCAYAAACGZHknAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoQrQzHunUgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f323c027090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# discount_rewards(epdlogp)\n",
    "x.size\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig=plt.figure(figsize=[5,5])\n",
    "ax1=plt.subplot()\n",
    "# ax1.plot(time_epr)\n",
    "# ax1.plot(discounted_epr)\n",
    "ax1.scatter(abs(time_epr),eptpred)\n",
    "# ax1.set_xlim([0, 200])\n",
    "# ax1.set_xlim([800, 1000])\n",
    "# ax1.set_ylim([-10, 000])\n",
    "# ax1.imshow(eph[:500,:500].T)\n",
    "time_epr.size\n",
    "# tpreds.size\n",
    "D1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.71828183,   7.3890561 ,  20.08553692])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-05 13:13:33,797] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss history has changed \n",
      "resetting env. episode 1 reward total was -20.000000. loss_func: 0.938122\n",
      "Loss history has changed \n",
      "resetting env. episode 2 reward total was -19.000000. loss_func: 1.064478\n",
      "Loss history has changed \n",
      "resetting env. episode 3 reward total was -21.000000. loss_func: 0.978837\n",
      "Loss history has changed \n",
      "resetting env. episode 4 reward total was -20.000000. loss_func: 1.007286\n",
      "Loss history has changed \n",
      "resetting env. episode 5 reward total was -20.000000. loss_func: 1.006256\n",
      "Loss history has changed \n",
      "resetting env. episode 6 reward total was -21.000000. loss_func: 1.022590\n",
      "Loss history has changed \n",
      "resetting env. episode 7 reward total was -19.000000. loss_func: 0.999554\n",
      "Loss history has changed \n",
      "resetting env. episode 8 reward total was -21.000000. loss_func: 1.020894\n",
      "Loss history has changed \n",
      "resetting env. episode 9 reward total was -21.000000. loss_func: 1.010773\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "Loss history has changed \n",
      "resetting env. episode 10 reward total was -21.000000. loss_func: 1.013818\n",
      "Loss history has changed \n",
      "resetting env. episode 11 reward total was -18.000000. loss_func: 1.003795\n",
      "Loss history has changed \n",
      "resetting env. episode 12 reward total was -21.000000. loss_func: 1.019284\n",
      "Loss history has changed \n",
      "resetting env. episode 13 reward total was -21.000000. loss_func: 1.003519\n",
      "Loss history has changed \n",
      "resetting env. episode 14 reward total was -20.000000. loss_func: 1.011527\n",
      "Loss history has changed \n",
      "resetting env. episode 15 reward total was -19.000000. loss_func: 1.003832\n",
      "Loss history has changed \n",
      "resetting env. episode 16 reward total was -21.000000. loss_func: 1.013838\n",
      "Loss history has changed \n",
      "resetting env. episode 17 reward total was -20.000000. loss_func: 1.003441\n",
      "Loss history has changed \n",
      "resetting env. episode 18 reward total was -21.000000. loss_func: 1.009849\n",
      "Loss history has changed \n",
      "resetting env. episode 19 reward total was -21.000000. loss_func: 1.001471\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "Loss history has changed \n",
      "resetting env. episode 20 reward total was -21.000000. loss_func: 1.019425\n",
      "Loss history has changed \n",
      "resetting env. episode 21 reward total was -20.000000. loss_func: 0.994098\n",
      "Loss history has changed \n",
      "resetting env. episode 22 reward total was -21.000000. loss_func: 1.021852\n",
      "Loss history has changed \n",
      "resetting env. episode 23 reward total was -21.000000. loss_func: 0.981980\n",
      "Loss history has changed \n",
      "resetting env. episode 24 reward total was -21.000000. loss_func: 1.031597\n",
      "Loss history has changed \n",
      "resetting env. episode 25 reward total was -21.000000. loss_func: 0.981998\n",
      "Loss history has changed \n",
      "resetting env. episode 26 reward total was -21.000000. loss_func: 1.022083\n",
      "Loss history has changed \n",
      "resetting env. episode 27 reward total was -21.000000. loss_func: 0.994904\n",
      "Loss history has changed \n",
      "resetting env. episode 28 reward total was -20.000000. loss_func: 1.012862\n",
      "Loss history has changed \n",
      "resetting env. episode 29 reward total was -19.000000. loss_func: 0.995039\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "Loss history has changed \n",
      "resetting env. episode 30 reward total was -21.000000. loss_func: 1.018845\n",
      "Loss history has changed \n",
      "resetting env. episode 31 reward total was -21.000000. loss_func: 0.994030\n",
      "Loss history has changed \n",
      "resetting env. episode 32 reward total was -20.000000. loss_func: 1.020886\n",
      "Loss history has changed \n",
      "resetting env. episode 33 reward total was -20.000000. loss_func: 0.990400\n",
      "Loss history has changed \n",
      "resetting env. episode 34 reward total was -21.000000. loss_func: 1.022367\n",
      "Loss history has changed \n",
      "resetting env. episode 35 reward total was -21.000000. loss_func: 0.991341\n",
      "Loss history has changed \n",
      "resetting env. episode 36 reward total was -20.000000. loss_func: 1.016863\n",
      "Loss history has changed \n",
      "resetting env. episode 37 reward total was -21.000000. loss_func: 0.992192\n",
      "Loss history has changed \n",
      "resetting env. episode 38 reward total was -21.000000. loss_func: 1.021206\n",
      "Loss history has changed \n",
      "resetting env. episode 39 reward total was -21.000000. loss_func: 0.983754\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "Loss history has changed \n",
      "resetting env. episode 40 reward total was -18.000000. loss_func: 1.022439\n",
      "Loss history has changed \n",
      "resetting env. episode 41 reward total was -21.000000. loss_func: 0.984734\n",
      "Loss history has changed \n",
      "resetting env. episode 42 reward total was -20.000000. loss_func: 1.017080\n",
      "Loss history has changed \n",
      "resetting env. episode 43 reward total was -20.000000. loss_func: 0.997896\n",
      "Loss history has changed \n",
      "resetting env. episode 44 reward total was -21.000000. loss_func: 1.012159\n",
      "Loss history has changed \n",
      "resetting env. episode 45 reward total was -21.000000. loss_func: 1.001879\n",
      "Loss history has changed \n",
      "resetting env. episode 46 reward total was -20.000000. loss_func: 1.011195\n",
      "Loss history has changed \n",
      "resetting env. episode 47 reward total was -21.000000. loss_func: 1.002700\n",
      "Loss history has changed \n",
      "resetting env. episode 48 reward total was -20.000000. loss_func: 1.007289\n",
      "Loss history has changed \n",
      "resetting env. episode 49 reward total was -21.000000. loss_func: 1.001675\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "Loss history has changed \n",
      "resetting env. episode 50 reward total was -20.000000. loss_func: 1.009482\n",
      "Loss history has changed \n",
      "resetting env. episode 51 reward total was -21.000000. loss_func: 1.001390\n",
      "Loss history has changed \n",
      "resetting env. episode 52 reward total was -21.000000. loss_func: 1.014327\n",
      "Loss history has changed \n",
      "resetting env. episode 53 reward total was -21.000000. loss_func: 0.991666\n",
      "Loss history has changed \n",
      "resetting env. episode 54 reward total was -21.000000. loss_func: 1.027379\n",
      "Loss history has changed \n",
      "resetting env. episode 55 reward total was -19.000000. loss_func: 0.970737\n",
      "Loss history has changed \n",
      "resetting env. episode 56 reward total was -20.000000. loss_func: 1.021406\n",
      "Loss history has changed \n",
      "resetting env. episode 57 reward total was -21.000000. loss_func: 0.986348\n",
      "Loss history has changed \n",
      "resetting env. episode 58 reward total was -20.000000. loss_func: 1.017637\n",
      "Loss history has changed \n",
      "resetting env. episode 59 reward total was -21.000000. loss_func: 0.996986\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "Loss history has changed \n",
      "resetting env. episode 60 reward total was -19.000000. loss_func: 1.009736\n",
      "Loss history has changed \n",
      "resetting env. episode 61 reward total was -21.000000. loss_func: 0.998855\n",
      "Loss history has changed \n",
      "resetting env. episode 62 reward total was -21.000000. loss_func: 1.011991\n",
      "Loss history has changed \n",
      "resetting env. episode 63 reward total was -21.000000. loss_func: 0.998251\n",
      "Loss history has changed \n",
      "resetting env. episode 64 reward total was -21.000000. loss_func: 1.014605\n",
      "Loss history has changed \n",
      "resetting env. episode 65 reward total was -21.000000. loss_func: 0.996266\n",
      "Loss history has changed \n",
      "resetting env. episode 66 reward total was -21.000000. loss_func: 1.016719\n",
      "Loss history has changed \n",
      "resetting env. episode 67 reward total was -20.000000. loss_func: 0.995053\n",
      "Loss history has changed \n",
      "resetting env. episode 68 reward total was -19.000000. loss_func: 1.019628\n",
      "Loss history has changed \n",
      "resetting env. episode 69 reward total was -19.000000. loss_func: 0.986372\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "Loss history has changed \n",
      "resetting env. episode 70 reward total was -21.000000. loss_func: 1.018117\n",
      "Loss history has changed \n",
      "resetting env. episode 71 reward total was -21.000000. loss_func: 0.991443\n",
      "Loss history has changed \n",
      "resetting env. episode 72 reward total was -20.000000. loss_func: 1.014757\n",
      "Loss history has changed \n",
      "resetting env. episode 73 reward total was -20.000000. loss_func: 0.992614\n",
      "Loss history has changed \n",
      "resetting env. episode 74 reward total was -21.000000. loss_func: 1.009396\n",
      "Loss history has changed \n",
      "resetting env. episode 75 reward total was -21.000000. loss_func: 0.991923\n",
      "resetting env. episode 76 reward total was -21.000000. loss_func: 1.018567\n",
      "resetting env. episode 77 reward total was -21.000000. loss_func: 0.991286\n",
      "resetting env. episode 78 reward total was -21.000000. loss_func: 1.016865\n",
      "resetting env. episode 79 reward total was -21.000000. loss_func: 0.989749\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 80 reward total was -21.000000. loss_func: 1.019292\n",
      "resetting env. episode 81 reward total was -19.000000. loss_func: 0.990308\n",
      "resetting env. episode 82 reward total was -21.000000. loss_func: 1.014888\n",
      "resetting env. episode 83 reward total was -21.000000. loss_func: 0.993266\n",
      "resetting env. episode 84 reward total was -20.000000. loss_func: 1.015870\n",
      "resetting env. episode 85 reward total was -21.000000. loss_func: 0.989764\n",
      "resetting env. episode 86 reward total was -21.000000. loss_func: 1.016182\n",
      "resetting env. episode 87 reward total was -21.000000. loss_func: 0.991627\n",
      "resetting env. episode 88 reward total was -20.000000. loss_func: 1.012973\n",
      "resetting env. episode 89 reward total was -21.000000. loss_func: 0.991421\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 90 reward total was -20.000000. loss_func: 1.015180\n",
      "resetting env. episode 91 reward total was -21.000000. loss_func: 0.994450\n",
      "resetting env. episode 92 reward total was -21.000000. loss_func: 1.017525\n",
      "resetting env. episode 93 reward total was -20.000000. loss_func: 0.992243\n",
      "resetting env. episode 94 reward total was -21.000000. loss_func: 1.016391\n",
      "resetting env. episode 95 reward total was -21.000000. loss_func: 0.991754\n",
      "resetting env. episode 96 reward total was -21.000000. loss_func: 1.015003\n",
      "resetting env. episode 97 reward total was -20.000000. loss_func: 0.995155\n",
      "resetting env. episode 98 reward total was -21.000000. loss_func: 1.012707\n",
      "resetting env. episode 99 reward total was -20.000000. loss_func: 0.994476\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 100 reward total was -21.000000. loss_func: 1.012065\n",
      "resetting env. episode 101 reward total was -21.000000. loss_func: 0.991621\n",
      "resetting env. episode 102 reward total was -20.000000. loss_func: 1.017395\n",
      "resetting env. episode 103 reward total was -20.000000. loss_func: 0.990974\n",
      "resetting env. episode 104 reward total was -19.000000. loss_func: 1.013680\n",
      "resetting env. episode 105 reward total was -20.000000. loss_func: 0.991952\n",
      "resetting env. episode 106 reward total was -21.000000. loss_func: 1.016472\n",
      "resetting env. episode 107 reward total was -21.000000. loss_func: 0.992037\n",
      "resetting env. episode 108 reward total was -21.000000. loss_func: 1.017639\n",
      "resetting env. episode 109 reward total was -21.000000. loss_func: 0.992342\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 110 reward total was -20.000000. loss_func: 1.016464\n",
      "resetting env. episode 111 reward total was -21.000000. loss_func: 0.991972\n",
      "resetting env. episode 112 reward total was -21.000000. loss_func: 1.013521\n",
      "resetting env. episode 113 reward total was -20.000000. loss_func: 0.993890\n",
      "resetting env. episode 114 reward total was -20.000000. loss_func: 1.013847\n",
      "resetting env. episode 115 reward total was -21.000000. loss_func: 0.991505\n",
      "resetting env. episode 116 reward total was -21.000000. loss_func: 1.016385\n",
      "resetting env. episode 117 reward total was -20.000000. loss_func: 0.993603\n",
      "resetting env. episode 118 reward total was -20.000000. loss_func: 1.014431\n",
      "resetting env. episode 119 reward total was -20.000000. loss_func: 0.993341\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 120 reward total was -21.000000. loss_func: 1.014844\n",
      "resetting env. episode 121 reward total was -21.000000. loss_func: 0.991372\n",
      "resetting env. episode 122 reward total was -21.000000. loss_func: 1.017663\n",
      "resetting env. episode 123 reward total was -21.000000. loss_func: 0.989881\n",
      "resetting env. episode 124 reward total was -21.000000. loss_func: 1.014199\n",
      "resetting env. episode 125 reward total was -21.000000. loss_func: 0.991820\n",
      "resetting env. episode 126 reward total was -20.000000. loss_func: 1.012716\n",
      "resetting env. episode 127 reward total was -20.000000. loss_func: 0.996896\n",
      "resetting env. episode 128 reward total was -20.000000. loss_func: 1.012883\n",
      "resetting env. episode 129 reward total was -19.000000. loss_func: 0.993765\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 130 reward total was -21.000000. loss_func: 1.017737\n",
      "resetting env. episode 131 reward total was -20.000000. loss_func: 0.991632\n",
      "resetting env. episode 132 reward total was -21.000000. loss_func: 1.016446\n",
      "resetting env. episode 133 reward total was -21.000000. loss_func: 0.988361\n",
      "resetting env. episode 134 reward total was -20.000000. loss_func: 1.016558\n",
      "resetting env. episode 135 reward total was -21.000000. loss_func: 0.991648\n",
      "resetting env. episode 136 reward total was -21.000000. loss_func: 1.012785\n",
      "resetting env. episode 137 reward total was -20.000000. loss_func: 0.995456\n",
      "resetting env. episode 138 reward total was -21.000000. loss_func: 1.013554\n",
      "resetting env. episode 139 reward total was -20.000000. loss_func: 0.994051\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 140 reward total was -21.000000. loss_func: 1.011183\n",
      "resetting env. episode 141 reward total was -20.000000. loss_func: 0.992625\n",
      "resetting env. episode 142 reward total was -20.000000. loss_func: 1.015513\n",
      "resetting env. episode 143 reward total was -21.000000. loss_func: 0.990906\n",
      "resetting env. episode 144 reward total was -20.000000. loss_func: 1.015419\n",
      "resetting env. episode 145 reward total was -21.000000. loss_func: 0.991051\n",
      "resetting env. episode 146 reward total was -20.000000. loss_func: 1.012569\n",
      "resetting env. episode 147 reward total was -20.000000. loss_func: 0.994471\n",
      "resetting env. episode 148 reward total was -21.000000. loss_func: 1.013554\n",
      "resetting env. episode 149 reward total was -20.000000. loss_func: 0.994753\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 150 reward total was -20.000000. loss_func: 1.014207\n",
      "resetting env. episode 151 reward total was -20.000000. loss_func: 0.993240\n",
      "resetting env. episode 152 reward total was -20.000000. loss_func: 1.016220\n",
      "resetting env. episode 153 reward total was -20.000000. loss_func: 0.992804\n",
      "resetting env. episode 154 reward total was -20.000000. loss_func: 1.012346\n",
      "resetting env. episode 155 reward total was -21.000000. loss_func: 0.994720\n",
      "resetting env. episode 156 reward total was -20.000000. loss_func: 1.012028\n",
      "resetting env. episode 157 reward total was -17.000000. loss_func: 0.993414\n",
      "resetting env. episode 158 reward total was -21.000000. loss_func: 1.017043\n",
      "resetting env. episode 159 reward total was -18.000000. loss_func: 0.988161\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 160 reward total was -21.000000. loss_func: 1.018881\n",
      "resetting env. episode 161 reward total was -20.000000. loss_func: 0.989838\n",
      "resetting env. episode 162 reward total was -21.000000. loss_func: 1.014199\n",
      "resetting env. episode 163 reward total was -20.000000. loss_func: 0.993504\n",
      "resetting env. episode 164 reward total was -20.000000. loss_func: 1.012205\n",
      "resetting env. episode 165 reward total was -21.000000. loss_func: 0.995214\n",
      "resetting env. episode 166 reward total was -21.000000. loss_func: 1.012321\n",
      "resetting env. episode 167 reward total was -21.000000. loss_func: 0.995635\n",
      "resetting env. episode 168 reward total was -21.000000. loss_func: 1.010295\n",
      "resetting env. episode 169 reward total was -21.000000. loss_func: 0.996736\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 170 reward total was -21.000000. loss_func: 1.012024\n",
      "resetting env. episode 171 reward total was -21.000000. loss_func: 0.993470\n",
      "resetting env. episode 172 reward total was -21.000000. loss_func: 1.016947\n",
      "resetting env. episode 173 reward total was -20.000000. loss_func: 0.989317\n",
      "resetting env. episode 174 reward total was -21.000000. loss_func: 1.021994\n",
      "resetting env. episode 175 reward total was -21.000000. loss_func: 0.987863\n",
      "resetting env. episode 176 reward total was -21.000000. loss_func: 1.018257\n",
      "resetting env. episode 177 reward total was -21.000000. loss_func: 0.991689\n",
      "resetting env. episode 178 reward total was -21.000000. loss_func: 1.012244\n",
      "resetting env. episode 179 reward total was -21.000000. loss_func: 0.994576\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 180 reward total was -21.000000. loss_func: 1.014360\n",
      "resetting env. episode 181 reward total was -21.000000. loss_func: 0.995481\n",
      "resetting env. episode 182 reward total was -21.000000. loss_func: 1.014171\n",
      "resetting env. episode 183 reward total was -21.000000. loss_func: 0.994671\n",
      "resetting env. episode 184 reward total was -21.000000. loss_func: 1.015024\n",
      "resetting env. episode 185 reward total was -20.000000. loss_func: 0.992430\n",
      "resetting env. episode 186 reward total was -21.000000. loss_func: 1.016498\n",
      "resetting env. episode 187 reward total was -21.000000. loss_func: 0.990509\n",
      "resetting env. episode 188 reward total was -21.000000. loss_func: 1.014475\n",
      "resetting env. episode 189 reward total was -21.000000. loss_func: 0.991234\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 190 reward total was -21.000000. loss_func: 1.014825\n",
      "resetting env. episode 191 reward total was -21.000000. loss_func: 0.993659\n",
      "resetting env. episode 192 reward total was -21.000000. loss_func: 1.012758\n",
      "resetting env. episode 193 reward total was -21.000000. loss_func: 0.994091\n",
      "resetting env. episode 194 reward total was -21.000000. loss_func: 1.012770\n",
      "resetting env. episode 195 reward total was -21.000000. loss_func: 0.993053\n",
      "resetting env. episode 196 reward total was -21.000000. loss_func: 1.013183\n",
      "resetting env. episode 197 reward total was -21.000000. loss_func: 0.992454\n",
      "resetting env. episode 198 reward total was -21.000000. loss_func: 1.014087\n",
      "resetting env. episode 199 reward total was -20.000000. loss_func: 0.992256\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 200 reward total was -19.000000. loss_func: 1.011803\n",
      "resetting env. episode 201 reward total was -21.000000. loss_func: 0.992418\n",
      "resetting env. episode 202 reward total was -21.000000. loss_func: 1.011327\n",
      "resetting env. episode 203 reward total was -21.000000. loss_func: 0.994854\n",
      "resetting env. episode 204 reward total was -21.000000. loss_func: 1.012170\n",
      "resetting env. episode 205 reward total was -21.000000. loss_func: 0.994273\n",
      "resetting env. episode 206 reward total was -21.000000. loss_func: 1.017624\n",
      "resetting env. episode 207 reward total was -20.000000. loss_func: 0.990896\n",
      "resetting env. episode 208 reward total was -20.000000. loss_func: 1.014821\n",
      "resetting env. episode 209 reward total was -21.000000. loss_func: 0.991479\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 210 reward total was -19.000000. loss_func: 1.010991\n",
      "resetting env. episode 211 reward total was -21.000000. loss_func: 0.991993\n",
      "resetting env. episode 212 reward total was -20.000000. loss_func: 1.012285\n",
      "resetting env. episode 213 reward total was -18.000000. loss_func: 0.993697\n",
      "resetting env. episode 214 reward total was -21.000000. loss_func: 1.016129\n",
      "resetting env. episode 215 reward total was -21.000000. loss_func: 0.992746\n",
      "resetting env. episode 216 reward total was -20.000000. loss_func: 1.013814\n",
      "resetting env. episode 217 reward total was -21.000000. loss_func: 0.991886\n",
      "resetting env. episode 218 reward total was -21.000000. loss_func: 1.012805\n",
      "resetting env. episode 219 reward total was -21.000000. loss_func: 0.992280\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 220 reward total was -20.000000. loss_func: 1.012591\n",
      "resetting env. episode 221 reward total was -21.000000. loss_func: 0.993257\n",
      "resetting env. episode 222 reward total was -20.000000. loss_func: 1.013879\n",
      "resetting env. episode 223 reward total was -21.000000. loss_func: 0.992857\n",
      "resetting env. episode 224 reward total was -21.000000. loss_func: 1.014262\n",
      "resetting env. episode 225 reward total was -21.000000. loss_func: 0.993487\n",
      "resetting env. episode 226 reward total was -20.000000. loss_func: 1.012053\n",
      "resetting env. episode 227 reward total was -21.000000. loss_func: 0.993438\n",
      "resetting env. episode 228 reward total was -19.000000. loss_func: 1.013324\n",
      "resetting env. episode 229 reward total was -21.000000. loss_func: 0.993662\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 230 reward total was -21.000000. loss_func: 1.015762\n",
      "resetting env. episode 231 reward total was -20.000000. loss_func: 0.992883\n",
      "resetting env. episode 232 reward total was -18.000000. loss_func: 1.010461\n",
      "resetting env. episode 233 reward total was -21.000000. loss_func: 0.993544\n",
      "resetting env. episode 234 reward total was -21.000000. loss_func: 1.017614\n",
      "resetting env. episode 235 reward total was -21.000000. loss_func: 0.992411\n",
      "resetting env. episode 236 reward total was -21.000000. loss_func: 1.014290\n",
      "resetting env. episode 237 reward total was -20.000000. loss_func: 0.992587\n",
      "resetting env. episode 238 reward total was -20.000000. loss_func: 1.015253\n",
      "resetting env. episode 239 reward total was -21.000000. loss_func: 0.993798\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 240 reward total was -21.000000. loss_func: 1.011275\n",
      "resetting env. episode 241 reward total was -20.000000. loss_func: 0.993169\n",
      "resetting env. episode 242 reward total was -21.000000. loss_func: 1.015623\n",
      "resetting env. episode 243 reward total was -21.000000. loss_func: 0.993859\n",
      "resetting env. episode 244 reward total was -20.000000. loss_func: 1.012511\n",
      "resetting env. episode 245 reward total was -18.000000. loss_func: 0.994236\n",
      "resetting env. episode 246 reward total was -21.000000. loss_func: 1.013711\n",
      "resetting env. episode 247 reward total was -21.000000. loss_func: 0.993478\n",
      "resetting env. episode 248 reward total was -20.000000. loss_func: 1.013284\n",
      "resetting env. episode 249 reward total was -21.000000. loss_func: 0.990859\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 250 reward total was -21.000000. loss_func: 1.016921\n",
      "resetting env. episode 251 reward total was -20.000000. loss_func: 0.991488\n",
      "resetting env. episode 252 reward total was -20.000000. loss_func: 1.012441\n",
      "resetting env. episode 253 reward total was -19.000000. loss_func: 0.992660\n",
      "resetting env. episode 254 reward total was -21.000000. loss_func: 1.015378\n",
      "resetting env. episode 255 reward total was -21.000000. loss_func: 0.994310\n",
      "resetting env. episode 256 reward total was -20.000000. loss_func: 1.010402\n",
      "resetting env. episode 257 reward total was -21.000000. loss_func: 0.995567\n",
      "resetting env. episode 258 reward total was -21.000000. loss_func: 1.015478\n",
      "resetting env. episode 259 reward total was -21.000000. loss_func: 0.994019\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 260 reward total was -21.000000. loss_func: 1.014507\n",
      "resetting env. episode 261 reward total was -20.000000. loss_func: 0.991821\n",
      "resetting env. episode 262 reward total was -21.000000. loss_func: 1.014981\n",
      "resetting env. episode 263 reward total was -21.000000. loss_func: 0.991227\n",
      "resetting env. episode 264 reward total was -21.000000. loss_func: 1.012297\n",
      "resetting env. episode 265 reward total was -20.000000. loss_func: 0.992141\n",
      "resetting env. episode 266 reward total was -21.000000. loss_func: 1.017516\n",
      "resetting env. episode 267 reward total was -20.000000. loss_func: 0.993683\n",
      "resetting env. episode 268 reward total was -16.000000. loss_func: 1.009788\n",
      "resetting env. episode 269 reward total was -20.000000. loss_func: 0.993688\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 270 reward total was -21.000000. loss_func: 1.013216\n",
      "resetting env. episode 271 reward total was -21.000000. loss_func: 0.994365\n",
      "resetting env. episode 272 reward total was -21.000000. loss_func: 1.015504\n",
      "resetting env. episode 273 reward total was -20.000000. loss_func: 0.994317\n",
      "resetting env. episode 274 reward total was -21.000000. loss_func: 1.013799\n",
      "resetting env. episode 275 reward total was -21.000000. loss_func: 0.992737\n",
      "resetting env. episode 276 reward total was -21.000000. loss_func: 1.013945\n",
      "resetting env. episode 277 reward total was -21.000000. loss_func: 0.992953\n",
      "resetting env. episode 278 reward total was -21.000000. loss_func: 1.015138\n",
      "resetting env. episode 279 reward total was -21.000000. loss_func: 0.993155\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 280 reward total was -21.000000. loss_func: 1.013922\n",
      "resetting env. episode 281 reward total was -21.000000. loss_func: 0.992152\n",
      "resetting env. episode 282 reward total was -21.000000. loss_func: 1.015920\n",
      "resetting env. episode 283 reward total was -21.000000. loss_func: 0.992598\n",
      "resetting env. episode 284 reward total was -21.000000. loss_func: 1.012346\n",
      "resetting env. episode 285 reward total was -18.000000. loss_func: 0.993233\n",
      "resetting env. episode 286 reward total was -21.000000. loss_func: 1.011046\n",
      "resetting env. episode 287 reward total was -21.000000. loss_func: 0.993896\n",
      "resetting env. episode 288 reward total was -18.000000. loss_func: 1.012434\n",
      "resetting env. episode 289 reward total was -21.000000. loss_func: 0.993763\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 290 reward total was -20.000000. loss_func: 1.011584\n",
      "resetting env. episode 291 reward total was -21.000000. loss_func: 0.993711\n",
      "resetting env. episode 292 reward total was -21.000000. loss_func: 1.014900\n",
      "resetting env. episode 293 reward total was -21.000000. loss_func: 0.992698\n",
      "resetting env. episode 294 reward total was -21.000000. loss_func: 1.012312\n",
      "resetting env. episode 295 reward total was -21.000000. loss_func: 0.992458\n",
      "resetting env. episode 296 reward total was -21.000000. loss_func: 1.014691\n",
      "resetting env. episode 297 reward total was -19.000000. loss_func: 0.993999\n",
      "resetting env. episode 298 reward total was -21.000000. loss_func: 1.015970\n",
      "resetting env. episode 299 reward total was -19.000000. loss_func: 0.993499\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 300 reward total was -21.000000. loss_func: 1.013475\n",
      "resetting env. episode 301 reward total was -21.000000. loss_func: 0.994722\n",
      "resetting env. episode 302 reward total was -21.000000. loss_func: 1.014179\n",
      "resetting env. episode 303 reward total was -19.000000. loss_func: 0.996059\n",
      "resetting env. episode 304 reward total was -21.000000. loss_func: 1.010652\n",
      "resetting env. episode 305 reward total was -20.000000. loss_func: 0.994950\n",
      "resetting env. episode 306 reward total was -20.000000. loss_func: 1.015837\n",
      "resetting env. episode 307 reward total was -21.000000. loss_func: 0.990702\n",
      "resetting env. episode 308 reward total was -21.000000. loss_func: 1.014268\n",
      "resetting env. episode 309 reward total was -21.000000. loss_func: 0.991169\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 310 reward total was -21.000000. loss_func: 1.014748\n",
      "resetting env. episode 311 reward total was -19.000000. loss_func: 0.994681\n",
      "resetting env. episode 312 reward total was -20.000000. loss_func: 1.014079\n",
      "resetting env. episode 313 reward total was -21.000000. loss_func: 0.996104\n",
      "resetting env. episode 314 reward total was -20.000000. loss_func: 1.013668\n",
      "resetting env. episode 315 reward total was -19.000000. loss_func: 0.995726\n",
      "resetting env. episode 316 reward total was -18.000000. loss_func: 1.013215\n",
      "resetting env. episode 317 reward total was -19.000000. loss_func: 0.993884\n",
      "resetting env. episode 318 reward total was -20.000000. loss_func: 1.014589\n",
      "resetting env. episode 319 reward total was -21.000000. loss_func: 0.994168\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 320 reward total was -21.000000. loss_func: 1.013309\n",
      "resetting env. episode 321 reward total was -21.000000. loss_func: 0.992156\n",
      "resetting env. episode 322 reward total was -21.000000. loss_func: 1.014841\n",
      "resetting env. episode 323 reward total was -21.000000. loss_func: 0.991068\n",
      "resetting env. episode 324 reward total was -21.000000. loss_func: 1.013940\n",
      "resetting env. episode 325 reward total was -21.000000. loss_func: 0.993337\n",
      "resetting env. episode 326 reward total was -20.000000. loss_func: 1.009440\n",
      "resetting env. episode 327 reward total was -21.000000. loss_func: 0.996537\n",
      "resetting env. episode 328 reward total was -20.000000. loss_func: 1.011585\n",
      "resetting env. episode 329 reward total was -21.000000. loss_func: 0.996043\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 330 reward total was -21.000000. loss_func: 1.015929\n",
      "resetting env. episode 331 reward total was -21.000000. loss_func: 0.995287\n",
      "resetting env. episode 332 reward total was -21.000000. loss_func: 1.015132\n",
      "resetting env. episode 333 reward total was -20.000000. loss_func: 0.993226\n",
      "resetting env. episode 334 reward total was -21.000000. loss_func: 1.016903\n",
      "resetting env. episode 335 reward total was -20.000000. loss_func: 0.995232\n",
      "resetting env. episode 336 reward total was -21.000000. loss_func: 1.014440\n",
      "resetting env. episode 337 reward total was -21.000000. loss_func: 0.988848\n",
      "resetting env. episode 338 reward total was -20.000000. loss_func: 1.016300\n",
      "resetting env. episode 339 reward total was -21.000000. loss_func: 0.988606\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 340 reward total was -21.000000. loss_func: 1.014539\n",
      "resetting env. episode 341 reward total was -21.000000. loss_func: 0.994261\n",
      "resetting env. episode 342 reward total was -21.000000. loss_func: 1.012794\n",
      "resetting env. episode 343 reward total was -21.000000. loss_func: 0.996689\n",
      "resetting env. episode 344 reward total was -21.000000. loss_func: 1.012133\n",
      "resetting env. episode 345 reward total was -21.000000. loss_func: 0.997751\n",
      "resetting env. episode 346 reward total was -21.000000. loss_func: 1.012380\n",
      "resetting env. episode 347 reward total was -21.000000. loss_func: 0.996347\n",
      "resetting env. episode 348 reward total was -20.000000. loss_func: 1.014606\n",
      "resetting env. episode 349 reward total was -21.000000. loss_func: 0.993615\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 350 reward total was -20.000000. loss_func: 1.015898\n",
      "resetting env. episode 351 reward total was -21.000000. loss_func: 0.990259\n",
      "resetting env. episode 352 reward total was -20.000000. loss_func: 1.019461\n",
      "resetting env. episode 353 reward total was -21.000000. loss_func: 0.987872\n",
      "resetting env. episode 354 reward total was -20.000000. loss_func: 1.012933\n",
      "resetting env. episode 355 reward total was -21.000000. loss_func: 0.992553\n",
      "resetting env. episode 356 reward total was -20.000000. loss_func: 1.011820\n",
      "resetting env. episode 357 reward total was -21.000000. loss_func: 0.995823\n",
      "resetting env. episode 358 reward total was -19.000000. loss_func: 1.011194\n",
      "resetting env. episode 359 reward total was -21.000000. loss_func: 0.996256\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 360 reward total was -21.000000. loss_func: 1.013103\n",
      "resetting env. episode 361 reward total was -19.000000. loss_func: 0.996113\n",
      "resetting env. episode 362 reward total was -21.000000. loss_func: 1.012275\n",
      "resetting env. episode 363 reward total was -20.000000. loss_func: 0.994731\n",
      "resetting env. episode 364 reward total was -21.000000. loss_func: 1.015087\n",
      "resetting env. episode 365 reward total was -20.000000. loss_func: 0.993470\n",
      "resetting env. episode 366 reward total was -20.000000. loss_func: 1.014349\n",
      "resetting env. episode 367 reward total was -21.000000. loss_func: 0.989689\n",
      "resetting env. episode 368 reward total was -20.000000. loss_func: 1.014565\n",
      "resetting env. episode 369 reward total was -21.000000. loss_func: 0.992540\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 370 reward total was -20.000000. loss_func: 1.013889\n",
      "resetting env. episode 371 reward total was -20.000000. loss_func: 0.994034\n",
      "resetting env. episode 372 reward total was -20.000000. loss_func: 1.014327\n",
      "resetting env. episode 373 reward total was -21.000000. loss_func: 0.994754\n",
      "resetting env. episode 374 reward total was -20.000000. loss_func: 1.014950\n",
      "resetting env. episode 375 reward total was -20.000000. loss_func: 0.994542\n",
      "resetting env. episode 376 reward total was -21.000000. loss_func: 1.014333\n",
      "resetting env. episode 377 reward total was -21.000000. loss_func: 0.993234\n",
      "resetting env. episode 378 reward total was -21.000000. loss_func: 1.014149\n",
      "resetting env. episode 379 reward total was -18.000000. loss_func: 0.993549\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 380 reward total was -20.000000. loss_func: 1.013251\n",
      "resetting env. episode 381 reward total was -21.000000. loss_func: 0.992747\n",
      "resetting env. episode 382 reward total was -21.000000. loss_func: 1.015094\n",
      "resetting env. episode 383 reward total was -20.000000. loss_func: 0.993109\n",
      "resetting env. episode 384 reward total was -19.000000. loss_func: 1.014086\n",
      "resetting env. episode 385 reward total was -20.000000. loss_func: 0.994287\n",
      "resetting env. episode 386 reward total was -21.000000. loss_func: 1.013751\n",
      "resetting env. episode 387 reward total was -20.000000. loss_func: 0.994345\n",
      "resetting env. episode 388 reward total was -21.000000. loss_func: 1.015274\n",
      "resetting env. episode 389 reward total was -21.000000. loss_func: 0.995135\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 390 reward total was -21.000000. loss_func: 1.016506\n",
      "resetting env. episode 391 reward total was -21.000000. loss_func: 0.994597\n",
      "resetting env. episode 392 reward total was -21.000000. loss_func: 1.014899\n",
      "resetting env. episode 393 reward total was -19.000000. loss_func: 0.993145\n",
      "resetting env. episode 394 reward total was -19.000000. loss_func: 1.013574\n",
      "resetting env. episode 395 reward total was -21.000000. loss_func: 0.991577\n",
      "resetting env. episode 396 reward total was -21.000000. loss_func: 1.017538\n",
      "resetting env. episode 397 reward total was -21.000000. loss_func: 0.993194\n",
      "resetting env. episode 398 reward total was -21.000000. loss_func: 1.014944\n",
      "resetting env. episode 399 reward total was -21.000000. loss_func: 0.995299\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 400 reward total was -17.000000. loss_func: 1.011251\n",
      "resetting env. episode 401 reward total was -21.000000. loss_func: 0.994711\n",
      "resetting env. episode 402 reward total was -20.000000. loss_func: 1.014853\n",
      "resetting env. episode 403 reward total was -21.000000. loss_func: 0.994555\n",
      "resetting env. episode 404 reward total was -21.000000. loss_func: 1.017973\n",
      "resetting env. episode 405 reward total was -20.000000. loss_func: 0.994325\n",
      "resetting env. episode 406 reward total was -21.000000. loss_func: 1.012598\n",
      "resetting env. episode 407 reward total was -21.000000. loss_func: 0.993854\n",
      "resetting env. episode 408 reward total was -21.000000. loss_func: 1.014667\n",
      "resetting env. episode 409 reward total was -21.000000. loss_func: 0.993601\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 410 reward total was -21.000000. loss_func: 1.016505\n",
      "resetting env. episode 411 reward total was -21.000000. loss_func: 0.993032\n",
      "resetting env. episode 412 reward total was -21.000000. loss_func: 1.017645\n",
      "resetting env. episode 413 reward total was -18.000000. loss_func: 0.994146\n",
      "resetting env. episode 414 reward total was -21.000000. loss_func: 1.012898\n",
      "resetting env. episode 415 reward total was -21.000000. loss_func: 0.994219\n",
      "resetting env. episode 416 reward total was -21.000000. loss_func: 1.015555\n",
      "resetting env. episode 417 reward total was -20.000000. loss_func: 0.994098\n",
      "resetting env. episode 418 reward total was -20.000000. loss_func: 1.013505\n",
      "resetting env. episode 419 reward total was -21.000000. loss_func: 0.994021\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 420 reward total was -21.000000. loss_func: 1.012585\n",
      "resetting env. episode 421 reward total was -21.000000. loss_func: 0.995044\n",
      "resetting env. episode 422 reward total was -20.000000. loss_func: 1.014956\n",
      "resetting env. episode 423 reward total was -21.000000. loss_func: 0.993303\n",
      "resetting env. episode 424 reward total was -19.000000. loss_func: 1.010811\n",
      "resetting env. episode 425 reward total was -19.000000. loss_func: 0.993349\n",
      "resetting env. episode 426 reward total was -21.000000. loss_func: 1.012498\n",
      "resetting env. episode 427 reward total was -21.000000. loss_func: 0.994088\n",
      "resetting env. episode 428 reward total was -21.000000. loss_func: 1.016694\n",
      "resetting env. episode 429 reward total was -21.000000. loss_func: 0.993508\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 430 reward total was -21.000000. loss_func: 1.014620\n",
      "resetting env. episode 431 reward total was -21.000000. loss_func: 0.993073\n",
      "resetting env. episode 432 reward total was -21.000000. loss_func: 1.014531\n",
      "resetting env. episode 433 reward total was -20.000000. loss_func: 0.993928\n",
      "resetting env. episode 434 reward total was -21.000000. loss_func: 1.013674\n",
      "resetting env. episode 435 reward total was -20.000000. loss_func: 0.994417\n",
      "resetting env. episode 436 reward total was -21.000000. loss_func: 1.017949\n",
      "resetting env. episode 437 reward total was -21.000000. loss_func: 0.994307\n",
      "resetting env. episode 438 reward total was -20.000000. loss_func: 1.012921\n",
      "resetting env. episode 439 reward total was -19.000000. loss_func: 0.993448\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 440 reward total was -21.000000. loss_func: 1.014088\n",
      "resetting env. episode 441 reward total was -21.000000. loss_func: 0.992276\n",
      "resetting env. episode 442 reward total was -21.000000. loss_func: 1.014242\n",
      "resetting env. episode 443 reward total was -20.000000. loss_func: 0.993816\n",
      "resetting env. episode 444 reward total was -21.000000. loss_func: 1.013014\n",
      "resetting env. episode 445 reward total was -21.000000. loss_func: 0.995241\n",
      "resetting env. episode 446 reward total was -19.000000. loss_func: 1.014960\n",
      "resetting env. episode 447 reward total was -21.000000. loss_func: 0.994134\n",
      "resetting env. episode 448 reward total was -19.000000. loss_func: 1.012354\n",
      "resetting env. episode 449 reward total was -21.000000. loss_func: 0.993748\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 450 reward total was -21.000000. loss_func: 1.014765\n",
      "resetting env. episode 451 reward total was -20.000000. loss_func: 0.994508\n",
      "resetting env. episode 452 reward total was -21.000000. loss_func: 1.018976\n",
      "resetting env. episode 453 reward total was -21.000000. loss_func: 0.993604\n",
      "resetting env. episode 454 reward total was -21.000000. loss_func: 1.015876\n",
      "resetting env. episode 455 reward total was -19.000000. loss_func: 0.993648\n",
      "resetting env. episode 456 reward total was -21.000000. loss_func: 1.015006\n",
      "resetting env. episode 457 reward total was -21.000000. loss_func: 0.994534\n",
      "resetting env. episode 458 reward total was -21.000000. loss_func: 1.013628\n",
      "resetting env. episode 459 reward total was -21.000000. loss_func: 0.993739\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 460 reward total was -19.000000. loss_func: 1.012547\n",
      "resetting env. episode 461 reward total was -21.000000. loss_func: 0.993747\n",
      "resetting env. episode 462 reward total was -21.000000. loss_func: 1.013277\n",
      "resetting env. episode 463 reward total was -21.000000. loss_func: 0.994177\n",
      "resetting env. episode 464 reward total was -21.000000. loss_func: 1.013036\n",
      "resetting env. episode 465 reward total was -20.000000. loss_func: 0.994120\n",
      "resetting env. episode 466 reward total was -21.000000. loss_func: 1.014785\n",
      "resetting env. episode 467 reward total was -21.000000. loss_func: 0.995021\n",
      "resetting env. episode 468 reward total was -21.000000. loss_func: 1.014389\n",
      "resetting env. episode 469 reward total was -21.000000. loss_func: 0.995167\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 470 reward total was -21.000000. loss_func: 1.015395\n",
      "resetting env. episode 471 reward total was -16.000000. loss_func: 0.994467\n",
      "resetting env. episode 472 reward total was -21.000000. loss_func: 1.015062\n",
      "resetting env. episode 473 reward total was -21.000000. loss_func: 0.994577\n",
      "resetting env. episode 474 reward total was -21.000000. loss_func: 1.018358\n",
      "resetting env. episode 475 reward total was -21.000000. loss_func: 0.994580\n",
      "resetting env. episode 476 reward total was -21.000000. loss_func: 1.015057\n",
      "resetting env. episode 477 reward total was -20.000000. loss_func: 0.993383\n",
      "resetting env. episode 478 reward total was -20.000000. loss_func: 1.012555\n",
      "resetting env. episode 479 reward total was -20.000000. loss_func: 0.994337\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 480 reward total was -21.000000. loss_func: 1.013315\n",
      "resetting env. episode 481 reward total was -21.000000. loss_func: 0.995383\n",
      "resetting env. episode 482 reward total was -21.000000. loss_func: 1.013077\n",
      "resetting env. episode 483 reward total was -21.000000. loss_func: 0.994376\n",
      "resetting env. episode 484 reward total was -21.000000. loss_func: 1.014880\n",
      "resetting env. episode 485 reward total was -20.000000. loss_func: 0.994807\n",
      "resetting env. episode 486 reward total was -21.000000. loss_func: 1.014411\n",
      "resetting env. episode 487 reward total was -20.000000. loss_func: 0.995612\n",
      "resetting env. episode 488 reward total was -21.000000. loss_func: 1.014080\n",
      "resetting env. episode 489 reward total was -21.000000. loss_func: 0.994551\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 490 reward total was -19.000000. loss_func: 1.013355\n",
      "resetting env. episode 491 reward total was -21.000000. loss_func: 0.993875\n",
      "resetting env. episode 492 reward total was -21.000000. loss_func: 1.016889\n",
      "resetting env. episode 493 reward total was -17.000000. loss_func: 0.995439\n",
      "resetting env. episode 494 reward total was -21.000000. loss_func: 1.014617\n",
      "resetting env. episode 495 reward total was -21.000000. loss_func: 0.993240\n",
      "resetting env. episode 496 reward total was -20.000000. loss_func: 1.013954\n",
      "resetting env. episode 497 reward total was -21.000000. loss_func: 0.992100\n",
      "resetting env. episode 498 reward total was -21.000000. loss_func: 1.018164\n",
      "resetting env. episode 499 reward total was -20.000000. loss_func: 0.994507\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 500 reward total was -20.000000. loss_func: 1.015426\n",
      "resetting env. episode 501 reward total was -20.000000. loss_func: 0.994476\n",
      "resetting env. episode 502 reward total was -21.000000. loss_func: 1.016040\n",
      "resetting env. episode 503 reward total was -21.000000. loss_func: 0.993179\n",
      "resetting env. episode 504 reward total was -18.000000. loss_func: 1.011691\n",
      "resetting env. episode 505 reward total was -21.000000. loss_func: 0.993627\n",
      "resetting env. episode 506 reward total was -21.000000. loss_func: 1.014394\n",
      "resetting env. episode 507 reward total was -21.000000. loss_func: 0.994752\n",
      "resetting env. episode 508 reward total was -21.000000. loss_func: 1.015549\n",
      "resetting env. episode 509 reward total was -21.000000. loss_func: 0.992850\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 510 reward total was -21.000000. loss_func: 1.014784\n",
      "resetting env. episode 511 reward total was -21.000000. loss_func: 0.994052\n",
      "resetting env. episode 512 reward total was -21.000000. loss_func: 1.015989\n",
      "resetting env. episode 513 reward total was -20.000000. loss_func: 0.993109\n",
      "resetting env. episode 514 reward total was -20.000000. loss_func: 1.013609\n",
      "resetting env. episode 515 reward total was -21.000000. loss_func: 0.993556\n",
      "resetting env. episode 516 reward total was -20.000000. loss_func: 1.012947\n",
      "resetting env. episode 517 reward total was -18.000000. loss_func: 0.994130\n",
      "resetting env. episode 518 reward total was -20.000000. loss_func: 1.015330\n",
      "resetting env. episode 519 reward total was -21.000000. loss_func: 0.994080\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 520 reward total was -21.000000. loss_func: 1.013100\n",
      "resetting env. episode 521 reward total was -19.000000. loss_func: 0.993987\n",
      "resetting env. episode 522 reward total was -21.000000. loss_func: 1.015435\n",
      "resetting env. episode 523 reward total was -21.000000. loss_func: 0.994323\n",
      "resetting env. episode 524 reward total was -20.000000. loss_func: 1.014584\n",
      "resetting env. episode 525 reward total was -21.000000. loss_func: 0.994319\n",
      "resetting env. episode 526 reward total was -21.000000. loss_func: 1.016655\n",
      "resetting env. episode 527 reward total was -21.000000. loss_func: 0.994975\n",
      "resetting env. episode 528 reward total was -21.000000. loss_func: 1.017048\n",
      "resetting env. episode 529 reward total was -21.000000. loss_func: 0.994228\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 530 reward total was -21.000000. loss_func: 1.012476\n",
      "resetting env. episode 531 reward total was -21.000000. loss_func: 0.994455\n",
      "resetting env. episode 532 reward total was -19.000000. loss_func: 1.014514\n",
      "resetting env. episode 533 reward total was -21.000000. loss_func: 0.994685\n",
      "resetting env. episode 534 reward total was -20.000000. loss_func: 1.013925\n",
      "resetting env. episode 535 reward total was -20.000000. loss_func: 0.994217\n",
      "resetting env. episode 536 reward total was -21.000000. loss_func: 1.018307\n",
      "resetting env. episode 537 reward total was -21.000000. loss_func: 0.993445\n",
      "resetting env. episode 538 reward total was -18.000000. loss_func: 1.014059\n",
      "resetting env. episode 539 reward total was -20.000000. loss_func: 0.994368\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 540 reward total was -21.000000. loss_func: 1.014989\n",
      "resetting env. episode 541 reward total was -21.000000. loss_func: 0.994057\n",
      "resetting env. episode 542 reward total was -21.000000. loss_func: 1.016253\n",
      "resetting env. episode 543 reward total was -21.000000. loss_func: 0.995521\n",
      "resetting env. episode 544 reward total was -21.000000. loss_func: 1.014988\n",
      "resetting env. episode 545 reward total was -20.000000. loss_func: 0.995026\n",
      "resetting env. episode 546 reward total was -20.000000. loss_func: 1.015088\n",
      "resetting env. episode 547 reward total was -20.000000. loss_func: 0.994985\n",
      "resetting env. episode 548 reward total was -21.000000. loss_func: 1.015430\n",
      "resetting env. episode 549 reward total was -21.000000. loss_func: 0.993492\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 550 reward total was -20.000000. loss_func: 1.016514\n",
      "resetting env. episode 551 reward total was -20.000000. loss_func: 0.993563\n",
      "resetting env. episode 552 reward total was -21.000000. loss_func: 1.014485\n",
      "resetting env. episode 553 reward total was -20.000000. loss_func: 0.995062\n",
      "resetting env. episode 554 reward total was -21.000000. loss_func: 1.016645\n",
      "resetting env. episode 555 reward total was -21.000000. loss_func: 0.994726\n",
      "resetting env. episode 556 reward total was -21.000000. loss_func: 1.014376\n",
      "resetting env. episode 557 reward total was -20.000000. loss_func: 0.993837\n",
      "resetting env. episode 558 reward total was -20.000000. loss_func: 1.015798\n",
      "resetting env. episode 559 reward total was -21.000000. loss_func: 0.994797\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 560 reward total was -21.000000. loss_func: 1.016813\n",
      "resetting env. episode 561 reward total was -20.000000. loss_func: 0.995336\n",
      "resetting env. episode 562 reward total was -21.000000. loss_func: 1.017177\n",
      "resetting env. episode 563 reward total was -21.000000. loss_func: 0.994026\n",
      "resetting env. episode 564 reward total was -20.000000. loss_func: 1.014262\n",
      "resetting env. episode 565 reward total was -21.000000. loss_func: 0.995015\n",
      "resetting env. episode 566 reward total was -21.000000. loss_func: 1.012991\n",
      "resetting env. episode 567 reward total was -20.000000. loss_func: 0.995609\n",
      "resetting env. episode 568 reward total was -21.000000. loss_func: 1.015377\n",
      "resetting env. episode 569 reward total was -21.000000. loss_func: 0.995422\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 570 reward total was -21.000000. loss_func: 1.017959\n",
      "resetting env. episode 571 reward total was -21.000000. loss_func: 0.990870\n",
      "resetting env. episode 572 reward total was -21.000000. loss_func: 1.020366\n",
      "resetting env. episode 573 reward total was -20.000000. loss_func: 0.993110\n",
      "resetting env. episode 574 reward total was -21.000000. loss_func: 1.016562\n",
      "resetting env. episode 575 reward total was -20.000000. loss_func: 0.997014\n",
      "resetting env. episode 576 reward total was -21.000000. loss_func: 1.011820\n",
      "resetting env. episode 577 reward total was -21.000000. loss_func: 0.998352\n",
      "resetting env. episode 578 reward total was -21.000000. loss_func: 1.011575\n",
      "resetting env. episode 579 reward total was -21.000000. loss_func: 0.995181\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 580 reward total was -20.000000. loss_func: 1.014366\n",
      "resetting env. episode 581 reward total was -20.000000. loss_func: 0.991974\n",
      "resetting env. episode 582 reward total was -21.000000. loss_func: 1.019553\n",
      "resetting env. episode 583 reward total was -21.000000. loss_func: 0.989622\n",
      "resetting env. episode 584 reward total was -21.000000. loss_func: 1.017390\n",
      "resetting env. episode 585 reward total was -21.000000. loss_func: 0.994182\n",
      "resetting env. episode 586 reward total was -21.000000. loss_func: 1.013626\n",
      "resetting env. episode 587 reward total was -21.000000. loss_func: 0.996478\n",
      "resetting env. episode 588 reward total was -21.000000. loss_func: 1.013548\n",
      "resetting env. episode 589 reward total was -21.000000. loss_func: 0.996403\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 590 reward total was -21.000000. loss_func: 1.015219\n",
      "resetting env. episode 591 reward total was -21.000000. loss_func: 0.997022\n",
      "resetting env. episode 592 reward total was -21.000000. loss_func: 1.014841\n",
      "resetting env. episode 593 reward total was -21.000000. loss_func: 0.994167\n",
      "resetting env. episode 594 reward total was -21.000000. loss_func: 1.014463\n",
      "resetting env. episode 595 reward total was -20.000000. loss_func: 0.992514\n",
      "resetting env. episode 596 reward total was -19.000000. loss_func: 1.016348\n",
      "resetting env. episode 597 reward total was -21.000000. loss_func: 0.991327\n",
      "resetting env. episode 598 reward total was -20.000000. loss_func: 1.014794\n",
      "resetting env. episode 599 reward total was -21.000000. loss_func: 0.994401\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 600 reward total was -20.000000. loss_func: 1.012150\n",
      "resetting env. episode 601 reward total was -21.000000. loss_func: 0.995234\n",
      "resetting env. episode 602 reward total was -21.000000. loss_func: 1.014224\n",
      "resetting env. episode 603 reward total was -21.000000. loss_func: 0.995604\n",
      "resetting env. episode 604 reward total was -20.000000. loss_func: 1.011872\n",
      "resetting env. episode 605 reward total was -20.000000. loss_func: 0.994943\n",
      "resetting env. episode 606 reward total was -21.000000. loss_func: 1.017736\n",
      "resetting env. episode 607 reward total was -21.000000. loss_func: 0.995285\n",
      "resetting env. episode 608 reward total was -21.000000. loss_func: 1.015753\n",
      "resetting env. episode 609 reward total was -21.000000. loss_func: 0.994080\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 610 reward total was -21.000000. loss_func: 1.017403\n",
      "resetting env. episode 611 reward total was -21.000000. loss_func: 0.994100\n",
      "resetting env. episode 612 reward total was -21.000000. loss_func: 1.015173\n",
      "resetting env. episode 613 reward total was -20.000000. loss_func: 0.994810\n",
      "resetting env. episode 614 reward total was -21.000000. loss_func: 1.016268\n",
      "resetting env. episode 615 reward total was -21.000000. loss_func: 0.996309\n",
      "resetting env. episode 616 reward total was -18.000000. loss_func: 1.013631\n",
      "resetting env. episode 617 reward total was -21.000000. loss_func: 0.995671\n",
      "resetting env. episode 618 reward total was -19.000000. loss_func: 1.014648\n",
      "resetting env. episode 619 reward total was -21.000000. loss_func: 0.993836\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 620 reward total was -21.000000. loss_func: 1.013602\n",
      "resetting env. episode 621 reward total was -19.000000. loss_func: 0.993645\n",
      "resetting env. episode 622 reward total was -21.000000. loss_func: 1.014976\n",
      "resetting env. episode 623 reward total was -18.000000. loss_func: 0.993722\n",
      "resetting env. episode 624 reward total was -21.000000. loss_func: 1.014914\n",
      "resetting env. episode 625 reward total was -21.000000. loss_func: 0.995690\n",
      "resetting env. episode 626 reward total was -21.000000. loss_func: 1.012395\n",
      "resetting env. episode 627 reward total was -20.000000. loss_func: 0.996562\n",
      "resetting env. episode 628 reward total was -21.000000. loss_func: 1.014695\n",
      "resetting env. episode 629 reward total was -20.000000. loss_func: 0.995702\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 630 reward total was -20.000000. loss_func: 1.014500\n",
      "resetting env. episode 631 reward total was -20.000000. loss_func: 0.995055\n",
      "resetting env. episode 632 reward total was -20.000000. loss_func: 1.015495\n",
      "resetting env. episode 633 reward total was -20.000000. loss_func: 0.992825\n",
      "resetting env. episode 634 reward total was -20.000000. loss_func: 1.016005\n",
      "resetting env. episode 635 reward total was -20.000000. loss_func: 0.994054\n",
      "resetting env. episode 636 reward total was -21.000000. loss_func: 1.014581\n",
      "resetting env. episode 637 reward total was -20.000000. loss_func: 0.995077\n",
      "resetting env. episode 638 reward total was -19.000000. loss_func: 1.014929\n",
      "resetting env. episode 639 reward total was -19.000000. loss_func: 0.995001\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 640 reward total was -19.000000. loss_func: 1.013330\n",
      "resetting env. episode 641 reward total was -21.000000. loss_func: 0.995587\n",
      "resetting env. episode 642 reward total was -20.000000. loss_func: 1.016075\n",
      "resetting env. episode 643 reward total was -20.000000. loss_func: 0.993838\n",
      "resetting env. episode 644 reward total was -20.000000. loss_func: 1.013542\n",
      "resetting env. episode 645 reward total was -21.000000. loss_func: 0.992815\n",
      "resetting env. episode 646 reward total was -18.000000. loss_func: 1.012813\n",
      "resetting env. episode 647 reward total was -21.000000. loss_func: 0.994164\n",
      "resetting env. episode 648 reward total was -21.000000. loss_func: 1.013158\n",
      "resetting env. episode 649 reward total was -21.000000. loss_func: 0.996393\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 650 reward total was -21.000000. loss_func: 1.012094\n",
      "resetting env. episode 651 reward total was -21.000000. loss_func: 0.996173\n",
      "resetting env. episode 652 reward total was -21.000000. loss_func: 1.015123\n",
      "resetting env. episode 653 reward total was -20.000000. loss_func: 0.994791\n",
      "resetting env. episode 654 reward total was -21.000000. loss_func: 1.011756\n",
      "resetting env. episode 655 reward total was -21.000000. loss_func: 0.993598\n",
      "resetting env. episode 656 reward total was -21.000000. loss_func: 1.016521\n",
      "resetting env. episode 657 reward total was -21.000000. loss_func: 0.993908\n",
      "resetting env. episode 658 reward total was -21.000000. loss_func: 1.017667\n",
      "resetting env. episode 659 reward total was -21.000000. loss_func: 0.993794\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 660 reward total was -20.000000. loss_func: 1.012682\n",
      "resetting env. episode 661 reward total was -21.000000. loss_func: 0.994336\n",
      "resetting env. episode 662 reward total was -21.000000. loss_func: 1.013128\n",
      "resetting env. episode 663 reward total was -21.000000. loss_func: 0.995783\n",
      "resetting env. episode 664 reward total was -21.000000. loss_func: 1.012579\n",
      "resetting env. episode 665 reward total was -18.000000. loss_func: 0.995254\n",
      "resetting env. episode 666 reward total was -20.000000. loss_func: 1.012107\n",
      "resetting env. episode 667 reward total was -21.000000. loss_func: 0.995241\n",
      "resetting env. episode 668 reward total was -21.000000. loss_func: 1.017185\n",
      "resetting env. episode 669 reward total was -21.000000. loss_func: 0.994026\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 670 reward total was -21.000000. loss_func: 1.017440\n",
      "resetting env. episode 671 reward total was -21.000000. loss_func: 0.994302\n",
      "resetting env. episode 672 reward total was -20.000000. loss_func: 1.014617\n",
      "resetting env. episode 673 reward total was -20.000000. loss_func: 0.993947\n",
      "resetting env. episode 674 reward total was -21.000000. loss_func: 1.018924\n",
      "resetting env. episode 675 reward total was -20.000000. loss_func: 0.994892\n",
      "resetting env. episode 676 reward total was -20.000000. loss_func: 1.012159\n",
      "resetting env. episode 677 reward total was -20.000000. loss_func: 0.995972\n",
      "resetting env. episode 678 reward total was -21.000000. loss_func: 1.014518\n",
      "resetting env. episode 679 reward total was -21.000000. loss_func: 0.994743\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 680 reward total was -21.000000. loss_func: 1.011681\n",
      "resetting env. episode 681 reward total was -20.000000. loss_func: 0.994484\n",
      "resetting env. episode 682 reward total was -21.000000. loss_func: 1.014214\n",
      "resetting env. episode 683 reward total was -21.000000. loss_func: 0.993364\n",
      "resetting env. episode 684 reward total was -20.000000. loss_func: 1.014596\n",
      "resetting env. episode 685 reward total was -21.000000. loss_func: 0.993574\n",
      "resetting env. episode 686 reward total was -21.000000. loss_func: 1.014314\n",
      "resetting env. episode 687 reward total was -21.000000. loss_func: 0.995497\n",
      "resetting env. episode 688 reward total was -21.000000. loss_func: 1.015341\n",
      "resetting env. episode 689 reward total was -21.000000. loss_func: 0.995972\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 690 reward total was -21.000000. loss_func: 1.013794\n",
      "resetting env. episode 691 reward total was -21.000000. loss_func: 0.995886\n",
      "resetting env. episode 692 reward total was -17.000000. loss_func: 1.012109\n",
      "resetting env. episode 693 reward total was -19.000000. loss_func: 0.994857\n",
      "resetting env. episode 694 reward total was -20.000000. loss_func: 1.014203\n",
      "resetting env. episode 695 reward total was -21.000000. loss_func: 0.993820\n",
      "resetting env. episode 696 reward total was -21.000000. loss_func: 1.012162\n",
      "resetting env. episode 697 reward total was -21.000000. loss_func: 0.993809\n",
      "resetting env. episode 698 reward total was -21.000000. loss_func: 1.011921\n",
      "resetting env. episode 699 reward total was -19.000000. loss_func: 0.995361\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 700 reward total was -21.000000. loss_func: 1.018858\n",
      "resetting env. episode 701 reward total was -21.000000. loss_func: 0.994441\n",
      "resetting env. episode 702 reward total was -20.000000. loss_func: 1.015124\n",
      "resetting env. episode 703 reward total was -21.000000. loss_func: 0.995204\n",
      "resetting env. episode 704 reward total was -20.000000. loss_func: 1.013789\n",
      "resetting env. episode 705 reward total was -20.000000. loss_func: 0.995291\n",
      "resetting env. episode 706 reward total was -19.000000. loss_func: 1.014001\n",
      "resetting env. episode 707 reward total was -21.000000. loss_func: 0.993857\n",
      "resetting env. episode 708 reward total was -21.000000. loss_func: 1.015097\n",
      "resetting env. episode 709 reward total was -21.000000. loss_func: 0.995965\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 710 reward total was -21.000000. loss_func: 1.016277\n",
      "resetting env. episode 711 reward total was -21.000000. loss_func: 0.994265\n",
      "resetting env. episode 712 reward total was -21.000000. loss_func: 1.018319\n",
      "resetting env. episode 713 reward total was -20.000000. loss_func: 0.992772\n",
      "resetting env. episode 714 reward total was -21.000000. loss_func: 1.012898\n",
      "resetting env. episode 715 reward total was -21.000000. loss_func: 0.992701\n",
      "resetting env. episode 716 reward total was -21.000000. loss_func: 1.014885\n",
      "resetting env. episode 717 reward total was -20.000000. loss_func: 0.994602\n",
      "resetting env. episode 718 reward total was -21.000000. loss_func: 1.014909\n",
      "resetting env. episode 719 reward total was -21.000000. loss_func: 0.995928\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 720 reward total was -21.000000. loss_func: 1.016967\n",
      "resetting env. episode 721 reward total was -19.000000. loss_func: 0.995808\n",
      "resetting env. episode 722 reward total was -21.000000. loss_func: 1.013204\n",
      "resetting env. episode 723 reward total was -21.000000. loss_func: 0.996124\n",
      "resetting env. episode 724 reward total was -21.000000. loss_func: 1.013604\n",
      "resetting env. episode 725 reward total was -20.000000. loss_func: 0.994904\n",
      "resetting env. episode 726 reward total was -21.000000. loss_func: 1.014323\n",
      "resetting env. episode 727 reward total was -19.000000. loss_func: 0.993725\n",
      "resetting env. episode 728 reward total was -19.000000. loss_func: 1.011903\n",
      "resetting env. episode 729 reward total was -21.000000. loss_func: 0.993126\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 730 reward total was -21.000000. loss_func: 1.014473\n",
      "resetting env. episode 731 reward total was -21.000000. loss_func: 0.994469\n",
      "resetting env. episode 732 reward total was -20.000000. loss_func: 1.013570\n",
      "resetting env. episode 733 reward total was -21.000000. loss_func: 0.995822\n",
      "resetting env. episode 734 reward total was -21.000000. loss_func: 1.013802\n",
      "resetting env. episode 735 reward total was -21.000000. loss_func: 0.995841\n",
      "resetting env. episode 736 reward total was -21.000000. loss_func: 1.015889\n",
      "resetting env. episode 737 reward total was -21.000000. loss_func: 0.994771\n",
      "resetting env. episode 738 reward total was -20.000000. loss_func: 1.014648\n",
      "resetting env. episode 739 reward total was -18.000000. loss_func: 0.994440\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 740 reward total was -21.000000. loss_func: 1.015380\n",
      "resetting env. episode 741 reward total was -20.000000. loss_func: 0.994595\n",
      "resetting env. episode 742 reward total was -21.000000. loss_func: 1.012588\n",
      "resetting env. episode 743 reward total was -21.000000. loss_func: 0.995277\n",
      "resetting env. episode 744 reward total was -21.000000. loss_func: 1.015276\n",
      "resetting env. episode 745 reward total was -21.000000. loss_func: 0.994768\n",
      "resetting env. episode 746 reward total was -21.000000. loss_func: 1.015352\n",
      "resetting env. episode 747 reward total was -19.000000. loss_func: 0.994453\n",
      "resetting env. episode 748 reward total was -20.000000. loss_func: 1.015098\n",
      "resetting env. episode 749 reward total was -19.000000. loss_func: 0.994522\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 750 reward total was -20.000000. loss_func: 1.013270\n",
      "resetting env. episode 751 reward total was -21.000000. loss_func: 0.994716\n",
      "resetting env. episode 752 reward total was -21.000000. loss_func: 1.015005\n",
      "resetting env. episode 753 reward total was -21.000000. loss_func: 0.995551\n",
      "resetting env. episode 754 reward total was -21.000000. loss_func: 1.014748\n",
      "resetting env. episode 755 reward total was -20.000000. loss_func: 0.995175\n",
      "resetting env. episode 756 reward total was -20.000000. loss_func: 1.013044\n",
      "resetting env. episode 757 reward total was -21.000000. loss_func: 0.995390\n",
      "resetting env. episode 758 reward total was -20.000000. loss_func: 1.016057\n",
      "resetting env. episode 759 reward total was -20.000000. loss_func: 0.995308\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 760 reward total was -21.000000. loss_func: 1.014761\n",
      "resetting env. episode 761 reward total was -18.000000. loss_func: 0.994627\n",
      "resetting env. episode 762 reward total was -21.000000. loss_func: 1.013352\n",
      "resetting env. episode 763 reward total was -20.000000. loss_func: 0.994104\n",
      "resetting env. episode 764 reward total was -20.000000. loss_func: 1.013438\n",
      "resetting env. episode 765 reward total was -20.000000. loss_func: 0.994625\n",
      "resetting env. episode 766 reward total was -21.000000. loss_func: 1.014869\n",
      "resetting env. episode 767 reward total was -21.000000. loss_func: 0.994823\n",
      "resetting env. episode 768 reward total was -21.000000. loss_func: 1.018367\n",
      "resetting env. episode 769 reward total was -20.000000. loss_func: 0.994645\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 770 reward total was -19.000000. loss_func: 1.012887\n",
      "resetting env. episode 771 reward total was -21.000000. loss_func: 0.994985\n",
      "resetting env. episode 772 reward total was -20.000000. loss_func: 1.013041\n",
      "resetting env. episode 773 reward total was -21.000000. loss_func: 0.995782\n",
      "resetting env. episode 774 reward total was -21.000000. loss_func: 1.015861\n",
      "resetting env. episode 775 reward total was -21.000000. loss_func: 0.995800\n",
      "resetting env. episode 776 reward total was -21.000000. loss_func: 1.017718\n",
      "resetting env. episode 777 reward total was -21.000000. loss_func: 0.994600\n",
      "resetting env. episode 778 reward total was -21.000000. loss_func: 1.014988\n",
      "resetting env. episode 779 reward total was -19.000000. loss_func: 0.994994\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 780 reward total was -21.000000. loss_func: 1.014698\n",
      "resetting env. episode 781 reward total was -21.000000. loss_func: 0.994389\n",
      "resetting env. episode 782 reward total was -19.000000. loss_func: 1.011743\n",
      "resetting env. episode 783 reward total was -20.000000. loss_func: 0.994643\n",
      "resetting env. episode 784 reward total was -21.000000. loss_func: 1.013545\n",
      "resetting env. episode 785 reward total was -21.000000. loss_func: 0.995356\n",
      "resetting env. episode 786 reward total was -20.000000. loss_func: 1.011491\n",
      "resetting env. episode 787 reward total was -21.000000. loss_func: 0.996191\n",
      "resetting env. episode 788 reward total was -21.000000. loss_func: 1.016642\n",
      "resetting env. episode 789 reward total was -21.000000. loss_func: 0.995495\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 790 reward total was -21.000000. loss_func: 1.014974\n",
      "resetting env. episode 791 reward total was -21.000000. loss_func: 0.994544\n",
      "resetting env. episode 792 reward total was -20.000000. loss_func: 1.016125\n",
      "resetting env. episode 793 reward total was -21.000000. loss_func: 0.993778\n",
      "resetting env. episode 794 reward total was -21.000000. loss_func: 1.011746\n",
      "resetting env. episode 795 reward total was -21.000000. loss_func: 0.995957\n",
      "resetting env. episode 796 reward total was -21.000000. loss_func: 1.013506\n",
      "resetting env. episode 797 reward total was -21.000000. loss_func: 0.996362\n",
      "resetting env. episode 798 reward total was -19.000000. loss_func: 1.012853\n",
      "resetting env. episode 799 reward total was -21.000000. loss_func: 0.997897\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 800 reward total was -21.000000. loss_func: 1.015339\n",
      "resetting env. episode 801 reward total was -20.000000. loss_func: 0.995758\n",
      "resetting env. episode 802 reward total was -20.000000. loss_func: 1.015137\n",
      "resetting env. episode 803 reward total was -16.000000. loss_func: 0.992973\n",
      "resetting env. episode 804 reward total was -21.000000. loss_func: 1.017982\n",
      "resetting env. episode 805 reward total was -21.000000. loss_func: 0.994452\n",
      "resetting env. episode 806 reward total was -21.000000. loss_func: 1.015588\n",
      "resetting env. episode 807 reward total was -21.000000. loss_func: 0.994600\n",
      "resetting env. episode 808 reward total was -20.000000. loss_func: 1.014893\n",
      "resetting env. episode 809 reward total was -21.000000. loss_func: 0.993995\n",
      "Saved model to disk at Models/H5k4_RL_pong_RMSprop_mse.ckpt\n",
      "resetting env. episode 810 reward total was -21.000000. loss_func: 1.012321\n",
      "resetting env. episode 811 reward total was -21.000000. loss_func: 0.996199\n",
      "resetting env. episode 812 reward total was -20.000000. loss_func: 1.013860\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f30601f5550c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Run predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;31m#         tpred = model.predict({'input_1':x,'input_2':np.array([[y]])});\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1585\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shouldsee/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "ys=[];byss=[];rss=[];tpreds=[];\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0;\n",
    "\n",
    "ModelName = 'H5k4_RL_pong_RMSprop_mse'\n",
    "ModelFile = 'Models/'+ModelName+'.ckpt';\n",
    "render = False;\n",
    "resume = 0;\n",
    "batch_size=1;\n",
    "learning_rate = 1e-2\n",
    "optimiser = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# episode_number = 0;\n",
    "\n",
    "if 'losshist' in locals():\n",
    "    pass\n",
    "else:\n",
    "    losshist = LossHist();\n",
    "\n",
    "while True:\n",
    "    if resume:\n",
    "        model = loadmodel(ModelFile)    \n",
    "#         losshist=losshist_temp;\n",
    "        \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss=lossfunc)\n",
    "    while True:\n",
    "        if render: env.render()\n",
    "\n",
    "        # preprocess the observation, set input to network to be difference image\n",
    "        cur_x = prepro(observation)\n",
    "        diff_x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "        prev_x = cur_x\n",
    "        x = np.reshape(diff_x,[1,D1,D2,1]);\n",
    "        \n",
    "        # Sample action and label it \n",
    "        aprob = 0.5;\n",
    "        action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "        y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "        \n",
    "        # Run predictor\n",
    "        tpred = model.predict([x,np.array([[y]])]);\n",
    "#         tpred = model.predict({'input_1':x,'input_2':np.array([[y]])});\n",
    "        \n",
    "    \n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) # observation\n",
    "        ys.append(y)\n",
    "        tpreds.append(tpred);\n",
    "#         spreds.append(spred);\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "        \n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys);\n",
    "            epr = np.vstack(drs)\n",
    "            eptpred=np.vstack(tpreds);\n",
    "#             epspred=np.vstack()\n",
    "            xs,hs,dlogps,drs,ys,tpreds = [],[],[],[],[],[] # reset array memory\n",
    "            \n",
    "            time_epr=abs(time_rewards(epr));\n",
    "#             time_epr = np.exp(-time_epr);\n",
    "            curr_pred = model.predict([epx,epy]);\n",
    "            pre_curr_loss = lossfunc(time_epr,-np.log(np.maximum(curr_pred,1E-15)))\n",
    "            curr_loss = tf.Tensor.eval(pre_curr_loss);\n",
    "            curr_loss1 = model.train_on_batch([epx,epy], np.exp(-time_epr));\n",
    "            \n",
    "            losshist.add(episode_number,curr_loss);\n",
    "            \n",
    "            if episode_number % batch_size == 0:\n",
    "                #accumulate gradient over batch where appropriate\n",
    "                pass\n",
    "   \n",
    "            # boring book-keeping\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print 'resetting env. episode %d reward total was %f. loss_func: %f' % (episode_number, reward_sum, curr_loss)\n",
    "            if episode_number % 10  == 9: \n",
    "                savemodel(model,ModelFile)\n",
    "\n",
    "            reward_sum = 0\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "            #     print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97662914298498926"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# curr_loss = lossfunc(time_epr,-np.log(np.maximum(curr_pred,1E-15)))\n",
    "# with sess.as_default():\n",
    "tf.InteractiveSession()\n",
    "tf.Tensor.eval(curr_loss)\n",
    "\n",
    "# np.max(curr_pred,1E-15)\n",
    "# np.maximum(curr_pred,1E-15)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06836867],\n",
       "       [ 1.06311238],\n",
       "       [ 1.06975412],\n",
       "       ..., \n",
       "       [ 1.1609776 ],\n",
       "       [ 1.26627338],\n",
       "       [ 1.24955106]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJCCAYAAAAC4omSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCsAgEIEKwCAQgQrAIBCBCsAgEL+P2vzF6CAfbFcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86f3d76dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# discount_rewards(epdlogp)\n",
    "x.size\n",
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "fig=plt.figure(figsize=[10,10])\n",
    "ax1=plt.subplot()\n",
    "# ax1.plot((time_epr))\n",
    "# ax1.plot(eptpred)\n",
    "# ax1.plot(np.sign(-(epspred-.5))*2*(eptpred-1))\n",
    "\n",
    "\n",
    "# ax1.plot(discounted_epr)\n",
    "# ax1.scatter(abs(time_epr),eptpred)\n",
    "# ax1.scatter(np.exp(-time_epr),eptpred)\n",
    "\n",
    "# ax1.scatter(np.exp(-time_epr),np.exp(-eptpred))\n",
    "ax1.scatter(np.exp(-time_epr),eptpred)\n",
    "\n",
    "# ax1.set_xlim([0, .100])\n",
    "# ax1.set_xlim([500, 1000])\n",
    "# ax1.set_ylim([-0, 50])\n",
    "eptpred\n",
    "\n",
    "# ax1.imshow(eph[:500,:500].T)\n",
    "# tpreds.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84387147"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(time_epr.ravel())\n",
    "# time_epr\n",
    "# tpreds\n",
    "# H\n",
    "# np.expand_dims(epx,1).shape\n",
    "# curr_loss = sess.run(loss,feed_dict={xinput: epx, input_y: epy, rtime: time_epr});\n",
    "# epx.shape\n",
    "# tf.reshape(epy);\n",
    "# D*H\n",
    "# oSaver = tf.train.Saver()\n",
    "# print(sess.run(score,feed_dict={xinput: epx, input_y: epy, rtime: time_epr}))\n",
    "# oSess = sess\n",
    "# oSaver.save(oSess, ModelFile)\n",
    "curr_loss1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
